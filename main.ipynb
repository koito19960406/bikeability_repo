{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thrown-sculpture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (0.12.0)\n",
      "Requirement already satisfied: geopandas in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from geopandas) (3.0.1)\n",
      "Requirement already satisfied: fiona>=1.8 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from geopandas) (1.8.18)\n",
      "Requirement already satisfied: shapely>=1.6 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from geopandas) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from geopandas) (1.2.2)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: munch in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: click<8,>=4.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (2020.12.5)\n",
      "Requirement already satisfied: six>=1.7 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (0.7.1)\n",
      "Requirement already satisfied: attrs>=17 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (20.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from pandas>=0.24.0->geopandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from pandas>=0.24.0->geopandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from pandas>=0.24.0->geopandas) (1.20.1)\n",
      "Requirement already satisfied: imagehash in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (4.2.0)\n",
      "Requirement already satisfied: PyWavelets in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from imagehash) (1.1.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from imagehash) (1.5.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from imagehash) (1.15.0)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from imagehash) (8.1.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from imagehash) (1.20.1)\n",
      "Requirement already satisfied: osmnx in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: networkx>=2.5 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from osmnx) (2.5)\n",
      "Requirement already satisfied: requests>=2.25 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from osmnx) (2.25.1)\n",
      "Requirement already satisfied: descartes>=1.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from osmnx) (1.1.0)\n",
      "Requirement already satisfied: Rtree>=0.9 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from osmnx) (0.9.7)\n",
      "Requirement already satisfied: pyproj>=2.6 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from osmnx) (3.0.1)\n",
      "Requirement already satisfied: Shapely>=1.7 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from osmnx) (1.7.1)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from osmnx) (3.3.4)\n",
      "Requirement already satisfied: pandas>=1.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from osmnx) (1.2.2)\n",
      "Requirement already satisfied: geopandas>=0.8 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from osmnx) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from osmnx) (1.20.1)\n",
      "Requirement already satisfied: fiona>=1.8 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from geopandas>=0.8->osmnx) (1.8.18)\n",
      "Requirement already satisfied: six>=1.7 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas>=0.8->osmnx) (1.15.0)\n",
      "Requirement already satisfied: attrs>=17 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas>=0.8->osmnx) (20.3.0)\n",
      "Requirement already satisfied: click<8,>=4.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas>=0.8->osmnx) (7.1.2)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas>=0.8->osmnx) (1.1.1)\n",
      "Requirement already satisfied: munch in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas>=0.8->osmnx) (2.5.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas>=0.8->osmnx) (0.7.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from fiona>=1.8->geopandas>=0.8->osmnx) (2020.12.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from matplotlib>=3.3->osmnx) (8.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from matplotlib>=3.3->osmnx) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from matplotlib>=3.3->osmnx) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from matplotlib>=3.3->osmnx) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from matplotlib>=3.3->osmnx) (2.8.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from networkx>=2.5->osmnx) (4.4.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from pandas>=1.1->osmnx) (2021.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from requests>=2.25->osmnx) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from requests>=2.25->osmnx) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from requests>=2.25->osmnx) (1.26.3)\n",
      "Requirement already satisfied: gluoncv in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (0.8.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from gluoncv) (2.25.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from gluoncv) (1.20.1)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from gluoncv) (8.1.0)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from gluoncv) (2.3.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from gluoncv) (3.3.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from gluoncv) (1.5.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from gluoncv) (4.57.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from matplotlib->gluoncv) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from matplotlib->gluoncv) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from matplotlib->gluoncv) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from matplotlib->gluoncv) (0.10.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->gluoncv) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from requests->gluoncv) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from requests->gluoncv) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from requests->gluoncv) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from requests->gluoncv) (2.10)\n",
      "Requirement already satisfied: rasterio in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (1.2.1)\n",
      "Requirement already satisfied: click<8,>=4.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from rasterio) (7.1.2)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from rasterio) (20.3.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from rasterio) (0.7.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from rasterio) (1.20.1)\n",
      "Requirement already satisfied: snuggs>=1.4.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from rasterio) (1.4.7)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from rasterio) (2020.12.5)\n",
      "Requirement already satisfied: click-plugins in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from rasterio) (1.1.1)\n",
      "Requirement already satisfied: affine in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from rasterio) (2.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n",
      "Requirement already satisfied: lightgbm in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from lightgbm) (1.20.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from lightgbm) (1.5.3)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from lightgbm) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: sklearn in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.20.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Collecting torch\n",
      "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 804.1 MB 854 bytes/s a 0:00:01     |█████████                       | 226.8 MB 55.7 MB/s eta 0:00:11\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from torch) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.8.1\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.4 MB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from torchvision) (1.20.1)\n",
      "Requirement already satisfied: torch==1.8.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from torchvision) (8.1.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.9.1\n"
     ]
    }
   ],
   "source": [
    "# !conda config --prepend channels conda-forge\n",
    "# !conda create -n ox --strict-channel-priority osmnx # run this if you haven't installed osmnx\n",
    "!pip install xmltodict\n",
    "!pip install geopandas\n",
    "!pip install imagehash\n",
    "!pip install osmnx\n",
    "!pip install gluoncv\n",
    "!pip install rasterio\n",
    "!pip install lightgbm\n",
    "!pip install joblib\n",
    "!pip install sklearn\n",
    "# !pip install selenium\n",
    "# !pip install webdriver_manager\n",
    "# !pip install inplace-abn\n",
    "# local python script\n",
    "from Perception_Prediction import pipeline_utils\n",
    "from get_gsv import get_gsv\n",
    "from get_aqi import get_aqi\n",
    "from stitch_image import stitch_images\n",
    "from Classification import classification\n",
    "from Segmentation import segmentation\n",
    "from Detection import detection\n",
    "from Edge_Detection import edge_detection\n",
    "from Blob_Detection import blob_detection\n",
    "from HLS_Statistics import hls_statistics\n",
    "from image_hash import image_hash\n",
    "\n",
    "# non-local libraries\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shutil, os\n",
    "import random\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import imagehash\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from geopandas.tools import sjoin\n",
    "import osmnx as ox\n",
    "import json\n",
    "from pandas.io.json import json_normalize #package for flattening json in pandas df\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import fiona\n",
    "import math\n",
    "import rasterio\n",
    "# from rasterio.plot import show\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import copy \n",
    "import scipy as sp\n",
    "from scipy.stats import chi2\n",
    "from sklearn.covariance import MinCovDet\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-bible",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "\n",
    "1. Get Street Network from OSM\n",
    "2. Create Sample Points from Street Network\n",
    "3. Sample 8,000 points (To reduce the computation when collecting meta data)\n",
    "4. Collect GSV meta data\n",
    "5. Get GSV images\n",
    "6. Stitch images\n",
    "7. Remove grey images\n",
    "8. Sample 1,500 images for survey, save locally, and upload them to S3 bucket\n",
    "9. Get URLs of survey images\n",
    "10. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-character",
   "metadata": {},
   "source": [
    "# Get Street Network from OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_list=['Singapore','Tokyo']\n",
    "city_list=[\n",
    "    'Chiyoda, Tokyo',\n",
    "    'Chuo, Tokyo',\n",
    "    'Minato, Tokyo',\n",
    "    'Shinjuku, Tokyo',\n",
    "    'Bunkyo, Tokyo',\n",
    "    'Taito, Tokyo',\n",
    "    'Sumida, Tokyo',\n",
    "    'Koto, Tokyo',\n",
    "    'Shinagawa, Tokyo',\n",
    "    'Meguro, Tokyo',\n",
    "    'Ota, Tokyo',\n",
    "    'Setagaya, Tokyo',\n",
    "    'Shibuya, Tokyo',\n",
    "    'Nakano, Tokyo',\n",
    "    'Suginami, Tokyo',\n",
    "    'Toshima, Tokyo',\n",
    "    'Kita, Tokyo',\n",
    "    'Arakawa, Tokyo',\n",
    "    'Itabashi, Tokyo',\n",
    "    'Nerima, Tokyo',\n",
    "    'Adachi,, Tokyo',\n",
    "    'Katsushika, Tokyo',\n",
    "    'Edogawa, Tokyo'\n",
    "]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "out_folder=os.path.join(root,'data/network/')\n",
    "get_gsv.getStreetNetwork(city_list,out_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-coupon",
   "metadata": {},
   "source": [
    "# Create Sample Points from Street Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the Tokyo shp (edges)\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "tokyo_shp_list = glob.glob(os.path.join(root,'data/network/*, Tokyo_street_network/edges.shp'))\n",
    "gdf = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in tokyo_shp_list],\n",
    "                                 ignore_index=True), crs=gpd.read_file(tokyo_shp_list[0]).crs)\n",
    "if not os.path.exists(os.path.join(root,'data/network/Tokyo_street_network')):\n",
    "    os.makedirs(os.path.join(root,'data/network/Tokyo_street_network'))\n",
    "gdf.to_file(os.path.join(root,'data/network/Tokyo_street_network/edges.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the Tokyo shp (nodes)\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "tokyo_shp_list = glob.glob(os.path.join(root,'data/network/*, Tokyo_street_network/nodes.shp'))\n",
    "gdf = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in tokyo_shp_list],\n",
    "                                 ignore_index=True), crs=gpd.read_file(tokyo_shp_list[0]).crs)\n",
    "if not os.path.exists(os.path.join(root,'data/network/Tokyo_street_network')):\n",
    "    os.makedirs(os.path.join(root,'data/network/Tokyo_street_network'))\n",
    "gdf.to_file(os.path.join(root,'data/network/Tokyo_street_network/nodes.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample points on the street network\n",
    "city_list=[\n",
    "#     'Singapore',\n",
    "    'Tokyo']\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "outfolder=os.path.join(root,'data/point_data/')\n",
    "if not os.path.exists(outfolder):\n",
    "    os.makedirs(outfolder)\n",
    "for city in city_list:\n",
    "    mini_dist = 1000 #the minimum distance between two generated points in meters\n",
    "    inshp = os.path.join(root,'data/network/{}_street_network/edges.shp'.format(city)) #the input shapefile of road network\n",
    "    outshp = os.path.join(outfolder,'{}_{}m_point.shp'.format(city,mini_dist)) #the output shapefile of the points\n",
    "    get_gsv.createPoints(inshp, outshp, mini_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the points created\n",
    "city_list=['Singapore','Tokyo']\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "outfolder=os.path.join(root,'data/point_data/')\n",
    "mini_dist=1000\n",
    "for city in city_list:\n",
    "    points=gpd.read_file(os.path.join(root,outfolder,'{}_{}m_point.shp'.format(city,mini_dist)))\n",
    "    print(city, points.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-brown",
   "metadata": {},
   "source": [
    "# Sample 8,000 points (To reduce the computation when collecting meta data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "#     'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "sample_num=8000\n",
    "for city in city_list:\n",
    "    inshp=os.path.join(root,'data/point_data/{}_1000m_point.shp'.format(city))\n",
    "    outshp=os.path.join(root,'data/point_data/{}_{}_sample_point.shp'.format(city,str(sample_num)))\n",
    "    get_gsv.getSamplePoints(inshp,outshp,sample_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-evanescence",
   "metadata": {},
   "source": [
    "# Collect GSV meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "#     'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "sample_num=8000\n",
    "for city in city_list:\n",
    "    inputShp =os.path.join(root,'data/point_data/{}_{}_sample_point.shp'.format(city,str(sample_num)))\n",
    "    outputTxt = os.path.join(root,'data/meta_data_{}/'.format(city))\n",
    "    batch_num=1000\n",
    "    get_gsv.GSVpanoMetadataCollector(inputShp,batch_num,outputTxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all the meta data files together\n",
    "city_list=[\n",
    "#     'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "for city in city_list:\n",
    "    file_list = glob.glob(os.path.join(root,'data/meta_data_{}/Pnt*.csv'.format(city)))\n",
    "    df = pd.DataFrame(pd.concat([pd.read_csv(i) for i in file_list],\n",
    "                                 ignore_index=True))\n",
    "    df.to_csv(os.path.join(root,'data/meta_data_{}/meta_data_{}.csv'.format(city,city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-tennessee",
   "metadata": {},
   "source": [
    "# Sample 7,142 Points (Because this is the max number with free tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "#     'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "sample_num=7142\n",
    "\n",
    "# create an output folder if it doesnt exist\n",
    "outfolder=os.path.join(root,'data/pano_id_list/')\n",
    "if not os.path.exists(outfolder):\n",
    "    os.makedirs(outfolder)\n",
    "    \n",
    "for city in city_list:\n",
    "    intxt=os.path.join(root,'data/meta_data_{}/meta_data_{}.csv'.format(city,city))\n",
    "    outtxt=os.path.join(outfolder,'{}_{}_sample_point.csv'.format(city,str(sample_num)))\n",
    "    get_gsv.getSamplePoints(intxt,outtxt,sample_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-current",
   "metadata": {},
   "source": [
    "# Get GSV images (!Don't run this twice!)\n",
    "## Also, dont forget to exclude indoor images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "#     'Singapore',\n",
    "#     'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "sample_num=7142\n",
    "    \n",
    "for city in city_list:\n",
    "    # create outfolder\n",
    "    outputImgFolder=os.path.join(root,'data/img_seperate_{}/'.format(city))\n",
    "    # prevent the second run\n",
    "    if os.path.exists(outputImgFolder):\n",
    "        continue\n",
    "    inputCSV = os.path.join(root,'data/pano_id_list/{}_{}_sample_point.csv'.format(city,str(sample_num)))\n",
    "    get_gsv.getGSV(inputCSV,outputImgFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-workstation",
   "metadata": {},
   "source": [
    "# Stitch the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "#     'Singapore'\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "sample_num=7142\n",
    "\n",
    "for city in city_list:\n",
    "    inputCSV = os.path.join(root,'data/pano_id_list/{}_{}_sample_point.csv'.format(city,str(sample_num)))\n",
    "    inputImgFolder = os.path.join(root,'data/img_seperate_{}/'.format(city))\n",
    "    outputImgFolder = os.path.join(root,'data/img_stitched_{}/'.format(city))\n",
    "    stitch_images.stitchImg(inputCSV,inputImgFolder,outputImgFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-football",
   "metadata": {},
   "source": [
    "# remove gray images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find gray images by using imagehash and move them to a seperate folder\n",
    "city_list=[\n",
    "#     'Singapore'\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "gray_img=os.path.join(root,'data/gray_img/pano=CAoSLEFGMVFpcE0tTkFGRmEtOXdibWE4N2FBY1dxLXV0RURxVzRiXzhOOFlPSEdp.jpg')\n",
    "hash = imagehash.average_hash(Image.open(gray_img))\n",
    "for city in city_list:\n",
    "    outfolder=os.path.join(root,'data/gray_img_{}/'.format(city))\n",
    "    if not os.path.exists(outfolder):\n",
    "        os.makedirs(outfolder)\n",
    "    img_list=glob.glob(os.path.join(root,'data/img_stitched_{}/*.jpg'.format(city)))\n",
    "    for img in tqdm.tqdm(img_list):\n",
    "        otherhash = imagehash.average_hash(Image.open(img))\n",
    "        if hash == otherhash:\n",
    "            shutil.move(img,os.path.join(outfolder,img.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check images in \"gray_img_...\" folder\n",
    "city_list=[\n",
    "#     'Singapore'\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in city_list:\n",
    "    gray_img_list=glob.glob(os.path.join(root,'data/gray_img_{}/*.jpg'.format(city)))\n",
    "    for gray_img in tqdm.tqdm(gray_img_list):\n",
    "        pil_im = Image.open(gray_img)\n",
    "        display(pil_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-aquatic",
   "metadata": {},
   "source": [
    "# sample 1,500 images for survey, save locally, and upload them to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "n=1500 # sample size\n",
    "\n",
    "# set up sagemaker session\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "print(sess)\n",
    "\n",
    "# create a s3 bucket\n",
    "bucket_name='gsv-perception-survey-img'\n",
    "key_prefix='data'\n",
    "s3 = boto3.resource('s3')\n",
    "my_region = boto3.session.Session().region_name\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)\n",
    "\n",
    "# go through city_list\n",
    "for city in city_list:\n",
    "    # sample 1,500 images\n",
    "    img_list=glob.glob(os.path.join(root,'data/img_stitched_{}/*.jpg'.format(city)))\n",
    "    sampled_img=random.sample(img_list, n)\n",
    "    # create a local output folder\n",
    "    out_folder=os.path.join(root,'data/img_survey/{}/'.format(city))\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.makedirs(out_folder)\n",
    "    \n",
    "    # save in img_survey folder and upload the data to bucket\n",
    "    for img in tqdm.tqdm(sampled_img):\n",
    "        # save locally\n",
    "        shutil.copy2(img, out_folder)\n",
    "        \n",
    "        # upload to s3\n",
    "        sess.upload_data(path=img, bucket=bucket_name,key_prefix=key_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-israeli",
   "metadata": {},
   "source": [
    "# Get URL of the Survey Images in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up necessary variables\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "s3 = boto3.resource('s3')\n",
    "bukect_str='gsv-perception-survey-img'\n",
    "my_bucket = s3.Bucket(bukect_str)\n",
    "url_list=[]\n",
    "outputFolder=os.path.join(root,'data/tabular_data/')\n",
    "\n",
    "# go through all the objects in the bucket\n",
    "for file in my_bucket.objects.all():\n",
    "    # modify file name for URL encoding\n",
    "    file_modified=file.key.replace('=','%3D')\n",
    "    # create url\n",
    "    url='https://{BUCKET_NAME}.s3-ap-southeast-1.amazonaws.com/{FILE_NAME}'.\\\n",
    "    format(BUCKET_NAME=bukect_str,FILE_NAME=file_modified)\n",
    "    url_list.append(url)\n",
    "url_df=pd.DataFrame(url_list,columns=['url'])\n",
    "url_df.to_csv(os.path.join(outputFolder,'aws_url.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # don't run this unless you want to delete all the sample images in a s3 bukect for surveys\n",
    "# import boto3\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket('gsv-perception-survey-img')\n",
    "# for obj in tqdm.tqdm(bucket.objects.filter(Prefix='data/')):\n",
    "#     s3.Object(bucket.name,obj.key).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-future",
   "metadata": {},
   "source": [
    "# Upload the remaining stitched images to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "#     'Singapore'\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# set up sagemaker session\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "print(sess)\n",
    "\n",
    "# create a s3 bucket\n",
    "bucket_name='gsv-img'\n",
    "s3 = boto3.resource('s3')\n",
    "my_region = boto3.session.Session().region_name\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "    time.sleep(5)\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)\n",
    "#     sys.exit()  \n",
    "    \n",
    "\n",
    "# go through city_list\n",
    "for city in city_list:\n",
    "    img_list=glob.glob(os.path.join(root,'data/img_stitched_{}/*.jpg'.format(city)))\n",
    "    # upload the data to bucket\n",
    "    for img in tqdm.tqdm(img_list,position=0):\n",
    "        sess.upload_data(path=img, bucket=bucket_name,key_prefix=city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-accuracy",
   "metadata": {},
   "source": [
    "# Get locations of Remaining Stitched Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# go through city_list\n",
    "for city in city_list:\n",
    "    # get meta data with pano IDs and locations\n",
    "    meta_data=os.path.join(root,'data/meta_data_{}/meta_data_{}.csv'.format(city,city))\n",
    "    meta_data_df=pd.read_csv(meta_data)\n",
    "    \n",
    "    # get all the remaining images' pano IDs and convert to dataframe\n",
    "    img_list=glob.glob(os.path.join(root,'data/img_stitched_{}/*.jpg'.format(city)))\n",
    "    img_list_len=len(img_list)\n",
    "    pano_id_list=[os.path.split(img)[1].replace('.jpg','').replace('pano=','') for img in img_list]\n",
    "    pano_id_df = pd.DataFrame(pano_id_list,columns=['panoId'])\n",
    "    img_location_df=pd.merge(pano_id_df,\n",
    "                             meta_data_df[['panoId','panoLon','panoLat','distDiff']],\n",
    "                             on=['panoId'],\n",
    "                             how='left')\n",
    "    \n",
    "    # since there are some duplicates of panoIds in the meta data, drop them\n",
    "    img_location_df=img_location_df.drop_duplicates(subset=['panoId'])\n",
    "    \n",
    "    # save to meta data folder\n",
    "    img_location_df.to_csv(os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-boutique",
   "metadata": {},
   "source": [
    "# Extract non-perception features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-oakland",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "- quality of pavement (pothole)\n",
    "- quality of street lighting\n",
    "- presence of bike lanes\n",
    "- Street amenities (Garbage cans, Benches, phone booth, )\n",
    "- Utility pole\n",
    "- Bike parking\n",
    "- Directional signs\n",
    "- presence of sidewalk\n",
    "- Presense and quality of crosswalk (with/without traffic lights, traffic signs)\n",
    "- street accesibility (curb cuts)\n",
    "- stop sign\n",
    "- scenery along the bike lanes (built up area)\n",
    "- scenery along the bike lanes (green area)\n",
    "- scenery along the bike lanes (SVF)\n",
    "- scenery along the bike lanes (water)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-documentary",
   "metadata": {},
   "source": [
    "#### option1: deploy model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "#     'Singapore'\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# go through city_list\n",
    "for city in city_list:\n",
    "    # define input variables\n",
    "    model_data='s3://sagemaker-ap-southeast-1-428024436188/semantic-segmentation-demo/output/mapillary-segmentation-3/output/model.tar.gz'\n",
    "    input_img_folder=os.path.join(root,'data/img_stitched_{}/'.format(city))\n",
    "    resized_img_folder=os.path.join(root,'data/img_data_resized_{}/'.format(city))\n",
    "    output_txt_folder=os.path.join(root,'data/tabular_data/{}/'.format(city))\n",
    "    output_img_folder=os.path.join(root,'output/segmentation/{}/').format(city)\n",
    "    class_json_file=os.path.join(root,'models/config_v2.0.json')\n",
    "    for variable in [resized_img_folder,output_txt_folder]:\n",
    "        if not os.path.exists(variable):\n",
    "            os.makedirs(variable)\n",
    "    segmentation.segment_images(model_data,input_img_folder,resized_img_folder,output_txt_folder,output_img_folder,class_json_file,s3_upload_bucket=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-government",
   "metadata": {},
   "source": [
    "#### option2: run a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-geography",
   "metadata": {},
   "source": [
    "##### singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ec2-user/SageMaker/inplace_abn/scripts/\n",
    "!python test_vistas.py /home/ec2-user/SageMaker/models/wide_resnet38_deeplab_vistas.pth.tar /home/ec2-user/SageMaker/data/img_stitched_Singapore /home/ec2-user/SageMaker/data/segmentation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the segmented images from segmentation_test to output/segmentation/Singapore\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "segmented_img_list=glob.glob(os.path.join(root,'data/segmentation_test/*.png'))\n",
    "for segmented_img in segmented_img_list:\n",
    "    shutil.copy2(segmented_img,os.path.join(root,'output/segmentation/Singapore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-prison",
   "metadata": {},
   "source": [
    "##### Tokyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ec2-user/SageMaker/inplace_abn/scripts/\n",
    "!python test_vistas.py /home/ec2-user/SageMaker/models/wide_resnet38_deeplab_vistas.pth.tar /home/ec2-user/SageMaker/data/img_stitched_Tokyo /home/ec2-user/SageMaker/output/segmentation/Tokyo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-trace",
   "metadata": {},
   "source": [
    "#### Extract pixel proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# import classes as a dataframe\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "config=os.path.join(root,'models/config_v1.2.json')\n",
    "with open(config) as f:\n",
    "    d = json.load(f)\n",
    "classes_df = json_normalize(d['labels'],max_level=3)\n",
    "\n",
    "# go through city_list\n",
    "for city in city_list:\n",
    "    # create output_folder\n",
    "    output_folder=os.path.join(root,'data/tabular_data/{}/'.format(city))\n",
    "    if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "            \n",
    "    # creating variables to be used for extracting the pixl values\n",
    "    result_dict={}\n",
    "    \n",
    "    # segmented image list\n",
    "    segmented_img_list=glob.glob(os.path.join(root,'output/segmentation/{}/*.png'.format(city)))\n",
    "    for segmented_img in tqdm.tqdm(segmented_img_list):\n",
    "        temp_dict={}\n",
    "        im = Image.open(segmented_img)\n",
    "        array=np.array(im)\n",
    "        # calculate each category's pixel proportion\n",
    "        for i in range(len(classes_df)):\n",
    "            total_num=array.shape[0]*array.shape[1]\n",
    "            x_num=np.count_nonzero(array==i)\n",
    "            ratio=x_num/total_num\n",
    "            temp_dict[i]=ratio\n",
    "        result_dict[os.path.split(segmented_img)[1].replace('.png','')]= temp_dict\n",
    "    result_df=pd.DataFrame(result_dict)\n",
    "    result_df=result_df.transpose()\n",
    "\n",
    "    # change the column names\n",
    "    result_df.columns=classes_df['name'].tolist()\n",
    "    result_df.to_csv(os.path.join(output_folder,'segmentation.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-impression",
   "metadata": {},
   "source": [
    "#### Extract indicators\n",
    "- quality of pavement (pothole)\n",
    "- quality of street lighting\n",
    "- presence of bike lanes\n",
    "- Street amenities (Garbage cans, Benches, phone booth, )\n",
    "- Utility pole\n",
    "- Bike parking\n",
    "- Directional signs\n",
    "- presence of sidewalk\n",
    "- Presense and quality of crosswalk (with/without traffic lights, traffic signs)\n",
    "- street accesibility (curb cuts)\n",
    "- stop sign\n",
    "- scenery along the bike lanes (built up area)\n",
    "- scenery along the bike lanes (green area)\n",
    "- scenery along the bike lanes (SVF)\n",
    "- scenery along the bike lanes (water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of indicators\n",
    "indicator_dict={'pavement':['object--pothole'],\n",
    "                'street_light':['object--street-light'],\n",
    "                'bike_lanes':['construction--flat--bike-lane'],\n",
    "                'street_amenity':['object--trash-can',\n",
    "                                  'object--bench',\n",
    "                                  'object--phone-booth'],\n",
    "                'utility_pole':['object--support--utility-pole'],\n",
    "                'bike_parking':['object--bike-rack'],\n",
    "                'side_walk':['construction--flat--sidewalk'],\n",
    "                'cross_walk':['construction--flat--crosswalk-plain',\n",
    "                             'marking--crosswalk-zebra'],\n",
    "                'accessibility':['construction--flat--curb-cut'],\n",
    "                'scenery_building':['construction--structure--building'],\n",
    "                'scenery_greenery':['nature--vegetation'],\n",
    "                'scenery_sky':['nature--sky'],\n",
    "                'scenery_water':['nature--water']\n",
    "               }\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# go through city_list\n",
    "for city in city_list:\n",
    "    segmentation_result=os.path.join(root,'data/tabular_data/{}/segmentation.csv'.format(city))\n",
    "    segmentation_result_df=pd.read_csv(segmentation_result)\n",
    "    # rename the first col to pano_id\n",
    "    segmentation_result_df=segmentation_result_df.rename(columns={'Unnamed: 0':'pano_id'})\n",
    "    for key in indicator_dict.keys():\n",
    "        temp_df=segmentation_result_df[['pano_id']]\n",
    "        for indicator in indicator_dict[key]:\n",
    "            indicator_df=segmentation_result_df[['pano_id',indicator]]\n",
    "            # append indicator_df to temp_df\n",
    "            temp_df=pd.merge(temp_df,\n",
    "                             indicator_df,\n",
    "                             on='pano_id',\n",
    "                             how='left'\n",
    "                            )\n",
    "        temp_df['total']=temp_df.iloc[:,1:].sum(axis=1)\n",
    "        \n",
    "        temp_df.to_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,key)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-potato",
   "metadata": {},
   "source": [
    "## Detection\n",
    "### cycling mode share\n",
    "    0. Filter detection data to only have transport related columns\n",
    "    1. Merge locational data with the detection data\n",
    "    2. Copy the dataframe\n",
    "    3. Conduct spatial aggregation with 500m buffer between the original and the copy\n",
    "    4. Group by pano_id\n",
    "    5. Calculate the sum of all the columns\n",
    "    6. Calculate the cycling mode share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# go through city_list\n",
    "for city in city_list:\n",
    "    # detection result\n",
    "    detection_result=os.path.join(root,'data/tabular_data/{}/detection.csv'.format(city))\n",
    "    detection_result_df=pd.read_csv(detection_result)\n",
    "    detection_result_df=detection_result_df.rename(columns={'Unnamed: 0':'pano_id'})\n",
    "    detection_result_df_transport=detection_result_df[['pano_id','person','bicycle','car','motorcycle','bus','train','truck']]\n",
    "    \n",
    "    # location data\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['geometry']=location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    location_gdf=gpd.GeoDataFrame(location_df, geometry='geometry')\n",
    "    location_gdf=location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    location_gdf=location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # merge location to detection result to convert it to gdf\n",
    "    location_gdf['panoId']='pano='+location_gdf['panoId']\n",
    "    detection_result_df_transport=pd.merge(detection_result_df_transport,\n",
    "                                           location_gdf[['panoId','geometry']],\n",
    "                                           left_on='pano_id',\n",
    "                                           right_on='panoId',\n",
    "                                           how='left')\n",
    "    detection_gdf=gpd.GeoDataFrame(detection_result_df_transport, geometry='geometry')\n",
    "    \n",
    "    # create 500m buffer around img_location_gdf\n",
    "    buffer_500m = location_gdf.copy()\n",
    "    buffer_500m[\"geometry\"]=location_gdf.geometry.buffer(500)\n",
    "    # coduct spatial join to extract points that intersect with the 500m buffer\n",
    "    detection_in_500m = gpd.sjoin(buffer_500m, detection_gdf, how=\"left\", op='intersects')\n",
    "    \n",
    "    # group by panoId to get sum\n",
    "    detection_in_500m_grouped=detection_in_500m.groupby(['pano_id']).\\\n",
    "                              agg({'person':'sum',\n",
    "                                   'bicycle':'sum',\n",
    "                                   'car':'sum',\n",
    "                                   'motorcycle':'sum',\n",
    "                                   'bus':'sum',\n",
    "                                   'train':'sum',\n",
    "                                   'truck':'sum'}).\\\n",
    "                              reset_index()\n",
    "    detection_in_500m_grouped['total']=detection_in_500m_grouped.iloc[:,1:].sum(axis=1)\n",
    "    detection_in_500m_grouped['bicycle_share']=detection_in_500m_grouped['bicycle']/detection_in_500m_grouped['total']\n",
    "    detection_in_500m_grouped['bicycle_share']=detection_in_500m_grouped['bicycle_share'].fillna(0)\n",
    "    detection_in_500m_grouped=detection_in_500m_grouped[['pano_id','bicycle_share']]\n",
    "    print(detection_in_500m_grouped)\n",
    "    detection_in_500m_grouped.to_csv(os.path.join(root,'data/tabular_data/{}/bicycle_share.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-immigration",
   "metadata": {},
   "source": [
    "### No. of Vehicles\n",
    "    0. Filter detection data to only have transport related columns\n",
    "    1. Merge locational data with the detection data\n",
    "    2. Copy the dataframe\n",
    "    3. Conduct spatial aggregation with 500m buffer between the original and the copy\n",
    "    4. Group by pano_id\n",
    "    5. Calculate the sum of all the columns\n",
    "    6. Calculate the cycling mode share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# go through city_list\n",
    "for city in city_list:\n",
    "    # detection result\n",
    "    detection_result=os.path.join(root,'data/tabular_data/{}/detection.csv'.format(city))\n",
    "    detection_result_df=pd.read_csv(detection_result)\n",
    "    detection_result_df=detection_result_df.rename(columns={'Unnamed: 0':'pano_id'})\n",
    "    detection_result_df_transport=detection_result_df[['pano_id','car','motorcycle','bus','truck']]\n",
    "    \n",
    "    # location data\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['geometry']=location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    location_gdf=gpd.GeoDataFrame(location_df, geometry='geometry')\n",
    "    location_gdf=location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    location_gdf=location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # merge location to detection result to convert it to gdf\n",
    "    location_gdf['panoId']='pano='+location_gdf['panoId']\n",
    "    detection_result_df_transport=pd.merge(detection_result_df_transport,\n",
    "                                           location_gdf[['panoId','geometry']],\n",
    "                                           left_on='pano_id',\n",
    "                                           right_on='panoId',\n",
    "                                           how='left')\n",
    "    detection_gdf=gpd.GeoDataFrame(detection_result_df_transport, geometry='geometry')\n",
    "    \n",
    "    # create 500m buffer around img_location_gdf\n",
    "    buffer_500m = location_gdf.copy()\n",
    "    buffer_500m[\"geometry\"]=location_gdf.geometry.buffer(500)\n",
    "    # coduct spatial join to extract points that intersect with the 500m buffer\n",
    "    detection_in_500m = gpd.sjoin(buffer_500m, detection_gdf, how=\"left\", op='intersects')\n",
    "    \n",
    "    # group by panoId to get sum\n",
    "    detection_in_500m_grouped=detection_in_500m.groupby(['pano_id']).\\\n",
    "                              agg({'car':'sum',\n",
    "                                   'motorcycle':'sum',\n",
    "                                   'bus':'sum',\n",
    "                                   'truck':'sum'}).\\\n",
    "                              reset_index()\n",
    "    detection_in_500m_grouped['no_of_vehicles']=detection_in_500m_grouped.iloc[:,1:].sum(axis=1)\n",
    "    detection_in_500m_grouped=detection_in_500m_grouped[['pano_id','no_of_vehicles']]\n",
    "    detection_in_500m_grouped.to_csv(os.path.join(root,'data/tabular_data/{}/no_of_vehicles.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-company",
   "metadata": {},
   "source": [
    "### stop signs/traffic lights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "extra-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# go through city_list\n",
    "for city in city_list:\n",
    "    # detection result\n",
    "    detection_result=os.path.join(root,'data/tabular_data/{}/detection.csv'.format(city))\n",
    "    detection_result_df=pd.read_csv(detection_result)\n",
    "    detection_result_df=detection_result_df.rename(columns={'Unnamed: 0':'pano_id'})\n",
    "    detection_result_df_trffic_light_stop_sign=detection_result_df[['pano_id','traffic light','stop sign']]\n",
    "    \n",
    "    # location data\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['geometry']=location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    location_gdf=gpd.GeoDataFrame(location_df, geometry='geometry')\n",
    "    location_gdf=location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    location_gdf=location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # merge location to detection result to convert it to gdf\n",
    "    location_gdf['panoId']='pano='+location_gdf['panoId']\n",
    "    detection_result_df_trffic_light_stop_sign=pd.merge(detection_result_df_trffic_light_stop_sign,\n",
    "                                           location_gdf[['panoId','geometry']],\n",
    "                                           left_on='pano_id',\n",
    "                                           right_on='panoId',\n",
    "                                           how='left')\n",
    "    detection_gdf=gpd.GeoDataFrame(detection_result_df_trffic_light_stop_sign, geometry='geometry')\n",
    "    \n",
    "    # create 100m buffer around img_location_gdf\n",
    "    buffer_100m = location_gdf.copy()\n",
    "    buffer_100m[\"geometry\"]=location_gdf.geometry.buffer(100)\n",
    "    # coduct spatial join to extract points that intersect with the 100m buffer\n",
    "    detection_in_100m = gpd.sjoin(buffer_100m, detection_gdf, how=\"left\", op='intersects')\n",
    "    \n",
    "    # group by panoId to get sum\n",
    "    detection_in_100m_grouped=detection_in_100m.groupby(['pano_id']).\\\n",
    "                              agg({'traffic light':'sum',\n",
    "                                   'stop sign':'sum'\n",
    "                                   }).\\\n",
    "                              reset_index()\n",
    "    \n",
    "    # create a column for traffic light stop sign indicator\n",
    "    detection_in_100m_grouped['traffic_light_stop_sign']=0\n",
    "    cond_list=[(detection_in_100m_grouped['stop sign']>0)|(detection_in_100m_grouped['traffic light']>0)]\n",
    "    choice_list=[1]\n",
    "    detection_in_100m_grouped['traffic_light_stop_sign']=np.select(cond_list,choice_list)\n",
    "    detection_in_100m_grouped.to_csv(os.path.join(root,'data/tabular_data/{}/traffic_light_stop_sign.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-order",
   "metadata": {},
   "source": [
    "## OSM\n",
    "### 500m buffer\n",
    "- Number of intersection with lights\n",
    "- Number of intersection without lights\n",
    "- Number of cul-de-sac / dead-end\n",
    "- Number of points of interest\n",
    "- Number of transit facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-husband",
   "metadata": {},
   "source": [
    "#### Number of intersection with lights & Number of intersection without lights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-begin",
   "metadata": {},
   "source": [
    "##### get crossing with and without traffic signals from highway nodes in OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# parameter for osmnx\n",
    "tags={'highway':['crossing','traffic_signal']}\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    crossing=ox.geometries_from_place(city, tags)\n",
    "    crossing=crossing.loc[crossing['element_type']=='node']\n",
    "    crossing=crossing.drop(['nodes'],axis=1)\n",
    "    crossing.to_file(os.path.join(root,'data/network/{}_street_network/{}_crossing.shp').format(city,city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in tqdm.tqdm(city_list):\n",
    "    # get the remaining images' locations\n",
    "    img_location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    #load the csv file and convert it to gdf\n",
    "    img_location_df=pd.read_csv(img_location)\n",
    "    img_location_df['geometry']=img_location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    img_location_gdf=gpd.GeoDataFrame(img_location_df, geometry='geometry')\n",
    "    img_location_gdf=img_location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    img_location_gdf=img_location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # get crossing shp\n",
    "    crossing=os.path.join(root,'data/network/{}_street_network/{}_crossing.shp'.format(city,city))\n",
    "    crossing_gdf=gpd.read_file(crossing)\n",
    "    #convert to Pseudo-Mercator\n",
    "    crossing_gdf=crossing_gdf.to_crs(\"EPSG:3857\")\n",
    "\n",
    "    # create 500m buffer around img_location_gdf\n",
    "    buffer_500m = img_location_gdf.copy()\n",
    "    buffer_500m[\"geometry\"]=img_location_gdf.geometry.buffer(500)\n",
    "    # coduct spatial join to extract points that intersect with the 500m buffer\n",
    "    nodes_in_500m = gpd.sjoin(crossing_gdf, buffer_500m, how=\"left\", op='intersects')\n",
    "    intersections_with_traffic_lights=nodes_in_500m.loc[nodes_in_500m['crossing']=='traffic_signals']\n",
    "    intersections_without_traffic_lights=nodes_in_500m.loc[(nodes_in_500m['crossing']!='traffic_signals')&(nodes_in_500m['crossing']!='')]\n",
    "    \n",
    "    # drop null rows in panoId column\n",
    "    intersections_with_traffic_lights=intersections_with_traffic_lights.dropna(subset=['panoId'])\n",
    "    intersections_without_traffic_lights=intersections_without_traffic_lights.dropna(subset=['panoId'])\n",
    "    # group by panoId to get counts of points within their buffers\n",
    "    intersections_with_traffic_lights_count=intersections_with_traffic_lights.groupby(['panoId']).size().reset_index(name='counts')\n",
    "    intersections_without_traffic_lights_count=intersections_without_traffic_lights.groupby(['panoId']).size().reset_index(name='counts')\n",
    "    # left join the count to the img_location_df\n",
    "    intersections_with_traffic_lights_count_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                                                intersections_with_traffic_lights_count,\n",
    "                                                                on='panoId',\n",
    "                                                                how='left'\n",
    "                                                                )\n",
    "    intersections_without_traffic_lights_count_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                                                intersections_without_traffic_lights_count,\n",
    "                                                                on='panoId',\n",
    "                                                                how='left'\n",
    "                                                                )\n",
    "    # fill na with 0\n",
    "    intersections_with_traffic_lights_count_left_joined=intersections_with_traffic_lights_count_left_joined.fillna(0)\n",
    "    intersections_without_traffic_lights_count_left_joined=intersections_without_traffic_lights_count_left_joined.fillna(0)\n",
    "    # add 'pano=' to the panoId coulumn\n",
    "    intersections_with_traffic_lights_count_left_joined['panoId']='pano='+intersections_with_traffic_lights_count_left_joined['panoId']\n",
    "    intersections_without_traffic_lights_count_left_joined['panoId']='pano='+intersections_without_traffic_lights_count_left_joined['panoId']\n",
    "    # save it to tabular_data folder\n",
    "    print(intersections_with_traffic_lights_count_left_joined)\n",
    "    print(intersections_without_traffic_lights_count_left_joined)\n",
    "    intersections_with_traffic_lights_count_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/intersections_with_traffic_lights.csv'.format(city)))\n",
    "    intersections_without_traffic_lights_count_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/intersections_without_traffic_lights.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-stack",
   "metadata": {},
   "source": [
    "#### Number of cul-de-sac / dead-end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-leather",
   "metadata": {},
   "source": [
    "##### get crossing with and without traffic signals from highway nodes in OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# parameter for osmnx\n",
    "tags={'highway':['turning_circle','turning_loop']}\n",
    "# go through cities in the list\n",
    "for city in tqdm.tqdm(city_list):\n",
    "    cul_de_sac=ox.geometries_from_place(city, tags)\n",
    "    cul_de_sac.to_file(os.path.join(root,'data/network/{}_street_network/{}_cul_de_sac.shp').format(city,city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of cul-de-sac / dead-end\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in city_list:\n",
    "    # get the remaining images' locations\n",
    "    img_location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    #load the csv file and convert it to gdf\n",
    "    img_location_df=pd.read_csv(img_location)\n",
    "    img_location_df['geometry']=img_location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    img_location_gdf=gpd.GeoDataFrame(img_location_df, geometry='geometry')\n",
    "    img_location_gdf=img_location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    img_location_gdf=img_location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # get the edge file from network folder\n",
    "    cul_de_sac=os.path.join(root,'data/network/{}_street_network/{}_cul_de_sac.shp'.format(city,city))\n",
    "    cul_de_sac_gdf=gpd.read_file(cul_de_sac)\n",
    "    # convert to Pseudo-Mercator\n",
    "    cul_de_sac_gdf=cul_de_sac_gdf.to_crs(\"EPSG:3857\")\n",
    "    print(cul_de_sac_gdf)\n",
    "    \n",
    "    # create 500m buffer around img_location_gdf\n",
    "    buffer_500m = img_location_gdf.copy()\n",
    "    buffer_500m[\"geometry\"]=img_location_gdf.geometry.buffer(500)\n",
    "    # coduct spatial join to extract points that intersect with the 500m buffer\n",
    "    cul_de_sac_500m = gpd.sjoin(cul_de_sac_gdf, buffer_500m, how=\"left\", op='intersects')\n",
    "    print(cul_de_sac)\n",
    "    \n",
    "    # drop null rows in panoId column\n",
    "    cul_de_sac_500m=cul_de_sac_500m.dropna(subset=['panoId'])\n",
    "    # group by panoId to get counts of points within their buffers\n",
    "    cul_de_sac_500m_count=cul_de_sac_500m.groupby(['panoId']).size().reset_index(name='counts')\n",
    "    # left join the count to the img_location_df\n",
    "    cul_de_sac_500m_count_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                               cul_de_sac_500m_count,\n",
    "                                               on='panoId',\n",
    "                                               how='left'\n",
    "                                              )\n",
    "    # fill na with 0\n",
    "    cul_de_sac_500m_count_left_joined=cul_de_sac_500m_count_left_joined.fillna(0)\n",
    "    # add 'pano=' to the panoId coulumn\n",
    "    cul_de_sac_500m_count_left_joined['panoId']='pano='+cul_de_sac_500m_count_left_joined['panoId']\n",
    "    # save it to tabular_data folder\n",
    "    print(cul_de_sac_500m_count_left_joined)\n",
    "    cul_de_sac_500m_count_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/cul_de_sac.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-writing",
   "metadata": {},
   "source": [
    "#### Number of points of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-south",
   "metadata": {},
   "source": [
    "##### get amenity of points in OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# parameter for osmnx\n",
    "tags={'amenity':True}\n",
    "\n",
    "# go through cities in the list\n",
    "for city in tqdm.tqdm(city_list):\n",
    "    # get pois\n",
    "    pois=ox.geometries_from_place(city, tags)\n",
    "    # only select point features and drop 'nodes' column\n",
    "    pois=pois.loc[pois['element_type']=='node']\n",
    "    pois=pois.drop(['nodes'],axis=1)\n",
    "    pois=pois[['unique_id','osmid','name','amenity','geometry']]\n",
    "    pois.to_file(os.path.join(root,'data/network/{}_street_network/{}_poi.shp').format(city,city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of pois\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in city_list:\n",
    "    # get the remaining images' locations\n",
    "    img_location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    #load the csv file and convert it to gdf\n",
    "    img_location_df=pd.read_csv(img_location)\n",
    "    img_location_df['geometry']=img_location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    img_location_gdf=gpd.GeoDataFrame(img_location_df, geometry='geometry')\n",
    "    img_location_gdf=img_location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    img_location_gdf=img_location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # get the poi file from network folder\n",
    "    pois=os.path.join(root,'data/network/{}_street_network/{}_poi.shp'.format(city,city))\n",
    "    pois_gdf=gpd.read_file(pois)\n",
    "    # convert to Pseudo-Mercator\n",
    "    pois_gdf=pois_gdf.to_crs(\"EPSG:3857\")\n",
    "    print(pois_gdf)\n",
    "    \n",
    "    # create 500m buffer around img_location_gdf\n",
    "    buffer_500m = img_location_gdf.copy()\n",
    "    buffer_500m[\"geometry\"]=img_location_gdf.geometry.buffer(500)\n",
    "    # coduct spatial join to extract points that intersect with the 500m buffer\n",
    "    pois_in_500m = gpd.sjoin(pois_gdf, buffer_500m, how=\"left\", op='intersects')\n",
    "    print(pois_in_500m)\n",
    "    \n",
    "    # drop null rows in panoId column\n",
    "    pois_in_500m=pois_in_500m.dropna(subset=['panoId'])\n",
    "    # group by panoId to get counts of points within their buffers\n",
    "    pois_in_500m_count=pois_in_500m.groupby(['panoId']).size().reset_index(name='counts')\n",
    "    # left join the count to the img_location_df\n",
    "    pois_in_500m_count_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                            pois_in_500m_count,\n",
    "                                            on='panoId',\n",
    "                                            how='left'\n",
    "                                           )\n",
    "    # fill na with 0\n",
    "    pois_in_500m_count_left_joined=pois_in_500m_count_left_joined.fillna(0)\n",
    "    # add 'pano=' to the panoId coulumn\n",
    "    pois_in_500m_count_left_joined['panoId']='pano='+pois_in_500m_count_left_joined['panoId']\n",
    "    # save it to tabular_data folder\n",
    "    print(pois_in_500m_count_left_joined)\n",
    "    pois_in_500m_count_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/poi.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-usage",
   "metadata": {},
   "source": [
    "#### Number of transit facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-anniversary",
   "metadata": {},
   "source": [
    "##### get MRT (Train) stations from OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# parameter for osmnx\n",
    "tags={'railway':'stop'}\n",
    "\n",
    "# go through cities in the list\n",
    "for city in tqdm.tqdm(city_list):\n",
    "    # get transit\n",
    "    transit=ox.geometries_from_place(city, tags)\n",
    "    \n",
    "    # drop duplicates\n",
    "    transit=transit.drop_duplicates(subset=['name'])\n",
    "    print(transit)\n",
    "    transit.to_file(os.path.join(root,'data/network/{}_street_network/{}_transit.shp').format(city,city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of transit facilities\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in city_list:\n",
    "    # get the remaining images' locations\n",
    "    img_location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    #load the csv file and convert it to gdf\n",
    "    img_location_df=pd.read_csv(img_location)\n",
    "    img_location_df['geometry']=img_location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    img_location_gdf=gpd.GeoDataFrame(img_location_df, geometry='geometry')\n",
    "    img_location_gdf=img_location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    img_location_gdf=img_location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # get the transit file from network folder\n",
    "    transit=os.path.join(root,'data/network/{}_street_network/{}_transit.shp'.format(city,city))\n",
    "    transit_gdf=gpd.read_file(transit)\n",
    "    # convert to Pseudo-Mercator\n",
    "    transit_gdf=transit_gdf.to_crs(\"EPSG:3857\")\n",
    "    print(transit_gdf)\n",
    "    \n",
    "    # create 500m buffer around img_location_gdf\n",
    "    buffer_500m = img_location_gdf.copy()\n",
    "    buffer_500m[\"geometry\"]=img_location_gdf.geometry.buffer(500)\n",
    "    # coduct spatial join to extract points that intersect with the 500m buffer\n",
    "    transit_in_500m = gpd.sjoin(transit_gdf, buffer_500m, how=\"left\", op='intersects')\n",
    "    print(pois_in_500m)\n",
    "    \n",
    "    # drop null rows in panoId column\n",
    "    transit_in_500m=transit_in_500m.dropna(subset=['panoId'])\n",
    "    # group by panoId to get counts of points within their buffers\n",
    "    transit_in_500m_count=transit_in_500m.groupby(['panoId']).size().reset_index(name='counts')\n",
    "    # left join the count to the img_location_df\n",
    "    transit_in_500m_count_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                            transit_in_500m_count,\n",
    "                                            on='panoId',\n",
    "                                            how='left'\n",
    "                                           )\n",
    "    # fill na with 0\n",
    "    transit_in_500m_count_left_joined=transit_in_500m_count_left_joined.fillna(0)\n",
    "    # add 'pano=' to the panoId coulumn\n",
    "    transit_in_500m_count_left_joined['panoId']='pano='+transit_in_500m_count_left_joined['panoId']\n",
    "    # save it to tabular_data folder\n",
    "    print(transit_in_500m_count_left_joined)\n",
    "    transit_in_500m_count_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/transit.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-place",
   "metadata": {},
   "source": [
    "### 100m buffer\n",
    "- Type of road\n",
    "- Type of pavement (paved vs unpaved)\n",
    "- Road width\n",
    "- Number of traffic lanes\n",
    "- Presence of off-street parking lot spaces\n",
    "- Number of speed bumps / choker / roundabout\n",
    "- Vehicular speed limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-scoop",
   "metadata": {},
   "source": [
    "#### Type of road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of type of road\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in city_list:\n",
    "    # get the remaining images' locations\n",
    "    img_location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    #load the csv file and convert it to gdf\n",
    "    img_location_df=pd.read_csv(img_location)\n",
    "    img_location_df['geometry']=img_location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    img_location_gdf=gpd.GeoDataFrame(img_location_df, geometry='geometry')\n",
    "    img_location_gdf=img_location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    img_location_gdf=img_location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # get the transit file from network folder\n",
    "    road=os.path.join(root,'data/network/{}_street_network/edges.shp'.format(city))\n",
    "    road_gdf=gpd.read_file(road)\n",
    "    # convert to Pseudo-Mercator\n",
    "    road_gdf=road_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # create 100m buffer around img_location_gdf\n",
    "    buffer_100m = img_location_gdf.copy()\n",
    "    buffer_100m[\"geometry\"]=img_location_gdf.geometry.buffer(100)\n",
    "    # coduct spatial join to extract points that intersect with the 100m buffer\n",
    "    road_in_100m = gpd.sjoin(road_gdf, buffer_100m, how=\"left\", op='intersects')\n",
    "    \n",
    "    # drop null rows in panoId column\n",
    "    road_in_100m=road_in_100m.dropna(subset=['panoId'])\n",
    "    # calculate the lengh of each line feature\n",
    "    road_in_100m['length']=road_in_100m['geometry'].length\n",
    "    # convert type of road ('highway' column) to numeric values\n",
    "    non_others_list=['service','track','primary','primary_link',\n",
    "                     'secondary','secondary_link','road','tertiary',\n",
    "                     'tertiary_link','residential','living_street','pedestrian',\n",
    "                     'footway','path','cycleway'\n",
    "                    ]\n",
    "    cond_list=[(road_in_100m['highway']=='service')|(road_in_100m['highway']=='track'),\n",
    "               (road_in_100m['highway']=='primary')|(road_in_100m['highway']=='primary_link'),\n",
    "               (road_in_100m['highway']=='secondary')|(road_in_100m['highway']=='secondary_link'),\n",
    "               road_in_100m['highway']=='road',\n",
    "               (road_in_100m['highway']=='tertiary')|(road_in_100m['highway']=='tertiary_link'),\n",
    "               (road_in_100m['highway']=='residential')|(road_in_100m['highway']=='living_street')|\\\n",
    "               (road_in_100m['highway']=='pedestrian')|(road_in_100m['highway']=='footway')|\\\n",
    "               (road_in_100m['highway']=='path'),\n",
    "               road_in_100m['highway']=='cycleway',\n",
    "               ~road_in_100m['highway'].isin(non_others_list)\n",
    "              ]\n",
    "    choice_list=[0.1,0.2,0.4,0.5,0.6,0.8,1,0]\n",
    "    road_in_100m['road_type_num'] = np.select(cond_list, choice_list)\n",
    "    road_in_100m['road_type_score']=road_in_100m['road_type_num']*road_in_100m['length']\n",
    "    print(road_in_100m)\n",
    "    # group by panoId to get sum\n",
    "    road_in_100m_grouped=road_in_100m.groupby(['panoId']).agg({'length':'sum','road_type_score':'sum'}).reset_index()\n",
    "    road_in_100m_grouped['road_type_mean']=road_in_100m_grouped['road_type_score']/road_in_100m_grouped['length']\n",
    "    print(road_in_100m_grouped)\n",
    "    # left join the count to the img_location_df\n",
    "    road_in_100m_grouped_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                            road_in_100m_grouped,\n",
    "                                            on='panoId',\n",
    "                                            how='left'\n",
    "                                           )\n",
    "    # fill na with 0\n",
    "    road_in_100m_grouped_left_joined=road_in_100m_grouped_left_joined.fillna(0)\n",
    "    # add 'pano=' to the panoId coulumn\n",
    "    road_in_100m_grouped_left_joined['panoId']='pano='+road_in_100m_grouped_left_joined['panoId']\n",
    "    # save it to tabular_data folder\n",
    "    print(road_in_100m_grouped_left_joined)\n",
    "    road_in_100m_grouped_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/road_type.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-characterization",
   "metadata": {},
   "source": [
    "#### Type of pavement (paved vs unpaved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-stand",
   "metadata": {},
   "source": [
    "##### get surface from OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# parameter for osmnx\n",
    "tags={'surface':True}\n",
    "\n",
    "# go through cities in the list\n",
    "for city in tqdm.tqdm(city_list):\n",
    "    # get surface\n",
    "    surface=ox.geometries_from_place(city, tags)\n",
    "    # only select line features and drop 'nodes' column\n",
    "    surface=surface.loc[surface['element_type']=='way']\n",
    "    surface=surface.drop(['nodes'],axis=1)\n",
    "    # keep only unique_id, osmid, surface, geometry\n",
    "    surface=surface[['unique_id','osmid','surface','geometry']]\n",
    "    print(surface)\n",
    "    surface.to_file(os.path.join(root,'data/network/{}_street_network/{}_surface.shp').format(city,city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of pavement\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in city_list:\n",
    "    # get the remaining images' locations\n",
    "    img_location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    #load the csv file and convert it to gdf\n",
    "    img_location_df=pd.read_csv(img_location)\n",
    "    img_location_df['geometry']=img_location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    img_location_gdf=gpd.GeoDataFrame(img_location_df, geometry='geometry')\n",
    "    img_location_gdf=img_location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    img_location_gdf=img_location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # get the surface file from network folder\n",
    "    road=os.path.join(root,'data/network/{}_street_network/{}_surface.shp'.format(city,city))\n",
    "    road_gdf=gpd.read_file(road)\n",
    "    # convert to Pseudo-Mercator\n",
    "    road_gdf=road_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # create 100m buffer around img_location_gdf\n",
    "    buffer_100m = img_location_gdf.copy()\n",
    "    buffer_100m[\"geometry\"]=img_location_gdf.geometry.buffer(100)\n",
    "    # coduct spatial join to extract points that intersect with the 100m buffer\n",
    "    road_in_100m = gpd.sjoin(road_gdf, buffer_100m, how=\"left\", op='intersects')\n",
    "    \n",
    "    # drop null rows in panoId column\n",
    "    road_in_100m=road_in_100m.dropna(subset=['panoId'])\n",
    "    # calculate the lengh of each line feature\n",
    "    road_in_100m['length']=road_in_100m['geometry'].length\n",
    "    # convert type of pavement ('surface' column) to numeric values\n",
    "    non_others_list=['unhewn_cobblestone','cobblestone','sett','metal',\n",
    "                     'wood','paved','concrete:lanes','concrete:plates',\n",
    "                     'paving_stones','asphalt','concrete'\n",
    "                    ]\n",
    "    cond_list=[(road_in_100m['surface']=='unhewn_cobblestone')|(road_in_100m['surface']=='cobblestone'),\n",
    "               (road_in_100m['surface']=='sett')|(road_in_100m['surface']=='metal')|(road_in_100m['surface']=='wood'),\n",
    "               road_in_100m['surface']=='paved',\n",
    "               (road_in_100m['surface']=='concrete:lanes')|(road_in_100m['surface']=='concrete:plates')|(road_in_100m['surface']=='paving_stones'),\n",
    "               (road_in_100m['surface']=='asphalt')|(road_in_100m['surface']=='concrete'),\n",
    "               ~road_in_100m['surface'].isin(non_others_list)\n",
    "              ]\n",
    "    choice_list=[0.2,0.4,0.5,0.6,1,0]\n",
    "    road_in_100m['surface_type_num'] = np.select(cond_list, choice_list)\n",
    "    road_in_100m['surface_type_score']=road_in_100m['surface_type_num']*road_in_100m['length']\n",
    "    print(road_in_100m)\n",
    "    # group by panoId to get sum\n",
    "    road_in_100m_grouped=road_in_100m.groupby(['panoId']).agg({'length':'sum','surface_type_score':'sum'}).reset_index()\n",
    "    road_in_100m_grouped['surface_type_mean']=road_in_100m_grouped['surface_type_score']/road_in_100m_grouped['length']\n",
    "    print(road_in_100m_grouped)\n",
    "    # left join the count to the img_location_df\n",
    "    road_in_100m_grouped_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                            road_in_100m_grouped,\n",
    "                                            on='panoId',\n",
    "                                            how='left'\n",
    "                                           )\n",
    "    # fill na with 0\n",
    "    road_in_100m_grouped_left_joined=road_in_100m_grouped_left_joined.fillna(0)\n",
    "    # add 'pano=' to the panoId coulumn\n",
    "    road_in_100m_grouped_left_joined['panoId']='pano='+road_in_100m_grouped_left_joined['panoId']\n",
    "    # save it to tabular_data folder\n",
    "    print(road_in_100m_grouped_left_joined)\n",
    "    road_in_100m_grouped_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/surface_type.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-presence",
   "metadata": {},
   "source": [
    "#### Road width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Road width\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in city_list:\n",
    "    # get the remaining images' locations\n",
    "    img_location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    #load the csv file and convert it to gdf\n",
    "    img_location_df=pd.read_csv(img_location)\n",
    "    img_location_df['geometry']=img_location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    img_location_gdf=gpd.GeoDataFrame(img_location_df, geometry='geometry')\n",
    "    img_location_gdf=img_location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    img_location_gdf=img_location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # get the width file from network folder\n",
    "    road=os.path.join(root,'data/network/{}_street_network/edges.shp'.format(city))\n",
    "    road_gdf=gpd.read_file(road)\n",
    "    # convert to Pseudo-Mercator\n",
    "    road_gdf=road_gdf.to_crs(\"EPSG:3857\")\n",
    "    # drop lines without width info\n",
    "    road_gdf=road_gdf.dropna(subset=['width'])\n",
    "    road_gdf=road_gdf.loc[road_gdf['width'].str.isdigit()]\n",
    "    road_gdf['width']=road_gdf['width'].astype(int)\n",
    "    print(road_gdf)\n",
    "    \n",
    "    # create 100m buffer around img_location_gdf\n",
    "    buffer_100m = img_location_gdf.copy()\n",
    "    buffer_100m[\"geometry\"]=img_location_gdf.geometry.buffer(100)\n",
    "    # coduct spatial join to extract points that intersect with the 100m buffer\n",
    "    road_in_100m = gpd.sjoin(road_gdf, buffer_100m, how=\"left\", op='intersects')\n",
    "    \n",
    "    # drop null rows in panoId column\n",
    "    road_in_100m=road_in_100m.dropna(subset=['panoId'])\n",
    "    # calculate the lengh of each line feature\n",
    "    road_in_100m['length']=road_in_100m['geometry'].length\n",
    "    # convert road width ('width' column) to numeric values\n",
    "    cond_list=[road_in_100m['width']<=10,\n",
    "               road_in_100m['width']>10\n",
    "              ]\n",
    "    choice_list=[road_in_100m['width']/10,1]\n",
    "    road_in_100m['road_width_num'] = np.select(cond_list, choice_list)\n",
    "    road_in_100m['road_width_score']=road_in_100m['road_width_num']*road_in_100m['length']\n",
    "    print(road_in_100m)\n",
    "    # group by panoId to get sum\n",
    "    road_in_100m_grouped=road_in_100m.groupby(['panoId']).agg({'length':'sum','road_width_score':'sum'}).reset_index()\n",
    "    road_in_100m_grouped['road_width_mean']=road_in_100m_grouped['road_width_score']/road_in_100m_grouped['length']\n",
    "    print(road_in_100m_grouped)\n",
    "    # left join the count to the img_location_df\n",
    "    road_in_100m_grouped_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                            road_in_100m_grouped,\n",
    "                                            on='panoId',\n",
    "                                            how='left'\n",
    "                                           )\n",
    "    # fill na with 0\n",
    "    road_in_100m_grouped_left_joined=road_in_100m_grouped_left_joined.fillna(0)\n",
    "    # add 'pano=' to the panoId coulumn\n",
    "    road_in_100m_grouped_left_joined['panoId']='pano='+road_in_100m_grouped_left_joined['panoId']\n",
    "    # save it to tabular_data folder\n",
    "    print(road_in_100m_grouped_left_joined)\n",
    "    road_in_100m_grouped_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/road_width.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-mexico",
   "metadata": {},
   "source": [
    "#### Number of traffic lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic lanes\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in tqdm.tqdm(city_list):\n",
    "    # get the remaining images' locations\n",
    "    img_location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    #load the csv file and convert it to gdf\n",
    "    img_location_df=pd.read_csv(img_location)\n",
    "    img_location_df['geometry']=img_location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    img_location_gdf=gpd.GeoDataFrame(img_location_df, geometry='geometry')\n",
    "    img_location_gdf=img_location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    img_location_gdf=img_location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # get the road file from network folder\n",
    "    road=os.path.join(root,'data/network/{}_street_network/edges.shp'.format(city))\n",
    "    road_gdf=gpd.read_file(road)\n",
    "    # convert to Pseudo-Mercator\n",
    "    road_gdf=road_gdf.to_crs(\"EPSG:3857\")\n",
    "    # drop na\n",
    "    road_gdf['lanes']=road_gdf['lanes'].fillna('0')\n",
    "    # drop cycle way\n",
    "    road_gdf=road_gdf.loc[road_gdf['highway']!='cycleway']\n",
    "    road_gdf=road_gdf.loc[road_gdf['lanes'].str.isdigit()]\n",
    "    road_gdf['lanes']=road_gdf['lanes'].astype(int)\n",
    "    \n",
    "    # create 100m buffer around img_location_gdf\n",
    "    buffer_100m = img_location_gdf.copy()\n",
    "    buffer_100m[\"geometry\"]=img_location_gdf.geometry.buffer(100)\n",
    "    # coduct spatial join to extract points that intersect with the 100m buffer\n",
    "    road_in_100m = gpd.sjoin(road_gdf, buffer_100m, how=\"left\", op='intersects')\n",
    "    \n",
    "    # drop null rows in panoId column\n",
    "    road_in_100m=road_in_100m.dropna(subset=['panoId'])\n",
    "    # calculate the lengh of each line feature\n",
    "    road_in_100m['length']=road_in_100m['geometry'].length\n",
    "    # convert road lanes ('lanes' column) to numeric values\n",
    "    cond_list=[(road_in_100m['lanes']==1)|(road_in_100m['lanes']==0),\n",
    "               road_in_100m['lanes']==2,\n",
    "               road_in_100m['lanes']==3,\n",
    "               road_in_100m['lanes']==4,\n",
    "               road_in_100m['lanes']>=5\n",
    "              ]\n",
    "    choice_list=[1,0.75,0.5,0.25,0]\n",
    "    road_in_100m['traffic_lane_num'] = np.select(cond_list, choice_list)\n",
    "    road_in_100m['traffic_lane_score']=road_in_100m['traffic_lane_num']*road_in_100m['length']\n",
    "    print(road_in_100m)\n",
    "    # group by panoId to get sum\n",
    "    road_in_100m_grouped=road_in_100m.groupby(['panoId']).agg({'length':'sum','traffic_lane_score':'sum'}).reset_index()\n",
    "    road_in_100m_grouped['traffic_lane_mean']=road_in_100m_grouped['traffic_lane_score']/road_in_100m_grouped['length']\n",
    "    print(road_in_100m_grouped)\n",
    "    # left join the count to the img_location_df\n",
    "    road_in_100m_grouped_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                            road_in_100m_grouped,\n",
    "                                            on='panoId',\n",
    "                                            how='left'\n",
    "                                           )\n",
    "    # fill na with 0\n",
    "    road_in_100m_grouped_left_joined=road_in_100m_grouped_left_joined.fillna(0)\n",
    "    # add 'pano=' to the panoId coulumn\n",
    "    road_in_100m_grouped_left_joined['panoId']='pano='+road_in_100m_grouped_left_joined['panoId']\n",
    "    # save it to tabular_data folder\n",
    "    print(road_in_100m_grouped_left_joined)\n",
    "    road_in_100m_grouped_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/traffic_lane.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-nicholas",
   "metadata": {},
   "source": [
    "#### Presence of off-street parking lot spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-cruise",
   "metadata": {},
   "source": [
    "##### get highway from OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# parameter for osmnx\n",
    "tags={'parking:lane:left':True,'parking:lane:right':True,'parking:lane:both':True}\n",
    "\n",
    "# go through cities in the list\n",
    "for city in tqdm.tqdm(city_list):\n",
    "    # get street_parking\n",
    "    street_parking=ox.geometries_from_place(city, tags)\n",
    "    \n",
    "    # only select line features and drop 'nodes' column\n",
    "    street_parking=street_parking.loc[street_parking['element_type']=='way']\n",
    "    street_parking=street_parking.drop(['nodes'],axis=1)\n",
    "    col_keep=['unique_id','osmid','name','parking:lane:left','parking:lane:right','parking:lane:both','geometry']\n",
    "    try:\n",
    "        street_parking=street_parking[col_keep]\n",
    "    except Exception as error:\n",
    "        missing_col=str(error).split(']')[0].replace('\"[','').replace(\"'\",'')\n",
    "        print(missing_col)\n",
    "        col_keep.remove(missing_col)\n",
    "        street_parking=street_parking[col_keep]\n",
    "    # only select parallel, diagonal, perpendicular, marked in parking:lane cols\n",
    "    parking_cols=[col for col in street_parking.columns if col.startswith('parking:lane')]\n",
    "    street_parking['total_score']=0\n",
    "    for parking_col in parking_cols:\n",
    "        street_parking['score_'+parking_col]=0\n",
    "        street_parking.loc[(street_parking[parking_col]=='parallel')|\\\n",
    "                           (street_parking[parking_col]=='diagonal')|\\\n",
    "                           (street_parking[parking_col]=='perpendicular')|\\\n",
    "                           (street_parking[parking_col]=='marked'),\n",
    "                            ['score_'+parking_col]]=1\n",
    "        street_parking['total_score']=street_parking['total_score']+street_parking['score_'+parking_col]\n",
    "    street_parking=street_parking.loc[street_parking['total_score']>0]\n",
    "    print(street_parking)\n",
    "    street_parking.to_file(os.path.join(root,'data/network/{}_street_network/{}_street_parking.shp').format(city,city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "placed-preserve",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:31: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 u           v  key       osmid oneway lanes   ref  \\\n",
      "0         25451929  6749812859    0    49961799   True     5   ECP   \n",
      "0         25451929  6749812859    0    49961799   True     5   ECP   \n",
      "0         25451929  6749812859    0    49961799   True     5   ECP   \n",
      "0         25451929  6749812859    0    49961799   True     5   ECP   \n",
      "1         25451929  6749812859    0   718881456   True     5   ECP   \n",
      "...            ...         ...  ...         ...    ...   ...   ...   \n",
      "313998  8336385420  8336385421    0   483606240   True  None  None   \n",
      "314007  8430404643  8430404652    0   654997378   True     3  None   \n",
      "314007  8430404643  8430404652    0   654997378   True     3  None   \n",
      "314008  8430404643  8430404652    0   821527756   True     3  None   \n",
      "314008  8430404643  8430404652    0   821527756   True     3  None   \n",
      "\n",
      "                      name_x   highway maxspeed  ...  score_pa_2 geometry_y  \\\n",
      "0         East Coast Parkway  motorway       70  ...         NaN       None   \n",
      "0         East Coast Parkway  motorway       70  ...         NaN       None   \n",
      "0         East Coast Parkway  motorway       70  ...         NaN       None   \n",
      "0         East Coast Parkway  motorway       70  ...         NaN       None   \n",
      "1         East Coast Parkway  motorway       70  ...         NaN       None   \n",
      "...                      ...       ...      ...  ...         ...        ...   \n",
      "313998  On-Road Cycling Lane  cycleway     None  ...         NaN       None   \n",
      "314007    Marine Parade Road   primary       60  ...         NaN       None   \n",
      "314007    Marine Parade Road   primary       60  ...         NaN       None   \n",
      "314008    Marine Parade Road   primary       60  ...         NaN       None   \n",
      "314008    Marine Parade Road   primary       60  ...         NaN       None   \n",
      "\n",
      "        index_right  Unnamed: 0                  panoId     panoLon   panoLat  \\\n",
      "0            4294.0      4498.0  RhTsZOurg7blcljN9NSK_A  103.868388  1.293833   \n",
      "0            2418.0      2541.0  fiEnwZhwigFdTGD9ScdN6Q  103.868186  1.294372   \n",
      "0            5441.0      5706.0  4DX7Ca-ihziK7ebGtTfiLA  103.868191  1.294395   \n",
      "0            3169.0      3324.0  GJZWSWCHauhop18uT_3oHQ  103.869063  1.294822   \n",
      "1            4294.0      4498.0  RhTsZOurg7blcljN9NSK_A  103.868388  1.293833   \n",
      "...             ...         ...                     ...         ...       ...   \n",
      "313998       2657.0      2792.0  qUTpGdBIlny9YzeBXw1DWg  103.998218  1.312781   \n",
      "314007       3087.0      3239.0  fnt_McKg4XL0x5N4moYLeQ  103.905566  1.301972   \n",
      "314007       3150.0      3304.0  jgC6O13soWN6G-22250P4Q  103.903651  1.301977   \n",
      "314008       3087.0      3239.0  fnt_McKg4XL0x5N4moYLeQ  103.905566  1.301972   \n",
      "314008       3150.0      3304.0  jgC6O13soWN6G-22250P4Q  103.903651  1.301977   \n",
      "\n",
      "        distDiff street_parking_num street_parking_score  \n",
      "0       0.000038                  1           766.083407  \n",
      "0       0.000070                  1           766.083407  \n",
      "0       0.000050                  1           766.083407  \n",
      "0       0.000144                  1           766.083407  \n",
      "1       0.000038                  1           766.083407  \n",
      "...          ...                ...                  ...  \n",
      "313998  0.000053                  1            16.815137  \n",
      "314007  0.000030                  1            38.053858  \n",
      "314007  0.000010                  1            38.053858  \n",
      "314008  0.000030                  1            38.053858  \n",
      "314008  0.000010                  1            38.053858  \n",
      "\n",
      "[360197 rows x 40 columns]\n",
      "                      panoId        length  street_parking_score  \\\n",
      "0     --H9Jq3ONyUwpVMok5YL7A   9604.180567           9604.180567   \n",
      "1     --i_iNs4NISOBiRHXFovwQ   4180.772180           3841.393763   \n",
      "2     --qpOz4Is5bD3McvvlbAEw   5480.332240           5480.332240   \n",
      "3     -15oVx_IsIVgd6MDojYaww  13327.909238          13327.909238   \n",
      "4     -19nkDsslnvvzBQLQLDyog   1890.163521           1890.163521   \n",
      "...                      ...           ...                   ...   \n",
      "5828  zwCqdoXUkxC7FClfHF8Vcw   2929.485913           2929.485913   \n",
      "5829  zwm-G8FsYAnRVAzl9V63ng   6164.128591           6164.128591   \n",
      "5830  zxUKu-FZoUA8jkI8m7R6uQ   4816.048213           4816.048213   \n",
      "5831  zz7cvtbKohuQE52EXX35Fw   5691.145934           5691.145934   \n",
      "5832  zziniKD3V2CC7tQzrDv_mA   3289.540721           3289.540721   \n",
      "\n",
      "      street_parking_mean  \n",
      "0                1.000000  \n",
      "1                0.918824  \n",
      "2                1.000000  \n",
      "3                1.000000  \n",
      "4                1.000000  \n",
      "...                   ...  \n",
      "5828             1.000000  \n",
      "5829             1.000000  \n",
      "5830             1.000000  \n",
      "5831             1.000000  \n",
      "5832             1.000000  \n",
      "\n",
      "[5833 rows x 4 columns]\n",
      "                                                 panoId        length  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ   6348.347232   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA   1450.064562   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA   1673.813864   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA   4238.337148   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw   6105.134562   \n",
      "...                                                 ...           ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...   4919.412261   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg   2213.970191   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ   4977.809348   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  30680.605921   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A   4214.781146   \n",
      "\n",
      "      street_parking_score  street_parking_mean  \n",
      "0              6348.347232             1.000000  \n",
      "1              1376.884265             0.949533  \n",
      "2              1673.813864             1.000000  \n",
      "3              4238.337148             1.000000  \n",
      "4              5840.740719             0.956693  \n",
      "...                    ...                  ...  \n",
      "5828           4919.412261             1.000000  \n",
      "5829           2118.401573             0.956834  \n",
      "5830           4977.809348             1.000000  \n",
      "5831          30680.605921             1.000000  \n",
      "5832           4214.781146             1.000000  \n",
      "\n",
      "[5833 rows x 4 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-9152749a9c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mroad_gdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# convert to Pseudo-Mercator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mroad_gdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroad_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPSG:3857\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# load street parking data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages/geopandas/geodataframe.py\u001b[0m in \u001b[0;36mto_crs\u001b[0;34m(self, crs, epsg, inplace)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0mgeom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages/geopandas/geoseries.py\u001b[0m in \u001b[0;36mto_crs\u001b[0;34m(self, crs, epsg)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \"\"\"\n\u001b[1;32m   1027\u001b[0m         return GeoSeries(\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         )\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages/geopandas/array.py\u001b[0m in \u001b[0;36mto_crs\u001b[0;34m(self, crs, epsg)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_xy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mGeometryArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages/geopandas/_vectorized.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(data, func)\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages/shapely/ops.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(func, geom)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Point'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LineString'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LinearRing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mizip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Polygon'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 shell = type(geom.exterior)(\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages/shapely/geometry/base.py\u001b[0m in \u001b[0;36m_get_coords\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;34m\"\"\"Access to geometry's coordinates (CoordinateSequence)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCoordinateSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages/shapely/geometry/base.py\u001b[0m in \u001b[0;36mis_empty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;34m\"\"\"True if the set of points in this geometry is empty, else False\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_geom\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_empty'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages/shapely/impl.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# off street parking\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in city_list:\n",
    "    # get the remaining images' locations\n",
    "    img_location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    #load the csv file and convert it to gdf\n",
    "    img_location_df=pd.read_csv(img_location)\n",
    "    img_location_df['geometry']=img_location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    img_location_gdf=gpd.GeoDataFrame(img_location_df, geometry='geometry')\n",
    "    img_location_gdf=img_location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    img_location_gdf=img_location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # get the road file from network folder\n",
    "    road=os.path.join(root,'data/network/{}_street_network/edges.shp'.format(city))\n",
    "    road_gdf=gpd.read_file(road)\n",
    "    # convert to Pseudo-Mercator\n",
    "    road_gdf=road_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # load street parking data\n",
    "    street_parking=gpd.read_file(os.path.join(root,'data/network/{}_street_network/{}_street_parking.shp').format(city,city))\n",
    "    street_parking['osmid']=street_parking['osmid'].astype(str)\n",
    "    \n",
    "    # vertically split osmid with ',' for road_gdf\n",
    "    road_gdf=road_gdf.assign(osmid=road_gdf['osmid'].str.split(',')).explode('osmid')\n",
    "    road_gdf['osmid']=road_gdf['osmid'].str.replace(\"\\\\[\",\"\").str.replace(\"\\\\]\",\"\")\n",
    "    \n",
    "    # join street parking to road by osmid\n",
    "    road_gdf=road_gdf.merge(street_parking,\n",
    "                       on='osmid',\n",
    "                       how='left'\n",
    "                      )\n",
    "    # convert road_gdf back to gdf again\n",
    "    road_gdf = gpd.GeoDataFrame(road_gdf, crs=\"EPSG:3857\", geometry='geometry_x')\n",
    "    \n",
    "    # create 100m buffer around img_location_gdf\n",
    "    buffer_100m = img_location_gdf.copy()\n",
    "    buffer_100m[\"geometry\"]=img_location_gdf.geometry.buffer(100)\n",
    "    # coduct spatial join to extract points that intersect with the 100m buffer\n",
    "    road_in_100m = gpd.sjoin(road_gdf, buffer_100m, how=\"left\", op='intersects')\n",
    "    \n",
    "    # drop null rows in panoId column\n",
    "    road_in_100m=road_in_100m.dropna(subset=['panoId'])\n",
    "    # calculate the lengh of each line feature\n",
    "    road_in_100m['length']=road_in_100m['geometry_x'].length\n",
    "    # convert street parking to numeric values\n",
    "    road_in_100m['total_scor']=road_in_100m['total_scor'].fillna(0)\n",
    "    cond_list=[road_in_100m['total_scor']>0,\n",
    "               road_in_100m['total_scor']==0]\n",
    "    choice_list=[0,1]\n",
    "    road_in_100m['street_parking_num'] = np.select(cond_list, choice_list)\n",
    "    road_in_100m['street_parking_score']=road_in_100m['street_parking_num']*road_in_100m['length']\n",
    "    print(road_in_100m)\n",
    "    # group by panoId to get sum\n",
    "    road_in_100m_grouped=road_in_100m.groupby(['panoId']).agg({'length':'sum','street_parking_score':'sum'}).reset_index()\n",
    "    road_in_100m_grouped['street_parking_mean']=road_in_100m_grouped['street_parking_score']/road_in_100m_grouped['length']\n",
    "    print(road_in_100m_grouped)\n",
    "    # left join the count to the img_location_df\n",
    "    road_in_100m_grouped_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                            road_in_100m_grouped,\n",
    "                                            on='panoId',\n",
    "                                            how='left'\n",
    "                                           )\n",
    "    # fill na with 0\n",
    "    road_in_100m_grouped_left_joined=road_in_100m_grouped_left_joined.fillna(0)\n",
    "    # add 'pano=' to the panoId coulumn\n",
    "    road_in_100m_grouped_left_joined['panoId']='pano='+road_in_100m_grouped_left_joined['panoId']\n",
    "    # save it to tabular_data folder\n",
    "    print(road_in_100m_grouped_left_joined)\n",
    "    road_in_100m_grouped_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/street_parking.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-jacket",
   "metadata": {},
   "source": [
    "#### Number of speed bumps / choker / roundabout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-ratio",
   "metadata": {},
   "source": [
    "##### get traffic_calming from OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# parameter for osmnx\n",
    "tags={'traffic_calming':True}\n",
    "\n",
    "# go through cities in the list\n",
    "for city in tqdm.tqdm(city_list):\n",
    "    # get traffic_calming\n",
    "    traffic_calming=ox.geometries_from_place(city, tags)\n",
    "    # only select line features and drop 'nodes' column\n",
    "    traffic_calming=traffic_calming.loc[traffic_calming['element_type']=='node']\n",
    "    traffic_calming=traffic_calming.drop(['nodes'],axis=1)\n",
    "    # keep only unique_id, osmid, traffic_calming, geometry\n",
    "    traffic_calming=traffic_calming[['unique_id','osmid','traffic_calming','geometry']]\n",
    "    print(traffic_calming)\n",
    "    traffic_calming.to_file(os.path.join(root,'data/network/{}_street_network/{}_traffic_calming.shp').format(city,city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "devoted-georgia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 panoId  traffic_calming\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ              0.0\n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA              0.0\n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA              0.0\n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA              0.0\n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw              0.0\n",
      "...                                                 ...              ...\n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...              0.0\n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg              0.0\n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ              0.0\n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA              0.0\n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A              0.0\n",
      "\n",
      "[5833 rows x 2 columns]\n",
      "                           panoId  traffic_calming\n",
      "0     pano=d1hU7t4QKuXqckKk29ENTQ              0.0\n",
      "1     pano=HRySgi0gFKuRD9QrQ3BdFw              0.0\n",
      "2     pano=LySKMao7KBtKwGBAST6INA              0.0\n",
      "3     pano=EjuTxRVkM6ywSKSNtLzFrg              0.0\n",
      "4     pano=pLawV8B_NY4JFfpRr8v-3A              0.0\n",
      "...                           ...              ...\n",
      "6176  pano=cvvj9FbzQNnYAXXZ32FL-A              0.0\n",
      "6177  pano=NoizYPZvzL1Q1YU8Y-HIfQ              0.0\n",
      "6178  pano=6F5q3CCcFuPTD8w1hL0VwA              0.0\n",
      "6179  pano=Xo1hWeHXGvfx6eTGzDas7Q              0.0\n",
      "6180  pano=Iq25sqICPU6LMag1xv4U-Q              0.0\n",
      "\n",
      "[6181 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Number of speed bumps / choker / roundabout\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in city_list:\n",
    "    # get the remaining images' locations\n",
    "    img_location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    #load the csv file and convert it to gdf\n",
    "    img_location_df=pd.read_csv(img_location)\n",
    "    img_location_df['geometry']=img_location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    img_location_gdf=gpd.GeoDataFrame(img_location_df, geometry='geometry')\n",
    "    img_location_gdf=img_location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    img_location_gdf=img_location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # get the road file from network folder\n",
    "    road=os.path.join(root,'data/network/{}_street_network/{}_traffic_calming.shp'.format(city,city))\n",
    "    road_gdf=gpd.read_file(road)\n",
    "    # convert to Pseudo-Mercator\n",
    "    road_gdf=road_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # create 100m buffer around img_location_gdf\n",
    "    buffer_100m = img_location_gdf.copy()\n",
    "    buffer_100m[\"geometry\"]=img_location_gdf.geometry.buffer(100)\n",
    "    # coduct spatial join to extract points that intersect with the 100m buffer\n",
    "    road_in_100m = gpd.sjoin(road_gdf, buffer_100m, how=\"left\", op='intersects')\n",
    "    \n",
    "    # drop null rows in panoId column\n",
    "    road_in_100m=road_in_100m.dropna(subset=['panoId'])\n",
    "    \n",
    "    # 1 to all the rows\n",
    "    road_in_100m['traffic_calming']=1\n",
    "    # group by panoId to get sum\n",
    "    road_in_100m_grouped=road_in_100m.groupby(['panoId']).agg({'traffic_calming':'sum'}).reset_index()\n",
    "    road_in_100m_grouped['traffic_calming']=road_in_100m_grouped['traffic_calming']\n",
    "    # left join the count to the img_location_df\n",
    "    road_in_100m_grouped_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                            road_in_100m_grouped,\n",
    "                                            on='panoId',\n",
    "                                            how='left'\n",
    "                                           )\n",
    "    # fill na with 0\n",
    "    road_in_100m_grouped_left_joined=road_in_100m_grouped_left_joined.fillna(0)\n",
    "    # add 'pano=' to the panoId coulumn\n",
    "    road_in_100m_grouped_left_joined['panoId']='pano='+road_in_100m_grouped_left_joined['panoId']\n",
    "    # save it to tabular_data folder\n",
    "    print(road_in_100m_grouped_left_joined)\n",
    "    road_in_100m_grouped_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/traffic_calming.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-display",
   "metadata": {},
   "source": [
    "#### Vehicular speed limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed limit\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "for city in city_list:\n",
    "    # get the remaining images' locations\n",
    "    img_location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    #load the csv file and convert it to gdf\n",
    "    img_location_df=pd.read_csv(img_location)\n",
    "    img_location_df['geometry']=img_location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    img_location_gdf=gpd.GeoDataFrame(img_location_df, geometry='geometry')\n",
    "    img_location_gdf=img_location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    img_location_gdf=img_location_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # get the road file from network folder\n",
    "    road=os.path.join(root,'data/network/{}_street_network/edges.shp'.format(city))\n",
    "    road_gdf=gpd.read_file(road)\n",
    "    # convert to Pseudo-Mercator\n",
    "    road_gdf=road_gdf.to_crs(\"EPSG:3857\")\n",
    "    road_gdf=road_gdf.dropna(subset=['maxspeed'])\n",
    "    # drop cycle way\n",
    "    road_gdf=road_gdf.loc[road_gdf['highway']!='cycleway']\n",
    "    road_gdf=road_gdf.loc[road_gdf['maxspeed'].str.isdigit()]\n",
    "    road_gdf['maxspeed']=road_gdf['maxspeed'].astype(int)\n",
    "    \n",
    "    # create 100m buffer around img_location_gdf\n",
    "    buffer_100m = img_location_gdf.copy()\n",
    "    buffer_100m[\"geometry\"]=img_location_gdf.geometry.buffer(100)\n",
    "    # coduct spatial join to extract points that intersect with the 100m buffer\n",
    "    road_in_100m = gpd.sjoin(road_gdf, buffer_100m, how=\"left\", op='intersects')\n",
    "    \n",
    "    # drop null rows in panoId column\n",
    "    road_in_100m=road_in_100m.dropna(subset=['panoId'])\n",
    "    # calculate the lengh of each line feature\n",
    "    road_in_100m['length']=road_in_100m['geometry'].length\n",
    "    # convert maxspeed to numeric values\n",
    "    cond_list=[road_in_100m['maxspeed']<=20,\n",
    "               (road_in_100m['maxspeed']>20)&(road_in_100m['maxspeed']<=40),\n",
    "               (road_in_100m['maxspeed']>40)&(road_in_100m['maxspeed']<=60),\n",
    "               (road_in_100m['maxspeed']>60)&(road_in_100m['maxspeed']<=80),\n",
    "               road_in_100m['maxspeed']>=5\n",
    "              ]\n",
    "    choice_list=[1,0.75,0.5,0.25,0]\n",
    "    road_in_100m['speed_limit_num'] = np.select(cond_list, choice_list)\n",
    "    road_in_100m['speed_limit_score']=road_in_100m['speed_limit_num']*road_in_100m['length']\n",
    "    print(road_in_100m)\n",
    "    # group by panoId to get sum\n",
    "    road_in_100m_grouped=road_in_100m.groupby(['panoId']).agg({'length':'sum','speed_limit_score':'sum'}).reset_index()\n",
    "    road_in_100m_grouped['speed_limit_mean']=road_in_100m_grouped['speed_limit_score']/road_in_100m_grouped['length']\n",
    "    print(road_in_100m_grouped)\n",
    "    # left join the count to the img_location_df\n",
    "    road_in_100m_grouped_left_joined=pd.merge(img_location_df[['panoId']],\n",
    "                                            road_in_100m_grouped,\n",
    "                                            on='panoId',\n",
    "                                            how='left'\n",
    "                                           )\n",
    "    # fill na with 0\n",
    "    road_in_100m_grouped_left_joined=road_in_100m_grouped_left_joined.fillna(1)\n",
    "    # add 'pano=' to the panoId coulumn\n",
    "    road_in_100m_grouped_left_joined['panoId']='pano='+road_in_100m_grouped_left_joined['panoId']\n",
    "    # save it to tabular_data folder\n",
    "    print(road_in_100m_grouped_left_joined)\n",
    "    road_in_100m_grouped_left_joined.to_csv(os.path.join(root,'data/tabular_data/{}/speed_limit.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-framework",
   "metadata": {},
   "source": [
    "## Land use\n",
    "### 500m buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-mustang",
   "metadata": {},
   "source": [
    "#### clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singapore\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "df = gpd.read_file(os.path.join(root,'data/land_use/Singapore/G_MP19_LAND_USE_PL.kml'), driver='KML')\n",
    "df['land_use']=df['Description'].str.split('<td>').str[1].str.split('</td>').str[0]\n",
    "\n",
    "# convert land_use to residential, commercial, and industrial\n",
    "df['land_use_modified']=None\n",
    "cond_list=[(df['land_use']=='RESIDENTIAL')|\\\n",
    "           (df['land_use']=='COMMERCIAL & RESIDENTIAL')|\\\n",
    "           (df['land_use']=='RESIDENTIAL WITH COMMERCIAL AT 1ST STOREY')|\\\n",
    "           (df['land_use']=='RESIDENTIAL / INSTITUTION'),\n",
    "           (df['land_use']=='COMMERCIAL')|\\\n",
    "           (df['land_use']=='HOTEL')|\\\n",
    "           (df['land_use']=='BUSINESS PARK')|\\\n",
    "           (df['land_use']=='COMMERCIAL / INSTITUTION')|\\\n",
    "           (df['land_use']=='BUSINESS PARK - WHITE'),\n",
    "           (df['land_use']=='BUSINESS 2')|\\\n",
    "           (df['land_use']=='UTILITY')|\\\n",
    "           (df['land_use']=='BUSINESS 1')|\\\n",
    "           (df['land_use']=='PORT / AIRPORT')|\\\n",
    "           (df['land_use']=='BUSINESS 2 - WHITE')|\\\n",
    "           (df['land_use']=='BUSINESS 1 - WHITE')\n",
    "          ]\n",
    "choice_list=['residential','commercial','industrial']\n",
    "df['land_use_modified'] = np.select(cond_list, choice_list)\n",
    "df=df[['geometry','land_use_modified']]\n",
    "\n",
    "# save as shp\n",
    "df.to_file(os.path.join(root,'data/land_use/Singapore/land_use.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokyo\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "df=gpd.read_file(os.path.join(root,'data/land_use/Tokyo/A29-19_13000.shp'))\n",
    "\n",
    "# convert A29_005 to residential, commercial, and industrial\n",
    "df['land_use_modified']=None\n",
    "cond_list=[(df['A29_005']=='第一種低層住居専用地域')|\\\n",
    "           (df['A29_005']=='第二種中高層住居専用地域')|\\\n",
    "           (df['A29_005']=='第一種中高層住居専用地域')|\\\n",
    "           (df['A29_005']=='第二種住居地域')|\\\n",
    "           (df['A29_005']=='第一種住居地域')|\\\n",
    "           (df['A29_005']=='準住居地域')|\\\n",
    "           (df['A29_005']=='第二種低層住居専用地域'),\n",
    "           (df['A29_005']=='近隣商業地域')|\\\n",
    "           (df['A29_005']=='商業地域'),\n",
    "           (df['A29_005']=='準工業地域')|\\\n",
    "           (df['A29_005']=='工業地域')|\\\n",
    "           (df['A29_005']=='工業専用地域')]\n",
    "choice_list=['residential','commercial','industrial']\n",
    "df['land_use_modified'] = np.select(cond_list, choice_list)\n",
    "df=df[['geometry','land_use_modified']]\n",
    "\n",
    "# save as shp\n",
    "df.to_file(os.path.join(root,'data/land_use/Tokyo/land_use.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-logistics",
   "metadata": {},
   "source": [
    "#### extract land use mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    # location data\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['geometry']=location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    location_gdf=gpd.GeoDataFrame(location_df, geometry='geometry')\n",
    "    location_gdf=location_gdf.set_crs(epsg=4326)\n",
    "    #convert to Pseudo-Mercator\n",
    "    location_gdf=location_gdf.to_crs(\"EPSG:3857\")\n",
    "    # merge location to detection result to convert it to gdf\n",
    "    location_gdf['panoId']='pano='+location_gdf['panoId']\n",
    "    \n",
    "    # load land use data\n",
    "    land_use_gdf=gpd.read_file(os.path.join(root,'data/land_use/{}/land_use.shp'.format(city)))\n",
    "    land_use_gdf=land_use_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    # create 500m buffer around img_location_gdf\n",
    "    buffer_500m = location_gdf.copy()\n",
    "    buffer_500m[\"geometry\"]=location_gdf.geometry.buffer(500)\n",
    "    # coduct spatial join to extract points that intersect with the 500m buffer\n",
    "    land_use_in_500m = gpd.sjoin(buffer_500m, land_use_gdf, how=\"left\", op='intersects')\n",
    "    # calculate the area\n",
    "    land_use_in_500m['area']=land_use_in_500m['geometry'].area\n",
    "    # group by panoId and land use to get sum\n",
    "    land_use_in_500m=land_use_in_500m.groupby(['panoId','land_use_m']).\\\n",
    "                              agg({'area':'sum'}).\\\n",
    "                              reset_index()\n",
    "    land_use_in_500m = land_use_in_500m.pivot(index='panoId', columns='land_use_m', values='area').reset_index()\n",
    "    # fill NaN with 0\n",
    "    land_use_in_500m=land_use_in_500m.fillna(0)\n",
    "    land_use_in_500m['total']=land_use_in_500m['residential']+\\\n",
    "                              land_use_in_500m['commercial']+\\\n",
    "                              land_use_in_500m['industrial']\n",
    "    # calculate proportion of each land use\n",
    "    land_use_in_500m['residential_percentage']=land_use_in_500m['residential']/land_use_in_500m['total']\n",
    "    land_use_in_500m['commercial_percentage']=land_use_in_500m['commercial']/land_use_in_500m['total']\n",
    "    land_use_in_500m['industrial_percentage']=land_use_in_500m['industrial']/land_use_in_500m['total']\n",
    "    # calculate land_use_mix\n",
    "    land_use_in_500m['residential_percentage_log']=land_use_in_500m['residential_percentage']*np.log(land_use_in_500m['residential_percentage'])\n",
    "    land_use_in_500m['commercial_percentage_log']=land_use_in_500m['commercial_percentage']*np.log(land_use_in_500m['commercial_percentage'])\n",
    "    land_use_in_500m['industrial_percentage_log']=land_use_in_500m['industrial_percentage']*np.log(land_use_in_500m['industrial_percentage'])\n",
    "    land_use_in_500m['residential_percentage_log']=land_use_in_500m['residential_percentage_log'].fillna(0)\n",
    "    land_use_in_500m['commercial_percentage_log']=land_use_in_500m['commercial_percentage_log'].fillna(0)\n",
    "    land_use_in_500m['industrial_percentage_log']=land_use_in_500m['industrial_percentage_log'].fillna(0)\n",
    "    land_use_in_500m['land_use_mix']=-1*(land_use_in_500m['residential_percentage_log']+\\\n",
    "                                         land_use_in_500m['commercial_percentage_log']+\\\n",
    "                                         land_use_in_500m['industrial_percentage_log'])/(math.log(3))\n",
    "    land_use_in_500m['land_use_mix']=np.where(land_use_in_500m['land_use_mix'] == -0.0, 0, land_use_in_500m['land_use_mix'])\n",
    "    \n",
    "    # merge with the original location_gdf\n",
    "    land_use_in_500m=pd.merge(location_gdf[['panoId']],\n",
    "                              land_use_in_500m,\n",
    "                              on='panoId',\n",
    "                              how='left'\n",
    "                             )\n",
    "    land_use_in_500m=land_use_in_500m.rename(columns={'panoId':'pano_id'})\n",
    "    land_use_in_500m=land_use_in_500m[['pano_id','land_use_mix']]\n",
    "    print(land_use_in_500m)\n",
    "    # save the dataframe to csv\n",
    "    land_use_in_500m.to_csv(os.path.join(root,'data/tabular_data/{}/land_use.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-pointer",
   "metadata": {},
   "source": [
    "## Slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-auditor",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-copper",
   "metadata": {},
   "source": [
    "#### Tokyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --user globaldem --password preciseelevation -O /home/ec2-user/SageMaker/data/topography/Tokyo/dem_tif_n30e120.tar http://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_DEM/distribute/v1.0.2/dem_tif_n30e120.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract it to Tokyo folder\n",
    "!tar -xf /home/ec2-user/SageMaker/data/topography/dem_tif_n30e120.tar --directory /home/ec2-user/SageMaker/data/topography/Tokyo/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-harvest",
   "metadata": {},
   "source": [
    "#### Singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --user globaldem --password preciseelevation -O /home/ec2-user/SageMaker/data/topography/Singapore/dem_tif_n00e090.tar http://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_DEM/distribute/v1.0.2/dem_tif_n00e090.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract it to Singapore folder\n",
    "!tar -xf /home/ec2-user/SageMaker/data/topography/Singapore/dem_tif_n00e090.tar --directory /home/ec2-user/SageMaker/data/topography/Singapore/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-injury",
   "metadata": {},
   "source": [
    "### Check the data and move the appropriate file one level up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokyo\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "dem=os.path.join(root,'data/topography/Tokyo/dem_tif_n30e120/n35e135_dem.tif')\n",
    "img = rasterio.open(dem)\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the file\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "topo_folder=os.path.join(root,'data/topography/')\n",
    "dem=os.path.join(topo_folder,'Tokyo/dem_tif_n30e120/n35e135_dem.tif')\n",
    "shutil.copy2(dem,os.path.join(topo_folder,'Tokyo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singapore\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "dem=os.path.join(root,'data/topography/Singapore/dem_tif_n00e090/n00e100_dem.tif')\n",
    "img = rasterio.open(dem)\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the file\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "topo_folder=os.path.join(root,'data/topography/')\n",
    "dem=os.path.join(topo_folder,'Singapore/dem_tif_n00e090/n00e100_dem.tif')\n",
    "shutil.copy2(dem,os.path.join(topo_folder,'Singapore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-carpet",
   "metadata": {},
   "source": [
    "### calculate slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    dem_list=glob.glob(os.path.join(root,'data/topography/{}/*.tif'.format(city)))\n",
    "    for dem in dem_list:\n",
    "        dem_array = rd.LoadGDAL(dem)\n",
    "        slope = rd.TerrainAttribute(dem, attrib='slope_riserun')\n",
    "        rd.rdShow(slope, axes=False, cmap='magma', figsize=(8, 5.5))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-physiology",
   "metadata": {},
   "source": [
    "### extract slope value to each sample point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code above didn't work, so I imported data from local computer\n",
    "# use that data to extract slope value to each sample point\n",
    "\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# define a function to return a value, given x and y\n",
    "def getRasterValue(raster,x,y):\n",
    "    row, col = raster.index(x,y)\n",
    "    value=raster.read(1)[row,col]\n",
    "    return value\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    #open slope file\n",
    "    slope_file=os.path.join(root,'data/topography/{}/slope_{}.tif'.format(city,city))\n",
    "    slope = rasterio.open(slope_file)\n",
    "    \n",
    "    # sample point locations\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['geometry']=location_df.apply(lambda x: Point((float(x.panoLon), float(x.panoLat))), axis=1)\n",
    "    location_gdf=gpd.GeoDataFrame(location_df, geometry='geometry')\n",
    "    location_gdf=location_gdf.set_crs(epsg=4326)\n",
    "    # merge location to detection result to convert it to gdf\n",
    "    location_gdf['panoId']='pano='+location_gdf['panoId']\n",
    "    # apply getRasterValue function to get slope value from the raster for each sample point\n",
    "    location_gdf['slope']= location_gdf.apply(lambda x: getRasterValue(slope, x['geometry'].xy[0][0], x['geometry'].xy[1][0]), axis=1)\n",
    "    # only keep panoId and slope\n",
    "    location_gdf=location_gdf[['panoId','slope']]\n",
    "    location_gdf=location_gdf.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # save the data\n",
    "    print(location_gdf)\n",
    "    location_gdf.to_csv(os.path.join(root,'data/tabular_data/{}/slope.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-europe",
   "metadata": {},
   "source": [
    "## Air quality\n",
    "- Input cities' names\n",
    "- use api (https://api.waqi.info/map/bounds/?latlng=&token=) to get station names and their locations (lat&lon)\n",
    "- use station names to download the file\n",
    "- conduct everything above locally\n",
    "- import data on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-promotion",
   "metadata": {},
   "source": [
    "### clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    # clean the csv file names and station names\n",
    "    # station names\n",
    "    station_df=pd.read_csv(os.path.join(root,'data/aqi/{}/{}_aqi_station.csv'.format(city,city)))\n",
    "    # lower-case all the letters\n",
    "    station_df['name']=station_df['name'].str.lower()\n",
    "    # convert ū,ō to u,o\n",
    "    station_df['name']=station_df['name'].str.replace('ū','u').str.replace('ō','o')\n",
    "    # convert tokyo-to to tokyo\n",
    "    station_df['name']=station_df['name'].str.replace('tokyo-to','tokyo')\n",
    "    # convert ', ' to '_'\n",
    "    station_df['name']=station_df['name'].str.replace(', ','_')\n",
    "    # remove '_japan'\n",
    "    station_df['name']=station_df['name'].str.replace('_japan','')\n",
    "    # convert ' ' to '_'\n",
    "    station_df['name']=station_df['name'].str.replace(' ','_')\n",
    "    # remove '-ku'\n",
    "    station_df['name']=station_df['name'].str.replace('-ku','')\n",
    "    # convert '-' to '_'\n",
    "    station_df['name']=station_df['name'].str.replace('-','_')\n",
    "    \n",
    "    # csv file names\n",
    "    csv_file_list_raw=glob.glob(os.path.join(root,'data/aqi/{}/historical_data/*.csv'.format(city)))\n",
    "    csv_file_list=[os.path.split(file)[1] for file in csv_file_list_raw]\n",
    "    # convert ū,ō to u,o\n",
    "    csv_file_list=[file.replace('ū','u') for file in csv_file_list]\n",
    "    csv_file_list=[file.replace('ō','o') for file in csv_file_list]\n",
    "    # convert tokyo-to to tokyo\n",
    "    csv_file_list=[file.replace('tokyo-to','tokyo') for file in csv_file_list]\n",
    "    # remove '-air-quality'\n",
    "    csv_file_list=[file.replace('-air-quality','') for file in csv_file_list]\n",
    "    # convert '-' to ' '\n",
    "    csv_file_list=[file.replace(',-','_') for file in csv_file_list]\n",
    "    # convert ', ' to '_'\n",
    "    csv_file_list=[file.replace(', ','_') for file in csv_file_list]\n",
    "    # remove '_japan'\n",
    "    csv_file_list=[file.replace('_japan','') for file in csv_file_list]  \n",
    "    # remove '.csv'\n",
    "    csv_file_list=[file.replace('.csv','') for file in csv_file_list] \n",
    "    # remove '-ku'\n",
    "    csv_file_list=[file.replace('-ku','') for file in csv_file_list] \n",
    "    # convert '-' to '_'\n",
    "    csv_file_list=[file.replace('-', '_') for file in csv_file_list] \n",
    "    \n",
    "    # create a new column to check if the file is extracted\n",
    "    station_df['extracted']=False\n",
    "    cond_list=[station_df['name'].isin(csv_file_list)]\n",
    "    choice_list=[True]\n",
    "    station_df['extracted']=np.select(cond_list,choice_list)\n",
    "    print(csv_file_list)\n",
    "    print(station_df.loc[station_df['extracted']==False])\n",
    "    \n",
    "    # save the modified dataframe\n",
    "    station_df.to_csv(os.path.join(root,'data/aqi/{}/{}_aqi_station_cleaned.csv'.format(city,city)))\n",
    "    # save the csv file as the modified names\n",
    "    for i in range(len(csv_file_list_raw)):\n",
    "        filename=csv_file_list[i]+'.csv'\n",
    "        os.rename(csv_file_list_raw[i],\n",
    "                  os.path.join(os.path.split(csv_file_list_raw[i])[0],filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-office",
   "metadata": {},
   "source": [
    "### calculate AQI for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AQI for each station\n",
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# create a function to get mean of AQI, given a station name\n",
    "def getMeanAQI(city, station_name):\n",
    "    hist_data_df=pd.read_csv(os.path.join(root,'data/aqi/{}/historical_data/{}.csv'.format(city,station_name)))\n",
    "    # calculate aqi for each day\n",
    "    hist_data_df[\"AQI\"] =hist_data_df.iloc[:,1:].max(axis=1)\n",
    "    # only select 2020 data\n",
    "    hist_data_df= hist_data_df.loc[hist_data_df['date'].str.contains('2020')]\n",
    "    # calculate the average of the whole data\n",
    "    aqi_mean=hist_data_df[\"AQI\"].mean()\n",
    "    return aqi_mean\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    # load data of station names\n",
    "    station_df=pd.read_csv(os.path.join(root,'data/aqi/{}/{}_aqi_station_cleaned.csv'.format(city,city)))\n",
    "    # drop those that didnt extract the aqi\n",
    "    station_df=station_df.loc[station_df['extracted']==1]\n",
    "    # drop extreme longitude\n",
    "    station_df=station_df.loc[station_df['lon']>0]\n",
    "    # create a aqi_mean column\n",
    "    station_df['aqi_mean']=station_df['name'].apply(lambda x: getMeanAQI(city, x))\n",
    "    # drop tokorozawa because it's creating duplicates\n",
    "    station_df=station_df.loc[station_df['name']!='tokorozawa']\n",
    "    # save the dataframe\n",
    "    station_df=station_df[['name','lat','lon','aqi_mean']]\n",
    "    print(station_df)\n",
    "    station_df.to_csv(os.path.join(root,'data/aqi/{}/{}_aqi_station_mean_extracted.csv'.format(city,city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-profession",
   "metadata": {},
   "source": [
    "### conduct spatial interpolation (IDW)\n",
    "- This couldn't be run, so I ran locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # conduct spatial interpolation (IDW)\n",
    "# city_list=[\n",
    "#     'Singapore',\n",
    "#     'Tokyo'\n",
    "#           ]\n",
    "# root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# # go through cities in the list\n",
    "# for city in city_list:\n",
    "#     # load mean aqi data at stations\n",
    "#     station_df=pd.read_csv(os.path.join(root,'data/aqi/{}/{}_aqi_station_mean_extracted.csv'.format(city,city)))\n",
    "    \n",
    "#     # sample point locations\n",
    "#     location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "#     location_df=pd.read_csv(location)\n",
    "#     # merge location to detection result to convert it to gdf\n",
    "#     location_df['panoId']='pano='+location_df['panoId']\n",
    "    \n",
    "#     # conduct IDW\n",
    "#     lons=np.array(station_df['lon']) \n",
    "#     lats=np.array(station_df['lat']) \n",
    "#     lon_lat=np.array(list(zip(lons,lats)))\n",
    "#     zdata=np.array(station_df['aqi_mean'])\n",
    "#     #test data\n",
    "#     lons_test=np.array(location_df['panoLon']) \n",
    "#     lats_test=np.array(location_df['panoLat']) \n",
    "#     lon_lat_test=np.array(list(zip(lons_test,lats_test)))\n",
    "#     #idw\n",
    "#     grid_z1 = griddata(lon_lat, zdata, (lons_test, lats_test), method='nearest')\n",
    "#     location_df['aqi']=grid_z1\n",
    "\n",
    "#     # save the data\n",
    "#     location_df=location_df[['panoId','panoLon','panoLat','aqi']]\n",
    "#     location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "#     print(location_df)\n",
    "#     location_df.to_csv(os.path.join(root,'data/tabular_data/{}/aqi.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-slovenia",
   "metadata": {},
   "source": [
    "# Extract features for perception prediction\n",
    "Steps:\n",
    "    1. Extract features for all the images\n",
    "    2. Get the features for img_survey based on the list of the pano_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-relative",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "*segmentation is a bit different from others because it was conducted in the previous section.\n",
    "\n",
    "*code below only filters data for img_survey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-newton",
   "metadata": {},
   "source": [
    "### 1st survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# get segmentation result and combine them\n",
    "combined_segmentation=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    # segmentation\n",
    "    segmentation_result_file=os.path.join(root,'data/tabular_data/{}/segmentation.csv'.format(city)) \n",
    "    segmentation_df=pd.read_csv(segmentation_result_file)\n",
    "    # change the first col name to pano_id\n",
    "    segmentation_df.rename(columns={segmentation_df.columns[0]:'pano_id'}, inplace=True)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the segmentation result to the list of the img_survey pano_id\n",
    "    img_survey_segmentation=pd.merge(img_survey_pano_id_df,segmentation_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_segmentation\n",
    "    combined_segmentation=combined_segmentation.append(img_survey_segmentation)\n",
    "\n",
    "# save it in the output folder as segmentation.csv\n",
    "combined_segmentation.to_csv(os.path.join(output_folder,'segmentation.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is only for copying the segmented images to output folder\n",
    "# # move the segmented img to output\n",
    "# city_list=[\n",
    "#     'Singapore',\n",
    "#     'Tokyo'\n",
    "#           ]\n",
    "# root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# # go through cities in the list\n",
    "# for city in city_list:\n",
    "#     img_segmented_list=glob.glob(os.path.join(root,'data/img_segmented/{}/*'.format(city)))\n",
    "#     # copy the segmented_img to output folder\n",
    "#     output_folder_segmented=os.path.join(root,'output/segmentation/{}/'.format(city))\n",
    "#     if not os.path.exists(output_folder_segmented):\n",
    "#         os.makedirs(output_folder_segmented)\n",
    "#     for img_segmented in img_segmented_list:\n",
    "#         shutil.copy2(img_segmented, output_folder_segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-wallace",
   "metadata": {},
   "source": [
    "## detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-milton",
   "metadata": {},
   "source": [
    "### 1st survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# get detection result and combine them\n",
    "combined_detection=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    # conduct detection for all the images\n",
    "    input_img_folder=os.path.join(root,'data/img_stitched_{}/'.format(city))\n",
    "    output_csv_folder=os.path.join(root,'data/tabular_data/{}/'.format(city))\n",
    "    output_img_folder=os.path.join(root,'output/detection/{}/'.format(city))\n",
    "    detection.detect_images(input_img_folder,output_csv_folder,output_img_folder,model='yolo3_darknet53_coco')\n",
    "    \n",
    "    # get data for img_survey\n",
    "    detection_result_file=os.path.join(root,'data/tabular_data/{}/detection.csv'.format(city)) \n",
    "    detection_df=pd.read_csv(detection_result_file)\n",
    "    # change the first col name to pano_id\n",
    "    detection_df.rename(columns={detection_df.columns[0]:'pano_id'}, inplace=True)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the detection result to the list of the img_survey pano_id\n",
    "    img_survey_detection=pd.merge(img_survey_pano_id_df,detection_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_detection\n",
    "    combined_detection=combined_detection.append(img_survey_detection)\n",
    "# save it in the output folder as detection.csv\n",
    "combined_detection.to_csv(os.path.join(output_folder,'detection.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-major",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-promotion",
   "metadata": {},
   "source": [
    "### 1st survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "model_folder=os.path.join(root,'models/')\n",
    "# get classification result and combine them\n",
    "combined_classification=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for city in city_list:\n",
    "    # conduct classification for all the images\n",
    "    input_img_folder=os.path.join(root,'data/img_stitched_{}/'.format(city))\n",
    "    output_folder=os.path.join(root,'data/tabular_data/{}/'.format(city))\n",
    "    classification.classify_image(model_folder,input_img_folder,output_folder)\n",
    "    \n",
    "    # get data for img_survey\n",
    "    classification_result_file=os.path.join(root,'data/tabular_data/{}/classification.csv'.format(city)) \n",
    "    classification_df=pd.read_csv(classification_result_file)\n",
    "    # change the first col name to pano_id\n",
    "    classification_df.rename(columns={classification_df.columns[0]:'pano_id'}, inplace=True)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the classification result to the list of the img_survey pano_id\n",
    "    img_survey_classification=pd.merge(img_survey_pano_id_df,classification_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_classification\n",
    "    combined_classification=combined_classification.append(img_survey_classification)\n",
    "# save it in the output folder as classification.csv\n",
    "combined_classification.to_csv(os.path.join(output_folder,'classification.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-craps",
   "metadata": {},
   "source": [
    "## edge detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-cotton",
   "metadata": {},
   "source": [
    "### 1st survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "# get edge_detection result and combine them\n",
    "combined_edge_detection=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for city in city_list:\n",
    "    # conduct edge_detection for all the images\n",
    "    input_img_folder=os.path.join(root,'data/img_stitched_{}/'.format(city))\n",
    "    output_img_folder=os.path.join(root,'output/edge_detection/{}/'.format(city))\n",
    "    output_csv_folder=os.path.join(root,'data/tabular_data/{}/'.format(city))\n",
    "    edge_detection.edge_detect_image(input_img_folder,output_img_folder,output_csv_folder)\n",
    "    \n",
    "    # get data for img_survey\n",
    "    edge_detection_result_file=os.path.join(root,'data/tabular_data/{}/edge_detection.csv'.format(city)) \n",
    "    edge_detection_df=pd.read_csv(edge_detection_result_file)\n",
    "    # change the first col name to pano_id\n",
    "    edge_detection_df.rename(columns={edge_detection_df.columns[0]:'pano_id'}, inplace=True)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the edge_detection result to the list of the img_survey pano_id\n",
    "    img_survey_edge_detection=pd.merge(img_survey_pano_id_df,edge_detection_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_edge_detection\n",
    "    combined_edge_detection=combined_edge_detection.append(img_survey_edge_detection)\n",
    "# save it in the output folder as edge_detection.csv\n",
    "combined_edge_detection.to_csv(os.path.join(output_folder,'edge_detection.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-worry",
   "metadata": {},
   "source": [
    "## blob detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-humor",
   "metadata": {},
   "source": [
    "### 1st survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "# get blob_detection result and combine them\n",
    "combined_blob_detection=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for city in city_list:\n",
    "    # conduct blob_detection for all the images\n",
    "    input_img_folder=os.path.join(root,'data/img_stitched_{}/'.format(city))\n",
    "    output_img_folder=os.path.join(root,'output/blob_detection/{}/'.format(city))\n",
    "    output_csv_folder=os.path.join(root,'data/tabular_data/{}/'.format(city))\n",
    "    blob_detection.blob_detect_image(input_img_folder,output_img_folder,output_csv_folder)\n",
    "    \n",
    "    # get data for img_survey\n",
    "    blob_detection_result_file=os.path.join(root,'data/tabular_data/{}/blob_detection.csv'.format(city)) \n",
    "    blob_detection_df=pd.read_csv(blob_detection_result_file)\n",
    "    # change the first col name to pano_id\n",
    "    blob_detection_df.rename(columns={blob_detection_df.columns[0]:'pano_id'}, inplace=True)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the blob_detection result to the list of the img_survey pano_id\n",
    "    img_survey_blob_detection=pd.merge(img_survey_pano_id_df,blob_detection_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_blob_detection\n",
    "    combined_blob_detection=combined_blob_detection.append(img_survey_blob_detection)\n",
    "# save it in the output folder as blob_detection.csv\n",
    "combined_blob_detection.to_csv(os.path.join(output_folder,'blob_detection.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-failing",
   "metadata": {},
   "source": [
    "## HLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-hello",
   "metadata": {},
   "source": [
    "### 1st survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "# get hls_statistics result and combine them\n",
    "combined_hls_statistics=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for city in city_list:\n",
    "    # conduct hls_statistics for all the images\n",
    "    input_img_folder=os.path.join(root,'data/img_stitched_{}/'.format(city))\n",
    "    output_csv_folder=os.path.join(root,'data/tabular_data/{}/'.format(city))\n",
    "    hls_statistics.blob_detect_image(input_img_folder,output_csv_folder)\n",
    "    \n",
    "    # get data for img_survey\n",
    "    hls_statistics_result_file=os.path.join(root,'data/tabular_data/{}/hls_statistics.csv'.format(city)) \n",
    "    hls_statistics_df=pd.read_csv(hls_statistics_result_file)\n",
    "    # change the first col name to pano_id\n",
    "    hls_statistics_df.rename(columns={hls_statistics_df.columns[0]:'pano_id'}, inplace=True)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the hls_statistics result to the list of the img_survey pano_id\n",
    "    img_survey_hls_statistics=pd.merge(img_survey_pano_id_df,hls_statistics_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_hls_statistics\n",
    "    combined_hls_statistics=combined_hls_statistics.append(img_survey_hls_statistics)\n",
    "# save it in the output folder as hls_statistics.csv\n",
    "combined_hls_statistics.to_csv(os.path.join(output_folder,'hls_statistics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-explorer",
   "metadata": {},
   "source": [
    "## Data organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-hartford",
   "metadata": {},
   "source": [
    "#### Outlier detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "result=pd.read_csv(os.path.join(root,'data/survey_result/Batch_4364547_batch_results.csv'))\n",
    "\n",
    "# clean the data\n",
    "result_clean=result[['WorkerId','WorkTimeInSeconds','Input.url','Answer.beauty',\n",
    "               'Answer.building_attractiveness','Answer.cleanliness','Answer.cycling_attractiveness',\n",
    "               'Answer.living_attractiveness','Answer.safety','Answer.spaciousness']]\n",
    "result_clean.columns=[col.replace('Answer.','',) for col in result_clean.columns]\n",
    "result_clean['panoId']=result_clean['Input.url'].\\\n",
    "    str.replace('https://gsv-perception-survey-img.s3-ap-southeast-1.amazonaws.com/data/','').\\\n",
    "    str.replace('pano%3D','pano=').\\\n",
    "    str[:-4]\n",
    "result_clean=result_clean.drop(['Input.url'],axis=1)\n",
    "\n",
    "# group by workerid to get mean worktime and range of scale used\n",
    "result_clean_grouped=result_clean.groupby(['WorkerId']).\\\n",
    "    agg({'WorkTimeInSeconds':['mean'],\n",
    "         'beauty':['min','max'],\n",
    "         'building_attractiveness':['min', 'max'],\n",
    "         'cleanliness':['min', 'max'],\n",
    "         'cycling_attractiveness':['min', 'max'],\n",
    "         'living_attractiveness':['min', 'max'],\n",
    "         'safety':['min', 'max'],\n",
    "         'spaciousness':['min', 'max']}).\\\n",
    "    reset_index()\n",
    "# get the col names of min&max values\n",
    "result_clean_grouped.columns=result_clean_grouped.columns.map(lambda t: t[0] + \"_\" + t[1])\n",
    "perception_score_min_cols=result_clean_grouped.columns[range(2, 16, 2)]\n",
    "result_clean_grouped['overall_min']=result_clean_grouped[perception_score_min_cols].min(axis=1)\n",
    "perception_score_max_cols=result_clean_grouped.columns[range(3, 17, 2)]\n",
    "result_clean_grouped['overall_max']=result_clean_grouped[perception_score_max_cols].max(axis=1)\n",
    "result_clean_grouped['scale_range']=result_clean_grouped['overall_max']-result_clean_grouped['overall_min']\n",
    "\n",
    "# visualize them\n",
    "ax=sns.boxplot(data=result_clean_grouped, y=\"WorkTimeInSeconds_mean\")\n",
    "ax.set_title(\"Box Plot of Work Time by Participants\", fontdict={\"fontsize\": \"25\", \"fontweight\" : \"3\"})\n",
    "plt.savefig(os.path.join(root, 'output/survey_result/work_time_boxplot.jpg'),bbox_inches = \"tight\",dpi=400)\n",
    "ax=sns.boxplot(data=result_clean_grouped, y=\"scale_range\")\n",
    "ax.set_title(\"Box Plot of Range of Scale by Participants\", fontdict={\"fontsize\": \"25\", \"fontweight\" : \"3\"})\n",
    "plt.savefig(os.path.join(root, 'output/survey_result/scale_range_boxplot.jpg'),bbox_inches = \"tight\",dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_method(df):\n",
    "    #M-Distance\n",
    "    x_minus_mu = df - np.mean(df)\n",
    "    cov = np.cov(df.values.T)                           #Covariance\n",
    "    inv_covmat = sp.linalg.inv(cov)                     #Inverse covariance\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat) \n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    md = np.sqrt(mahal.diagonal())\n",
    "    \n",
    "    #Flag as outlier\n",
    "    outlier = []\n",
    "    #Cut-off point\n",
    "    C = np.sqrt(chi2.ppf((1-0.001), df=df.shape[1]))    #degrees of freedom = number of variables\n",
    "    for index, value in enumerate(md):\n",
    "        if value > C:\n",
    "            outlier.append(index)\n",
    "        else:\n",
    "            continue\n",
    "    return outlier, md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use MD\n",
    "worktime_scale_range=result_clean_grouped[[\"WorkTimeInSeconds_mean\",\"scale_range\"]]\n",
    "outliers_mahal_bi, md_bi = mahalanobis_method(df=worktime_scale_range)\n",
    "\n",
    "#Visualization\n",
    "#add column to a slice of a DataFrame\n",
    "df_bi_cp = copy.deepcopy(worktime_scale_range) \n",
    "\n",
    "#Add md and robust md to copy of dataframe\n",
    "df_bi_cp['md'] = md_bi\n",
    "\n",
    "def flag_outliers(df, outliers):\n",
    "    flag = []\n",
    "    for index in range(df.shape[0]):\n",
    "        if index in outliers:\n",
    "            flag.append(1)\n",
    "        else:\n",
    "            flag.append(0)\n",
    "    return flag\n",
    "\n",
    "#Flag outliers with 1, others with 0\n",
    "df_bi_cp['flag'] = flag_outliers(worktime_scale_range, outliers_mahal_bi)\n",
    "\n",
    "#show and save\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "ax = sns.scatterplot(x=\"WorkTimeInSeconds_mean\", y=\"scale_range\", hue='flag', data=df_bi_cp)\n",
    "plt.savefig(os.path.join(root, 'output/survey_result/worktime_scale_outlier.jpg'),bbox_inches = \"tight\",dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-algeria",
   "metadata": {},
   "source": [
    "### combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take average of the survey result by panoid\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "result=pd.read_csv(os.path.join(root,'data/survey_result/Batch_4364547_batch_results.csv'))\n",
    "result_clean=result.iloc[:,27:35]\n",
    "result_clean.columns=[col.replace('Answer.','',) for col in result_clean.columns]\n",
    "result_clean['panoId']=result_clean['Input.url'].\\\n",
    "    str.replace('https://gsv-perception-survey-img.s3-ap-southeast-1.amazonaws.com/data/','').\\\n",
    "    str.replace('pano%3D','pano=').\\\n",
    "    str[:-4]\n",
    "result_clean=result_clean[['panoId','beauty','building_attractiveness','cleanliness','cycling_attractiveness',\n",
    "                          'living_attractiveness','safety','spaciousness']]\n",
    "# take average by panoId\n",
    "result_clean=result_clean.groupby(['panoId']).\\\n",
    "    agg({'beauty':'mean',\n",
    "         'building_attractiveness':'mean',\n",
    "         'cleanliness':'mean',\n",
    "         'cycling_attractiveness':'mean',\n",
    "         'living_attractiveness':'mean',\n",
    "         'safety':'mean',\n",
    "         'spaciousness':'mean'}).\\\n",
    "    reset_index()\n",
    "# save the grouped result\n",
    "result_clean=result_clean.rename(columns={'panoId':'pano_id'})\n",
    "result_clean.to_csv(os.path.join(root,'data/survey_result/grouped_survey_result.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "data_list=['segmentation',\n",
    "           'detection',\n",
    "           'classification',\n",
    "           'edge_detection',\n",
    "           'blob_detection',\n",
    "           'hls_statistics'\n",
    "          ]\n",
    "\n",
    "# survey result\n",
    "result=pd.read_csv(os.path.join(root,'data/survey_result/grouped_survey_result.csv'))\n",
    "\n",
    "# go through all the dataset\n",
    "for data in data_list:\n",
    "    df_temp=pd.DataFrame()\n",
    "    for city in city_list:\n",
    "        # load the data\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        # append the data_df to df_temp\n",
    "        df_temp=df_temp.append(data_df)\n",
    "    # left join the df_temp to result\n",
    "    if not 'pano_id' in df_temp.columns:\n",
    "        df_temp=df_temp.rename(columns={df_temp.columns[0]:'pano_id'})\n",
    "    df_temp.drop(df_temp.filter(regex='Unnamed').columns, axis=1, inplace=True)\n",
    "    df_temp.columns=['pano_id' if col =='pano_id' else data+'_'+col for col in df_temp.columns]\n",
    "    result=pd.merge(result,df_temp,on='pano_id',how='left')\n",
    "result.drop(result.filter(regex='Unnamed').columns, axis=1, inplace=True)\n",
    "# save the combined data\n",
    "result.to_csv(os.path.join(root,'data/survey_result/survey_result_with_independent_variables.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-ontario",
   "metadata": {},
   "source": [
    "## Predict perception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-nelson",
   "metadata": {},
   "source": [
    "#### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# Global_vars\n",
    "seed = 1234\n",
    "num_folds = 10\n",
    "n_jobs = 3\n",
    "scoring='neg_root_mean_squared_error'\n",
    "model_output_folder=os.path.join(root,'models/')\n",
    "\n",
    "# load the data\n",
    "result=pd.read_csv(os.path.join(root,'data/survey_result/survey_result_with_independent_variables.csv'))\n",
    "\n",
    "# create a list of target variables\n",
    "target_variables=['beauty',\n",
    "                  'building_attractiveness','cleanliness','cycling_attractiveness',\n",
    "                  'living_attractiveness','safety','spaciousness'\n",
    "                 ]\n",
    "for target_variable in tqdm.tqdm(target_variables):\n",
    "    X=result.iloc[:,9:]\n",
    "    y = result[target_variable]\n",
    "    # split X,y into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n",
    "\n",
    "    # Create pipelines\n",
    "    pipelines = pipeline_utils.create_pipelines(seed)\n",
    "\n",
    "#     # Run cv experiment without hyper_param_tuning\n",
    "#     results_df = run_cv_and_test(X_train, y_train, X_test, y_test, pipelines, scoring, seed, num_folds,\n",
    "#                                  dataset_name=parser.name, n_jobs=n_jobs)\n",
    "\n",
    "#     # Save cv experiment to csv\n",
    "#     if is_save_results:\n",
    "#         dataset_results_name = parser.name + \"_results.csv\"\n",
    "#         results_path = os.path.join(\"..\", \"..\", \"data\", \"processed\", dataset_results_name)\n",
    "#         results_df.to_csv(results_path, index=False)\n",
    "\n",
    "\n",
    "    # Run same experiment with hypertuned parameters\n",
    "#     print(\"#\"*30 + \"Hyper tuning parameters\" \"#\"*30)\n",
    "    hypertuned_params = pipeline_utils.get_hypertune_params()\n",
    "    hypertune_results_df = pipeline_utils.run_cv_and_test_hypertuned_params(X_train, y_train, X_test, y_test, \n",
    "                                                                            pipelines, scoring, seed, num_folds,\n",
    "                                                                            hypertuned_params, target_variable, n_jobs, \n",
    "                                                                            model_output_folder)\n",
    "    print(hypertune_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-psychiatry",
   "metadata": {},
   "source": [
    "### predict for the rest of the data (including the ones surveyed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-newsletter",
   "metadata": {},
   "source": [
    "#### create models using the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a list of target variables\n",
    "# target_variables=['beauty',\n",
    "#                   'building_attractiveness','cleanliness','cycling_attractiveness',\n",
    "#                   'living_attractiveness','safety','spaciousness'\n",
    "#                  ]\n",
    "# model_name='LGBMR-PCA'\n",
    "\n",
    "# model=lgb.train(params,\n",
    "#                 dtrain,\n",
    "#                 valid_sets=[dtrain,dval],\n",
    "#                 verbose_eval=100,\n",
    "#                 early_stopping_rounds=100\n",
    "#                 )\n",
    "\n",
    "# prediction=np.rint(model.predict(X_test,num_iteration=model.best_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "model_output_folder=os.path.join(root,'models/')\n",
    "model_name='StandardScaler_LGBMR-PCA'\n",
    "\n",
    "data_list=['segmentation',\n",
    "           'detection',\n",
    "           'classification',\n",
    "           'edge_detection',\n",
    "           'blob_detection',\n",
    "           'hls_statistics'\n",
    "          ]\n",
    "\n",
    "# create a list of target variables\n",
    "target_variables=['beauty',\n",
    "                  'building_attractiveness','cleanliness','cycling_attractiveness',\n",
    "                  'living_attractiveness','safety','spaciousness'\n",
    "                 ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load a dataframe of panoid\n",
    "    sample_points=pd.read_csv(os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city)))\n",
    "    sample_points=sample_points[['panoId']]\n",
    "    sample_points=sample_points.rename(columns={'panoId':'pano_id'})\n",
    "    sample_points['pano_id']='pano='+sample_points['pano_id']\n",
    "    sample_points_keep=sample_points.copy()\n",
    "    \n",
    "    # go through data_list and left join the data to sample_points df\n",
    "    for data in data_list:\n",
    "        # load the data\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        # left join the df_temp to result\n",
    "        if not 'pano_id' in data_df.columns:\n",
    "            data_df=data_df.rename(columns={data_df.columns[0]:'pano_id'})\n",
    "        data_df.drop(data_df.filter(regex='Unnamed').columns, axis=1, inplace=True)\n",
    "        sample_points=pd.merge(sample_points,data_df,\n",
    "                              on='pano_id',how='left')\n",
    "    \n",
    "    # get X\n",
    "    X=sample_points.iloc[:,1:]\n",
    "    print(X)\n",
    "    \n",
    "    # go through the target variables\n",
    "    for target_variable in tqdm.tqdm(target_variables):\n",
    "        # load the model\n",
    "        model= joblib.load(os.path.join(model_output_folder,target_variable+\"__\"+model_name+'.joblib'))\n",
    "        prediction=model.predict(X)\n",
    "        prediction_df = pd.DataFrame(data=prediction, columns=[\"predicted_\"+target_variable])\n",
    "        sample_points_keep_copy=sample_points_keep.copy()\n",
    "        concatenated_dataframes = pd.concat([sample_points_keep_copy, prediction], axis=1)\n",
    "        print(concatenated_dataframes)\n",
    "        \n",
    "        # save the dataframe\n",
    "        concatenated_dataframes.to_csv(os.path.join(root,'data/tabular_data/{}/predicted_{}.csv'.format(city,target_variable)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-guest",
   "metadata": {},
   "source": [
    "# 2nd survey\n",
    "## Select 800 images and upload it to S3 to redo the survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take average of the survey result by panoid\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "result=pd.read_csv(os.path.join(root,'data/survey_result/Batch_4364547_batch_results.csv'))\n",
    "result_clean=result.iloc[:,27:35]\n",
    "result_clean.columns=[col.replace('Answer.','',) for col in result_clean.columns]\n",
    "result_clean['panoId']=result_clean['Input.url'].\\\n",
    "    str.replace('https://gsv-perception-survey-img.s3-ap-southeast-1.amazonaws.com/data/','').\\\n",
    "    str.replace('pano%3D','pano=').\\\n",
    "    str[:-4]\n",
    "result_clean=result_clean[['panoId','beauty','building_attractiveness','cleanliness','cycling_attractiveness',\n",
    "                          'living_attractiveness','safety','spaciousness']]\n",
    "# take average by min & max\n",
    "result_clean=result_clean.groupby(['panoId']).\\\n",
    "    agg({'beauty':['median', 'min', 'max'],\n",
    "         'building_attractiveness':['median', 'min', 'max'],\n",
    "         'cleanliness':['median', 'min', 'max'],\n",
    "         'cycling_attractiveness':['median', 'min', 'max'],\n",
    "         'living_attractiveness':['median', 'min', 'max'],\n",
    "         'safety':['median', 'min', 'max'],\n",
    "         'spaciousness':['median', 'min', 'max']}).\\\n",
    "    reset_index()\n",
    "\n",
    "# calculate the difference between min max\n",
    "result_clean=result_clean.rename(columns={'panoId':'pano_id'})\n",
    "# create a list of target variables\n",
    "target_variables=['beauty',\n",
    "                  'building_attractiveness','cleanliness','cycling_attractiveness',\n",
    "                  'living_attractiveness','safety','spaciousness']\n",
    "for target_variable in target_variables:\n",
    "    result_clean[(target_variable,'min_max_dif')]=result_clean[(target_variable,'max')]-result_clean[(target_variable,'min')]\n",
    "    \n",
    "# merge with city names\n",
    "result_clean.columns=result_clean.columns.map(lambda t: t[0] + \"_\" + t[1])\n",
    "result_clean.columns=result_clean.columns.map(lambda t: t.strip('_'))\n",
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "for city in city_list:\n",
    "    # location data\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['city']=city\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    location_df['pano_id']='pano='+location_df['pano_id']\n",
    "    # merge with the result_clean\n",
    "    result_clean=pd.merge(result_clean,location_df[['pano_id','city']],\n",
    "                         on='pano_id',how='left')\n",
    "# merge city_x and city_y and rename it to city\n",
    "result_clean['city_x']=result_clean['city_x'].fillna(result_clean['city_y'])\n",
    "result_clean=result_clean.rename(columns={'city_x':'city'})\n",
    "\n",
    "# filter the data to choose smaller min_max_dif\n",
    "result_clean_min_max_dif=result_clean.iloc[:,[0,22,23,24,25,26,27,28,29]]\n",
    "result_clean_city_filtered=pd.DataFrame()\n",
    "for city in city_list:\n",
    "    df_temp=result_clean.loc[result_clean['city']==city]\n",
    "    df_temp_min_max_dif_less_than_5=df_temp.loc[(df_temp['beauty_min_max_dif']<5)&\\\n",
    "                                              (df_temp['building_attractiveness_min_max_dif']<5)&\\\n",
    "                                              (df_temp['cleanliness_min_max_dif']<5)&\\\n",
    "                                              (df_temp['cycling_attractiveness_min_max_dif']<5)&\\\n",
    "                                              (df_temp['living_attractiveness_min_max_dif']<5)&\\\n",
    "                                              (df_temp['safety_min_max_dif']<5)&\\\n",
    "                                              (df_temp['spaciousness_min_max_dif']<5)\n",
    "                                               ]\n",
    "    # randomly sample 400 images\n",
    "    n=400\n",
    "    seed=1234\n",
    "    df_temp_min_max_dif_less_than_5_400=df_temp_min_max_dif_less_than_5.sample(n=n,random_state=seed)\n",
    "    # append to result_clean_city_filtered\n",
    "    result_clean_city_filtered=result_clean_city_filtered.append(df_temp_min_max_dif_less_than_5_400)\n",
    "\n",
    "# upload it to S3\n",
    "# set up sagemaker session\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "print(sess)\n",
    "\n",
    "# create a s3 bucket\n",
    "bucket_name='gsv-perception-survey-img'\n",
    "key_prefix='data_resampled_800'\n",
    "s3 = boto3.client('s3')\n",
    "my_region = boto3.session.Session().region_name\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)\n",
    "\n",
    "# go through city_list\n",
    "for city in city_list:\n",
    "    # sample 1,500 images\n",
    "    result_city_specific=result_clean_city_filtered.loc[result_clean_city_filtered['city']==city]\n",
    "    img_list=[os.path.join(root,'data/img_stitched_{}/{}.jpg'.format(city,row['pano_id']))\\\n",
    "              for index,row in result_city_specific.iterrows()]\n",
    "    # create a local output folder\n",
    "    out_folder=os.path.join(root,'data/img_survey_resampled_800/{}/'.format(city))\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.makedirs(out_folder)\n",
    "    \n",
    "    # save in img_survey folder and upload the data to bucket\n",
    "    for img in tqdm.tqdm(img_list):\n",
    "        # save locally\n",
    "        shutil.copy2(img, out_folder)\n",
    "        \n",
    "        # upload to s3\n",
    "        sess.upload_data(path=img, bucket=bucket_name,key_prefix=key_prefix)\n",
    "\n",
    "# Create a csv file for s3 URL\n",
    "url_list=[]\n",
    "outputFolder=os.path.join(root,'data/tabular_data/')\n",
    "\n",
    "# go through all the objects in the bucket\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "pages = paginator.paginate(Bucket=bucket_name, Prefix=key_prefix)\n",
    "\n",
    "for page in pages:\n",
    "    for obj in page['Contents']:\n",
    "        # convert file names into url\n",
    "        file_modified=obj['Key'].replace('=','%3D')\n",
    "        # create url\n",
    "        url='https://{BUCKET_NAME}.s3-ap-southeast-1.amazonaws.com/{FILE_NAME}'.\\\n",
    "        format(BUCKET_NAME=bucket_name,FILE_NAME=file_modified)\n",
    "        url_list.append(url)\n",
    "url_df=pd.DataFrame(url_list,columns=['url'])\n",
    "url_df.to_csv(os.path.join(outputFolder,'survey_resampled_800_aws_url.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-station",
   "metadata": {},
   "source": [
    "## segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# get segmentation result and combine them\n",
    "combined_segmentation=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey_2nd/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    # segmentation\n",
    "    segmentation_result_file=os.path.join(root,'data/tabular_data/{}/segmentation.csv'.format(city)) \n",
    "    segmentation_df=pd.read_csv(segmentation_result_file)\n",
    "    # change the first col name to pano_id\n",
    "    segmentation_df.rename(columns={segmentation_df.columns[0]:'pano_id'}, inplace=True)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey_resampled_800/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the segmentation result to the list of the img_survey pano_id\n",
    "    img_survey_segmentation=pd.merge(img_survey_pano_id_df,segmentation_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_segmentation\n",
    "    combined_segmentation=combined_segmentation.append(img_survey_segmentation)\n",
    "\n",
    "# save it in the output folder as segmentation.csv\n",
    "combined_segmentation.to_csv(os.path.join(output_folder,'segmentation.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-accommodation",
   "metadata": {},
   "source": [
    "## detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# get detection result and combine them\n",
    "combined_detection=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey_2nd/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    # detection\n",
    "    detection_result_file=os.path.join(root,'data/tabular_data/{}/detection.csv'.format(city)) \n",
    "    detection_df=pd.read_csv(detection_result_file)\n",
    "    # change the first col name to pano_id\n",
    "    detection_df.rename(columns={detection_df.columns[0]:'pano_id'}, inplace=True)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey_resampled_800/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the detection result to the list of the img_survey pano_id\n",
    "    img_survey_detection=pd.merge(img_survey_pano_id_df,detection_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_detection\n",
    "    combined_detection=combined_detection.append(img_survey_detection)\n",
    "\n",
    "# save it in the output folder as detection.csv\n",
    "combined_detection.to_csv(os.path.join(output_folder,'detection.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-phoenix",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# get classification result and combine them\n",
    "combined_classification=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey_2nd/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    # classification\n",
    "    classification_result_file=os.path.join(root,'data/tabular_data/{}/classification.csv'.format(city)) \n",
    "    classification_df=pd.read_csv(classification_result_file)\n",
    "    # change the first col name to pano_id\n",
    "    classification_df.rename(columns={classification_df.columns[0]:'pano_id'}, inplace=True)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey_resampled_800/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the classification result to the list of the img_survey pano_id\n",
    "    img_survey_classification=pd.merge(img_survey_pano_id_df,classification_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_classification\n",
    "    combined_classification=combined_classification.append(img_survey_classification)\n",
    "\n",
    "# save it in the output folder as classification.csv\n",
    "combined_classification.to_csv(os.path.join(output_folder,'classification.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-renaissance",
   "metadata": {},
   "source": [
    "## edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# get edge_detection result and combine them\n",
    "combined_edge_detection=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey_2nd/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    # edge_detection\n",
    "    edge_detection_result_file=os.path.join(root,'data/tabular_data/{}/edge_detection.csv'.format(city)) \n",
    "    edge_detection_df=pd.read_csv(edge_detection_result_file)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey_resampled_800/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the edge_detection result to the list of the img_survey pano_id\n",
    "    img_survey_edge_detection=pd.merge(img_survey_pano_id_df,edge_detection_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_edge_detection\n",
    "    combined_edge_detection=combined_edge_detection.append(img_survey_edge_detection)\n",
    "\n",
    "# save it in the output folder as edge_detection.csv\n",
    "combined_edge_detection.to_csv(os.path.join(output_folder,'edge_detection.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-settle",
   "metadata": {},
   "source": [
    "## blob detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# get blob_detection result and combine them\n",
    "combined_blob_detection=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey_2nd/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    # blob_detection\n",
    "    blob_detection_result_file=os.path.join(root,'data/tabular_data/{}/blob_detection.csv'.format(city)) \n",
    "    blob_detection_df=pd.read_csv(blob_detection_result_file)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey_resampled_800/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the blob_detection result to the list of the img_survey pano_id\n",
    "    img_survey_blob_detection=pd.merge(img_survey_pano_id_df,blob_detection_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_blob_detection\n",
    "    combined_blob_detection=combined_blob_detection.append(img_survey_blob_detection)\n",
    "\n",
    "# save it in the output folder as blob_detection.csv\n",
    "combined_blob_detection.to_csv(os.path.join(output_folder,'blob_detection.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-feelings",
   "metadata": {},
   "source": [
    "## HLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# get hls_statistics result and combine them\n",
    "combined_hls_statistics=pd.DataFrame()\n",
    "# create output folder\n",
    "output_folder=os.path.join(root,'data/tabular_data/survey_2nd/')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# go through cities in the list\n",
    "for city in city_list:\n",
    "    # hls_statistics\n",
    "    hls_statistics_result_file=os.path.join(root,'data/tabular_data/{}/hls_statistics.csv'.format(city)) \n",
    "    hls_statistics_df=pd.read_csv(hls_statistics_result_file)\n",
    "    \n",
    "    # get a dataframe of survey image list\n",
    "    img_survey_list=glob.glob(os.path.join(root,'data/img_survey_resampled_800/{}/*.jpg'.format(city)))\n",
    "    img_survey_pano_id=[os.path.split(img_survey)[1].replace('.jpg','') for img_survey in img_survey_list]\n",
    "    img_survey_pano_id_df=pd.DataFrame(img_survey_pano_id,columns=['pano_id'])\n",
    "    \n",
    "    # left join the hls_statistics result to the list of the img_survey pano_id\n",
    "    img_survey_hls_statistics=pd.merge(img_survey_pano_id_df,hls_statistics_df,\n",
    "                                     on='pano_id',\n",
    "                                     how='left'\n",
    "                                    )\n",
    "    # append the merged df to combined_hls_statistics\n",
    "    combined_hls_statistics=combined_hls_statistics.append(img_survey_hls_statistics)\n",
    "\n",
    "# save it in the output folder as hls_statistics.csv\n",
    "combined_hls_statistics.to_csv(os.path.join(output_folder,'hls_statistics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-sector",
   "metadata": {},
   "source": [
    "## Get 2nd survey results and combine with the 1st survey results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=[\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# load the 1st and 2nd srvey results\n",
    "result_2nd=pd.read_csv(os.path.join(root,'data/survey_result/Batch_4385106_batch_results.csv'))\n",
    "result_1st=result=pd.read_csv(os.path.join(root,'data/survey_result/Batch_4364547_batch_results.csv'))\n",
    "# convert Input.url to pano_id\n",
    "# 2nd\n",
    "result_2nd['pano_id']=result_2nd['Input.url'].\\\n",
    "    str.replace('https://gsv-perception-survey-img.s3-ap-southeast-1.amazonaws.com/data_resampled_800/','').\\\n",
    "    str.replace('pano%3D','pano=').\\\n",
    "    str[:-4]\n",
    "result_2nd=result_2nd.drop(['Input.url'],axis=1)\n",
    "# 1st\n",
    "result_1st['pano_id']=result_1st['Input.url'].\\\n",
    "    str.replace('https://gsv-perception-survey-img.s3-ap-southeast-1.amazonaws.com/data/','').\\\n",
    "    str.replace('pano%3D','pano=').\\\n",
    "    str[:-4]\n",
    "result_1st=result_1st.drop(['Input.url'],axis=1)\n",
    "\n",
    "# get unique pano_id of result_2nd\n",
    "result_2nd_unique_pano_list=result_2nd['pano_id'].unique()\n",
    "# select result_1st with pano_id in the result_2nd_unique_pano\n",
    "result_1st_for_2nd=result_1st.loc[result_1st['pano_id'].isin(result_2nd_unique_pano_list)]\n",
    "# append result_1st_for_2nd to result_2nd\n",
    "result_2nd=result_2nd.append(result_1st_for_2nd)\n",
    "# save result_2nd\n",
    "result_2nd.to_csv(os.path.join(root,'data/survey_result/survey_1st_2nd_combined.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-grammar",
   "metadata": {},
   "source": [
    "## detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad_method(df, variable_name):\n",
    "    #Takes two parameters: dataframe & variable of interest as string\n",
    "    columns = df.columns\n",
    "    med = np.median(df, axis = 0)\n",
    "    mad = np.abs(stats.median_abs_deviation(df))\n",
    "    threshold = 3\n",
    "    outlier = []\n",
    "    index=0\n",
    "    for item in range(len(columns)):\n",
    "        if columns[item] == variable_name:\n",
    "            index == item\n",
    "    for i, v in enumerate(df.loc[:,variable_name]):\n",
    "        t = (v-med[index])/mad[index]\n",
    "        if t > threshold:\n",
    "            outlier.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    return outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "result_1st_2nd=pd.read_csv(os.path.join(root,'data/survey_result/survey_1st_2nd_combined.csv'))\n",
    "\n",
    "# clean the data\n",
    "result_clean=result_1st_2nd[['pano_id','Answer.beauty',\n",
    "               'Answer.building_attractiveness','Answer.cleanliness','Answer.cycling_attractiveness',\n",
    "               'Answer.living_attractiveness','Answer.safety','Answer.spaciousness']]\n",
    "result_clean.columns=[col.replace('Answer.','',) for col in result_clean.columns]\n",
    "\n",
    "# get unique pano_id\n",
    "result_clean_unique_pano_list=result_clean['pano_id'].unique()\n",
    "# go through each pano_id to compute outlier\n",
    "# store all the index in a dictionary\n",
    "outlier_index_dict={}\n",
    "for i in range(len(result_clean.columns)-1):\n",
    "    outlier_index_dict[result_clean.columns[i+1]]=[]\n",
    "    \n",
    "for pano in tqdm.tqdm(result_clean_unique_pano_list):\n",
    "    result_clean_pano=result_clean.loc[result_clean['pano_id']==pano]\n",
    "    result_clean_pano=result_clean_pano.drop(['pano_id'],axis=1)\n",
    "    for i in range(len(result_clean.columns)-1):\n",
    "        outlier_index_local=mad_method(result_clean_pano, result_clean.columns[1+i])\n",
    "        outlier=result_clean_pano.iloc[outlier_index_local]\n",
    "        outlier_index_gobal=outlier.index.tolist()\n",
    "        outlier_index_dict[result_clean.columns[i+1]].extend(outlier_index_gobal)\n",
    "        \n",
    "# go through each score in outlier_index_dict to exclude the outliers from the data\n",
    "# and save the data seperately by score\n",
    "result_unique_pano_df=pd.DataFrame(result_clean_unique_pano_list,columns=['pano_id'])\n",
    "for key in outlier_index_dict:\n",
    "    # exclude outliers\n",
    "    result_clean_only_key=result_clean.loc[:,['pano_id',key]]\n",
    "    result_clean_only_key.drop(outlier_index_dict[key], inplace=True)\n",
    "    \n",
    "    # calculate mean values\n",
    "    result_clean_only_key=result_clean_only_key.groupby(['pano_id']).agg({key:'mean'}).reset_index()\n",
    "    \n",
    "    # left join the dataframe to result_unique_pano_df\n",
    "    result_unique_pano_df=pd.merge(result_unique_pano_df,result_clean_only_key,\n",
    "                                  on='pano_id',how='left')\n",
    "# save result_unique_pano_df\n",
    "print(result_unique_pano_df)\n",
    "result_unique_pano_df.to_csv(os.path.join(root,'data/survey_result/survey_1st_2nd_combined_without_outliers_grouped.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-court",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Combine data & feature engineering\n",
    "- Segmentation\n",
    "    - tree_ss\n",
    "    - sky_ss\n",
    "    - street_ss\n",
    "    - built_ss\n",
    "    - others_ss\n",
    "    - nature\n",
    "    - shannon\n",
    "- Classification\n",
    "    - slum_ic \n",
    "    - market_ic\n",
    "    - built_other_ic\n",
    "    - green_other_ic\n",
    "- Detection\n",
    "    - bicycle_od\n",
    "    - bus_od\n",
    "    - car_od\n",
    "    - motorcycle_od\n",
    "    - person_od\n",
    "    - traffic_light_od\n",
    "    - truck_od\n",
    "- Edge detection\n",
    "    - canny_edge_llf\n",
    "- Blob detection\n",
    "    - no_of_blobs_llf\n",
    "- HLS\n",
    "    - hue_mean_llf\n",
    "    - hue_std_llf\n",
    "    - lightness_mean_llf\n",
    "    - lightness_std_llf\n",
    "    - saturation_mean_llf\n",
    "    - saturation_std_llf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-companion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "data_list=['segmentation',\n",
    "           'detection',\n",
    "           'classification',\n",
    "           'edge_detection',\n",
    "           'blob_detection',\n",
    "           'hls_statistics'\n",
    "          ]\n",
    "\n",
    "# survey result\n",
    "result=pd.read_csv(os.path.join(root,'data/survey_result/survey_1st_2nd_combined_without_outliers_grouped.csv'))\n",
    "\n",
    "# go through all the dataset\n",
    "for data in data_list:\n",
    "    data_df=pd.read_csv(os.path.join(root,'data/tabular_data/survey_2nd/{}.csv'.format(data)))\n",
    "    data_df.drop(data_df.filter(regex='Unnamed').columns, axis=1, inplace=True)\n",
    "    data_df.columns=['pano_id' if col =='pano_id' else data+'_'+col for col in data_df.columns]\n",
    "    result=pd.merge(result,data_df,on='pano_id',how='left')\n",
    "result.drop(result.filter(regex='Unnamed').columns, axis=1, inplace=True)\n",
    "\n",
    "# feature engineering \n",
    "def featureEngineer(result,num_id_col):\n",
    "    # move relevant columns from result to result_final\n",
    "    result_final=result.iloc[:,0:num_id_col]\n",
    "    # Segmentation\n",
    "    seg_col_list=['segmentation_nature--vegetation',\n",
    "                  'segmentation_nature--sky',\n",
    "                  'segmentation_construction--flat--road',\n",
    "                  'segmentation_construction--flat--sidewalk',\n",
    "                  'segmentation_construction--flat--bike-lane',\n",
    "                  'segmentation_construction--flat--crosswalk-plain',\n",
    "                  'segmentation_construction--flat--curb-cut',\n",
    "                  'segmentation_construction--flat--parking',\n",
    "                  'segmentation_construction--flat--pedestrian-area',\n",
    "                  'segmentation_construction--flat--service-lane',\n",
    "                  'segmentation_construction--structure--building'\n",
    "                 ]\n",
    "    nature_col_list=[\"segmentation_nature\", \"segmentation_animal\"]\n",
    "    result_final['tree_ss']=result[seg_col_list[0]]\n",
    "    result_final['sky_ss']=result[seg_col_list[1]]\n",
    "    result_final['street_ss']=result[seg_col_list[2:10]].sum(axis=1)\n",
    "    result_final['built_ss']=result[seg_col_list[10]]\n",
    "    result_seg=result[result.columns[pd.Series(result.columns).str.startswith('segmentation')]]\n",
    "    result_final['others_ss']=result_seg[result_seg.columns.difference(seg_col_list)].sum(axis=1)\n",
    "    result_final['nature']=result[result.columns[result.columns.str.startswith(tuple(nature_col_list))]].sum(axis=1)\n",
    "    result_shannon=result_seg.apply(lambda x: x*np.log(x))\n",
    "    result_shannon=result_shannon.fillna(0)\n",
    "    result_final['shannon']=(-1*(result_shannon.sum(axis=1)))/(math.log(len(result.columns)-1))\n",
    "    # Classification\n",
    "    result_final['slum_ic']=result[['classification_slum',\n",
    "                                    'classification_alley',\n",
    "                                    'classification_junkyard'\n",
    "                                   ]].sum(axis=1)\n",
    "    result_final['market_ic']=result[['classification_bazaar/indoor',\n",
    "                                    'classification_bazaar/outdoor',\n",
    "                                    'classification_flea_market/indoor',\n",
    "                                    'classification_market/outdoor'\n",
    "                                   ]].sum(axis=1)\n",
    "    result_final['built_other_ic']=result[['classification_downtown',\n",
    "                                    'classification_embassy',\n",
    "                                    'classification_plaza'\n",
    "                                   ]].sum(axis=1)\n",
    "    result_final['green_other_ic']=result[['classification_forest_path',\n",
    "                                    'classification_forest_road'\n",
    "                                   ]].sum(axis=1)\n",
    "    # detection\n",
    "    result_final['bicycle_od']=result['detection_bicycle']\n",
    "    result_final['bus_od']=result['detection_bus']\n",
    "    result_final['car_od']=result['detection_car']\n",
    "    result_final['motorcycle_od']=result['detection_motorcycle']\n",
    "    result_final['person_od']=result['detection_person']\n",
    "    result_final['traffic_light_od']=result['detection_traffic light']\n",
    "    result_final['truck_od']=result['detection_truck']\n",
    "    # edge dection\n",
    "    result_final['canny_edge_llf']=result['edge_detection_edge_ratio']\n",
    "    # blob detection\n",
    "    result_final['no_of_blobs_llf']=result['blob_detection_blob_num']\n",
    "    # hls\n",
    "    result_final['hue_mean_llf']=result['hls_statistics_h_mean']\n",
    "    result_final['hue_std_llf']=result['hls_statistics_h_mean']\n",
    "    result_final['lightness_mean_llf']=result['hls_statistics_l_mean']\n",
    "    result_final['lightness_std_llf']=result['hls_statistics_l_std']\n",
    "    result_final['saturation_mean_llf']=result['hls_statistics_s_mean']\n",
    "    result_final['saturation_std_llf']=result['hls_statistics_s_std']\n",
    "    \n",
    "    return result_final\n",
    "\n",
    "result_final=featureEngineer(result,8)\n",
    "print(result_final)\n",
    "\n",
    "# # save the combined data\n",
    "result_final.to_csv(os.path.join(root,'data/survey_result/survey_2nd_result_with_independent_variables.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-middle",
   "metadata": {},
   "source": [
    "## Predict perception\n",
    "### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# Global_vars\n",
    "seed = 1234\n",
    "num_folds = 10\n",
    "n_jobs = 4\n",
    "scoring='neg_root_mean_squared_error'\n",
    "model_output_folder=os.path.join(root,'models/survey_2nd')\n",
    "if not os.path.exists(model_output_folder):\n",
    "    os.makedirs(model_output_folder)\n",
    "\n",
    "# load the data\n",
    "result=pd.read_csv(os.path.join(root,'data/survey_result/survey_2nd_result_with_independent_variables.csv'))\n",
    "\n",
    "# create a list of target variables\n",
    "target_variables=['beauty',\n",
    "                  'building_attractiveness','cleanliness','cycling_attractiveness',\n",
    "                  'living_attractiveness','safety','spaciousness'\n",
    "                 ]\n",
    "for target_variable in tqdm.tqdm(target_variables):\n",
    "    X=result.iloc[:,9:]\n",
    "    y = result[target_variable]\n",
    "    # split X,y into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n",
    "\n",
    "    # Create pipelines\n",
    "    pipelines = pipeline_utils.create_pipelines(seed)\n",
    "\n",
    "    # Run same experiment with hypertuned parameters\n",
    "    hypertuned_params = pipeline_utils.get_hypertune_params()\n",
    "    hypertune_results_df = pipeline_utils.run_cv_and_test_hypertuned_params(X_train, y_train, X_test, y_test, \n",
    "                                                                            pipelines, scoring, seed, num_folds,\n",
    "                                                                            hypertuned_params, target_variable, n_jobs, \n",
    "                                                                            model_output_folder)\n",
    "    print(hypertune_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-wisdom",
   "metadata": {},
   "source": [
    "### Train models with hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# Global_vars\n",
    "seed = 1234\n",
    "n_jobs = 4\n",
    "model_name='StandardScaler_LGBMR-PCA'\n",
    "model_output_folder=os.path.join(root,'models/survey_2nd/train_models/')\n",
    "if not os.path.exists(model_output_folder):\n",
    "    os.makedirs(model_output_folder)\n",
    "\n",
    "# load the data\n",
    "result=pd.read_csv(os.path.join(root,'data/survey_result/survey_2nd_result_with_independent_variables.csv'))\n",
    "\n",
    "# create a list of target variables\n",
    "target_variables=['beauty',\n",
    "                  'building_attractiveness','cleanliness','cycling_attractiveness',\n",
    "                  'living_attractiveness','safety','spaciousness'\n",
    "                 ]\n",
    "\n",
    "# Lists for all the results\n",
    "results = []\n",
    "# go through the target variables\n",
    "for target_variable in tqdm.tqdm(target_variables):\n",
    "    # get X and y\n",
    "    X=result.iloc[:,9:]\n",
    "    y = result[target_variable]\n",
    "    # split X,y into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n",
    "    \n",
    "    # load the model to get hyper parameters\n",
    "#     model= joblib.load(os.path.join(root,'models/survey_2nd',target_variable+\"__\"+model_name+'.joblib'))\n",
    "#     hyper_params=model.best_params_\n",
    "#     hyper_params_new={}\n",
    "#     for key in hyper_params:\n",
    "#         hyper_params_new[key.replace('LGBMR__','')] = hyper_params[key]\n",
    "    \n",
    "    hyper_params_new={\n",
    "    \"objective\": \"regression\",\n",
    "#     'num_leaves':100,\n",
    "    \"metric\": \"rmse\",\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\"}\n",
    "    # create a pipeline\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "#                                      ('StandardScaler', StandardScaler()), \n",
    "#                                      ('PCA', PCA(0.9)),\n",
    "                                     ('LGBMR', LGBMRegressor(**hyper_params_new,random_state=seed))\n",
    "                                 ])\n",
    "#     pipeline_temp =Pipeline(model_pipeline.steps[:-1])\n",
    "#     X_trans = pipeline_temp.fit_transform(X_train,y_train)\n",
    "#     eval_set = [(X_trans, y_train), (pipeline_temp.transform(X_test), y_test)]\n",
    "    model_pipeline.fit(X_train,y_train,LGBMR__verbose=10,\n",
    "                       LGBMR__early_stopping_rounds=35000,\n",
    "                       LGBMR__eval_set = [(X_train, y_train), (X_test, y_test)])\n",
    "    \n",
    "    # predict \n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "    # get different performance metrics\n",
    "    score_mean_absolute_error=mean_absolute_error(y_test, y_pred)\n",
    "    score_mean_absolute_percentage_error = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    score_root_mean_squared_error=mean_squared_error(y_test, y_pred, squared=False)\n",
    "    score_r2=r2_score(y_test, y_pred)\n",
    "    # create dict and append to rows_list\n",
    "    results_dict = {\"Target_variable\": target_variable,\n",
    "                    \"mean_absolute_error\":score_mean_absolute_error,\n",
    "                    \"mean_absolute_percentage_error\":score_mean_absolute_percentage_error,\n",
    "                    \"root_mean_squared_error\":score_root_mean_squared_error,\n",
    "                    \"r2_score\":score_r2\n",
    "                    }\n",
    "    print(results_dict)\n",
    "    results.append(results_dict)\n",
    "    \n",
    "    # plot the pred and actual value\n",
    "    g=plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "    g.axes.set_xlabel('True Values ')\n",
    "    g.axes.set_ylabel('Predictions ')\n",
    "    title=\"Predictions vs. Actual Values for {}\".format(target_variable)\n",
    "    g.axes.set_title(title, fontdict={\"fontsize\": \"25\", \"fontweight\" : \"3\"})\n",
    "    plt.savefig(os.path.join(root, 'output/perception_prediction/prediction_vs_actual_{}.jpg'.format(target_variable)),bbox_inches = \"tight\",dpi=400)\n",
    "    plt.show()\n",
    "    \n",
    "    # save the model\n",
    "    joblib.dump(model_pipeline, os.path.join(model_output_folder,target_variable+\"__\"+model_name+'.joblib'))\n",
    "    \n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(os.path.join(model_output_folder, 'accuracy_result.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-grammar",
   "metadata": {},
   "source": [
    "### predict for the rest of the data (including the ones surveyed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "model_output_folder=os.path.join(root,'models/survey_2nd/train_models/')\n",
    "model_name='StandardScaler_LGBMR-PCA'\n",
    "\n",
    "data_list=['segmentation',\n",
    "           'detection',\n",
    "           'classification',\n",
    "           'edge_detection',\n",
    "           'blob_detection',\n",
    "           'hls_statistics'\n",
    "          ]\n",
    "\n",
    "# create a list of target variables\n",
    "target_variables=['beauty',\n",
    "                  'building_attractiveness','cleanliness','cycling_attractiveness',\n",
    "                  'living_attractiveness','safety','spaciousness'\n",
    "                 ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load a dataframe of panoid\n",
    "    sample_points=pd.read_csv(os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city)))\n",
    "    sample_points=sample_points[['panoId']]\n",
    "    sample_points=sample_points.rename(columns={'panoId':'pano_id'})\n",
    "    sample_points['pano_id']='pano='+sample_points['pano_id']\n",
    "    sample_points_keep=sample_points.copy()\n",
    "    \n",
    "    # go through data_list and left join the data to sample_points df\n",
    "    for data in data_list:\n",
    "        # load the data\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        # left join the df_temp to result\n",
    "        if not 'pano_id' in data_df.columns:\n",
    "            data_df=data_df.rename(columns={data_df.columns[0]:'pano_id'})\n",
    "        data_df.drop(data_df.filter(regex='Unnamed').columns, axis=1, inplace=True)\n",
    "        data_df.columns=['pano_id' if col =='pano_id' else data+'_'+col for col in data_df.columns]\n",
    "        sample_points=pd.merge(sample_points,data_df,\n",
    "                              on='pano_id',how='left')\n",
    "    result_final=featureEngineer(sample_points,1)\n",
    "    # get X\n",
    "    X=result_final.iloc[:,1:]\n",
    "    print(X)\n",
    "    # go through the target variables\n",
    "    for target_variable in tqdm.tqdm(target_variables):\n",
    "        # load the model\n",
    "        model= joblib.load(os.path.join(model_output_folder,target_variable+\"__\"+model_name+'.joblib'))\n",
    "        prediction=model.predict(X)\n",
    "        prediction_df = pd.DataFrame(data=prediction, columns=[\"predicted_\"+target_variable])\n",
    "        sample_points_keep_copy=sample_points_keep.copy()\n",
    "        concatenated_dataframes = pd.concat([sample_points_keep_copy, prediction_df], axis=1)\n",
    "        print(concatenated_dataframes)\n",
    "        \n",
    "        # save the dataframe\n",
    "        concatenated_dataframes.to_csv(os.path.join(root,'data/tabular_data/{}/predicted_{}.csv'.format(city,target_variable)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-leonard",
   "metadata": {},
   "source": [
    "### Replace predicted scores with the surveyed scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-receipt",
   "metadata": {},
   "source": [
    "# Combining Indicators\n",
    "## non-SVI& SVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-curve",
   "metadata": {},
   "source": [
    "## Connectivity\n",
    "- Intersection with traffic lights\n",
    "- Intersection without traffic lights\n",
    "- Cul de sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daily-atlantic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Unnamed: 0.1  \\\n",
      "0              0             0   \n",
      "1              1             1   \n",
      "2              2             2   \n",
      "3              3             3   \n",
      "4              4             4   \n",
      "...          ...           ...   \n",
      "5828        5828          6115   \n",
      "5829        5829          6116   \n",
      "5830        5830          6117   \n",
      "5831        5831          6119   \n",
      "5832        5832          6120   \n",
      "\n",
      "                                                pano_id     panoLon   panoLat  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ  103.914512  1.400917   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA  103.965906  1.375535   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA  103.819038  1.293706   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA  103.703747  1.341976   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw  103.900929  1.311461   \n",
      "...                                                 ...         ...       ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...  103.835671  1.289808   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg  103.966108  1.373809   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ  103.701994  1.344878   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  103.632836  1.324383   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A  103.747407  1.428744   \n",
      "\n",
      "      distDiff  intersections_with_traffic_lights  \\\n",
      "0     0.000084                           0.979592   \n",
      "1     0.000015                           1.000000   \n",
      "2     0.000104                           0.846939   \n",
      "3     0.000184                           0.928571   \n",
      "4     0.000057                           1.000000   \n",
      "...        ...                                ...   \n",
      "5828  0.000131                           0.938776   \n",
      "5829  0.000204                           1.000000   \n",
      "5830  0.000181                           0.867347   \n",
      "5831  0.000031                           1.000000   \n",
      "5832  0.000018                           1.000000   \n",
      "\n",
      "      intersections_without_traffic_lights  cul_de_sac       city  \\\n",
      "0                                 0.613095    0.888889  Singapore   \n",
      "1                                 0.690476    0.888889  Singapore   \n",
      "2                                 0.720238    0.888889  Singapore   \n",
      "3                                 0.107143    0.888889  Singapore   \n",
      "4                                 0.559524    1.000000  Singapore   \n",
      "...                                    ...         ...        ...   \n",
      "5828                              0.613095    1.000000  Singapore   \n",
      "5829                              0.571429    1.000000  Singapore   \n",
      "5830                              0.333333    0.888889  Singapore   \n",
      "5831                              0.761905    1.000000  Singapore   \n",
      "5832                              0.922619    1.000000  Singapore   \n",
      "\n",
      "      connectivity  \n",
      "0        16.543840  \n",
      "1        17.195767  \n",
      "2        16.373772  \n",
      "3        12.830688  \n",
      "4        17.063492  \n",
      "...            ...  \n",
      "5828     17.012472  \n",
      "5829     17.142857  \n",
      "5830     13.930461  \n",
      "5831     18.412698  \n",
      "5832     19.484127  \n",
      "\n",
      "[5833 rows x 11 columns]\n",
      "      Unnamed: 0  Unnamed: 0.1                      pano_id     panoLon  \\\n",
      "0              0             0  pano=d1hU7t4QKuXqckKk29ENTQ  139.682778   \n",
      "1              1             1  pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401   \n",
      "2              2             2  pano=LySKMao7KBtKwGBAST6INA  139.866208   \n",
      "3              3             3  pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055   \n",
      "4              4             4  pano=pLawV8B_NY4JFfpRr8v-3A  139.622156   \n",
      "...          ...           ...                          ...         ...   \n",
      "6176        6176          6278  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896   \n",
      "6177        6177          6279  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517   \n",
      "6178        6178          6280  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967   \n",
      "6179        6179          6281  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638   \n",
      "6180        6180          6282  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100   \n",
      "\n",
      "        panoLat  distDiff  intersections_with_traffic_lights  \\\n",
      "0     35.574288  0.000018                           1.000000   \n",
      "1     35.723570  0.000001                           1.000000   \n",
      "2     35.664492  0.000007                           0.622449   \n",
      "3     35.607340  0.000356                           1.000000   \n",
      "4     35.766678  0.000014                           0.989796   \n",
      "...         ...       ...                                ...   \n",
      "6176  35.694996  0.000006                           0.928571   \n",
      "6177  35.704148  0.000004                           1.000000   \n",
      "6178  35.725836  0.000009                           1.000000   \n",
      "6179  35.756726  0.000188                           1.000000   \n",
      "6180  35.751490  0.000019                           1.000000   \n",
      "\n",
      "      intersections_without_traffic_lights  cul_de_sac   city  connectivity  \n",
      "0                                 0.839286         1.0  Tokyo     18.928571  \n",
      "1                                 0.559524         1.0  Tokyo     17.063492  \n",
      "2                                 0.922619         1.0  Tokyo     16.967120  \n",
      "3                                 1.000000         1.0  Tokyo     20.000000  \n",
      "4                                 0.952381         1.0  Tokyo     19.614512  \n",
      "...                                    ...         ...    ...           ...  \n",
      "6176                              0.636905         1.0  Tokyo     17.103175  \n",
      "6177                              0.857143         1.0  Tokyo     19.047619  \n",
      "6178                              1.000000         1.0  Tokyo     20.000000  \n",
      "6179                              0.946429         1.0  Tokyo     19.642857  \n",
      "6180                              0.839286         1.0  Tokyo     18.928571  \n",
      "\n",
      "[6181 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_list=['intersections_with_traffic_lights',\n",
    "           'intersections_without_traffic_lights',\n",
    "           'cul_de_sac'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "   # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        data_df=data_df.rename(columns={data_df.columns[0]:'pano_id',data_df.columns[-1]:data})\n",
    "    \n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        ) \n",
    "        location_df.to_csv(os.path.join(root,'data/tabular_data/{}/connectivity_before_scaling.csv'.format(city)))\n",
    "        \n",
    "# scaling\n",
    "# put both cities data together first\n",
    "df_temp=pd.DataFrame()\n",
    "for city in city_list:\n",
    "    connectivity_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/connectivity_before_scaling.csv'.format(city)))\n",
    "    connectivity_df['city']=city\n",
    "    df_temp=df_temp.append(connectivity_df)\n",
    "# scaling using Min-Max scale\n",
    "cols_transfomation_negative=data_list\n",
    "# fit the scaler\n",
    "scaler = MinMaxScaler()\n",
    "df_temp[cols_transfomation_negative]=scaler.fit_transform(df_temp[cols_transfomation_negative])\n",
    "df_temp[cols_transfomation_negative]=1-df_temp[cols_transfomation_negative]\n",
    "# calculate the indicator\n",
    "df_temp['connectivity']=df_temp[data_list].sum(axis=1)*(20/len(data_list))\n",
    "\n",
    "# save seperately by cities\n",
    "for city in city_list:\n",
    "    df_temp_city=df_temp.loc[df_temp['city']==city]\n",
    "    print(df_temp_city)\n",
    "    df_temp_city.to_csv(os.path.join(root,'data/tabular_data/{}/connectivity.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-uzbekistan",
   "metadata": {},
   "source": [
    "## Environment\n",
    "- Slope\n",
    "- Number of points of interest\n",
    "- Shannon land use mix index\n",
    "- Air quality index (AQI)\n",
    "- Scenery: buildings\n",
    "- Scenery: greenery\n",
    "- Scenery: water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "built-shopper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Unnamed: 0.1  \\\n",
      "0              0             0   \n",
      "1              1             1   \n",
      "2              2             2   \n",
      "3              3             3   \n",
      "4              4             4   \n",
      "...          ...           ...   \n",
      "5828        5828          6115   \n",
      "5829        5829          6116   \n",
      "5830        5830          6117   \n",
      "5831        5831          6119   \n",
      "5832        5832          6120   \n",
      "\n",
      "                                                pano_id     panoLon   panoLat  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ  103.914512  1.400917   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA  103.965906  1.375535   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA  103.819038  1.293706   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA  103.703747  1.341976   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw  103.900929  1.311461   \n",
      "...                                                 ...         ...       ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...  103.835671  1.289808   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg  103.966108  1.373809   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ  103.701994  1.344878   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  103.632836  1.324383   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A  103.747407  1.428744   \n",
      "\n",
      "      distDiff     slope       poi  land_use       aqi  \\\n",
      "0     0.000084  0.990570  0.005054  0.261089  0.528719   \n",
      "1     0.000015  0.941890  0.000000  0.361611  0.623963   \n",
      "2     0.000104  0.891331  0.008212  0.101656  0.475659   \n",
      "3     0.000184  0.813828  0.054959  0.665010  0.644805   \n",
      "4     0.000057  0.997301  0.048642  0.317303  0.512588   \n",
      "...        ...       ...       ...       ...       ...   \n",
      "5828  0.000131  0.686820  0.023373  0.634979  0.515324   \n",
      "5829  0.000204  0.990817  0.000000  0.230324  0.626108   \n",
      "5830  0.000181  0.870596  0.013266  0.357428  0.660571   \n",
      "5831  0.000031  0.948390  0.000000  0.000000  0.615572   \n",
      "5832  0.000018  0.934221  0.000000  0.000000  0.511336   \n",
      "\n",
      "      construction--structure--building  scenery_building  nature--vegetation  \\\n",
      "0                              0.084285          0.084285            0.240269   \n",
      "1                              0.098788          0.098788            0.142943   \n",
      "2                              0.056864          0.056864            0.278456   \n",
      "3                              0.065220          0.065220            0.373365   \n",
      "4                              0.219165          0.219165            0.105193   \n",
      "...                                 ...               ...                 ...   \n",
      "5828                           0.195024          0.195024            0.207520   \n",
      "5829                           0.059570          0.059570            0.264463   \n",
      "5830                           0.545886          0.545886            0.068556   \n",
      "5831                           0.024952          0.024952            0.052423   \n",
      "5832                           0.026038          0.026038            0.316331   \n",
      "\n",
      "      scenery_greenery  nature--water  scenery_water       city  environment  \n",
      "0             0.240269       0.000000       0.000000  Singapore     6.028534  \n",
      "1             0.142943       0.000000       0.000000  Singapore     6.197699  \n",
      "2             0.278456       0.000000       0.000000  Singapore     5.177653  \n",
      "3             0.373365       0.000000       0.000000  Singapore     7.477677  \n",
      "4             0.105193       0.000000       0.000000  Singapore     6.286261  \n",
      "...                ...            ...            ...        ...          ...  \n",
      "5828          0.207520       0.184564       0.184564  Singapore     6.993153  \n",
      "5829          0.264463       0.000000       0.000000  Singapore     6.203662  \n",
      "5830          0.068556       0.000000       0.000000  Singapore     7.189436  \n",
      "5831          0.052423       0.000000       0.000000  Singapore     4.689534  \n",
      "5832          0.316331       0.000000       0.000000  Singapore     5.108357  \n",
      "\n",
      "[5833 rows x 18 columns]\n",
      "      Unnamed: 0  Unnamed: 0.1                      pano_id     panoLon  \\\n",
      "0              0             0  pano=d1hU7t4QKuXqckKk29ENTQ  139.682778   \n",
      "1              1             1  pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401   \n",
      "2              2             2  pano=LySKMao7KBtKwGBAST6INA  139.866208   \n",
      "3              3             3  pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055   \n",
      "4              4             4  pano=pLawV8B_NY4JFfpRr8v-3A  139.622156   \n",
      "...          ...           ...                          ...         ...   \n",
      "6176        6176          6278  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896   \n",
      "6177        6177          6279  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517   \n",
      "6178        6178          6280  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967   \n",
      "6179        6179          6281  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638   \n",
      "6180        6180          6282  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100   \n",
      "\n",
      "        panoLat  distDiff     slope       poi  land_use       aqi  \\\n",
      "0     35.574288  0.000018  0.919123  0.024005  0.842089  0.460704   \n",
      "1     35.723570  0.000001  0.979139  0.009476  0.533357  0.424238   \n",
      "2     35.664492  0.000007  0.954535  0.012634  0.914101  0.353818   \n",
      "3     35.607340  0.000356  0.944189  0.011371  0.524602  0.406564   \n",
      "4     35.766678  0.000014  0.986737  0.003790  0.342951  0.360041   \n",
      "...         ...       ...       ...       ...       ...       ...   \n",
      "6176  35.694996  0.000006  0.943981  0.109918  0.547895  0.220556   \n",
      "6177  35.704148  0.000004  0.829054  0.197094  0.627163  0.438656   \n",
      "6178  35.725836  0.000009  0.980906  0.003790  0.410118  0.343141   \n",
      "6179  35.756726  0.000188  0.966229  0.013898  0.000000  0.418276   \n",
      "6180  35.751490  0.000019  0.990562  0.037271  0.840916  0.305290   \n",
      "\n",
      "      construction--structure--building  scenery_building  nature--vegetation  \\\n",
      "0                              0.413802          0.413802            0.023707   \n",
      "1                              0.327120          0.327120            0.063221   \n",
      "2                              0.543613          0.543613            0.020380   \n",
      "3                              0.470069          0.470069            0.073956   \n",
      "4                              0.206431          0.206431            0.093844   \n",
      "...                                 ...               ...                 ...   \n",
      "6176                           0.464005          0.464005            0.020577   \n",
      "6177                           0.330244          0.330244            0.116064   \n",
      "6178                           0.164340          0.164340            0.056622   \n",
      "6179                           0.618107          0.618107            0.109629   \n",
      "6180                           0.492637          0.492637            0.028429   \n",
      "\n",
      "      scenery_greenery  nature--water  scenery_water   city  environment  \n",
      "0             0.023707            0.0            0.0  Tokyo     7.666944  \n",
      "1             0.063221            0.0            0.0  Tokyo     6.675860  \n",
      "2             0.020380            0.0            0.0  Tokyo     7.997375  \n",
      "3             0.073956            0.0            0.0  Tokyo     6.945001  \n",
      "4             0.093844            0.0            0.0  Tokyo     5.696554  \n",
      "...                ...            ...            ...    ...          ...  \n",
      "6176          0.020577            0.0            0.0  Tokyo     6.591233  \n",
      "6177          0.116064            0.0            0.0  Tokyo     7.252214  \n",
      "6178          0.056622            0.0            0.0  Tokyo     5.596910  \n",
      "6179          0.109629            0.0            0.0  Tokyo     6.074682  \n",
      "6180          0.028429            0.0            0.0  Tokyo     7.700299  \n",
      "\n",
      "[6181 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_list=['slope',\n",
    "           'poi',\n",
    "           'land_use',\n",
    "           'aqi',\n",
    "           'scenery_building',\n",
    "           'scenery_greenery',\n",
    "           'scenery_water'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        data_df=data_df.rename(columns={data_df.columns[0]:'pano_id',data_df.columns[-1]:data})\n",
    "    \n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        ) \n",
    "        location_df.to_csv(os.path.join(root,'data/tabular_data/{}/environment_before_scaling.csv'.format(city)))\n",
    "        \n",
    "# scaling\n",
    "# put both cities data together first\n",
    "df_temp=pd.DataFrame()\n",
    "for city in city_list:\n",
    "    environment_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/environment_before_scaling.csv'.format(city)))\n",
    "    environment_df['city']=city\n",
    "    df_temp=df_temp.append(environment_df)\n",
    "# scaling using Min-Max scale\n",
    "cols_transfomation_positive=['poi']\n",
    "cols_transfomation_negative=['slope','aqi']\n",
    "# fit the scaler\n",
    "scaler = MinMaxScaler()\n",
    "df_temp[cols_transfomation_positive]=scaler.fit_transform(df_temp[cols_transfomation_positive])\n",
    "df_temp[cols_transfomation_negative]=scaler.fit_transform(df_temp[cols_transfomation_negative])\n",
    "df_temp[cols_transfomation_negative]=1-df_temp[cols_transfomation_negative]\n",
    "\n",
    "# calculate the indicator\n",
    "df_temp['environment']=df_temp[data_list].sum(axis=1)*(20/len(data_list))\n",
    "# save seperately by cities\n",
    "for city in city_list:\n",
    "    df_temp_city=df_temp.loc[df_temp['city']==city]\n",
    "    print(df_temp_city)\n",
    "    df_temp_city.to_csv(os.path.join(root,'data/tabular_data/{}/environment.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-corpus",
   "metadata": {},
   "source": [
    "## Infrastructure\n",
    "- Type of road\n",
    "- Presence of potholes\n",
    "- Presence of street light\n",
    "- Presence of bike lanes\n",
    "- Number of transit facilities\n",
    "- Type of pavement (paved vs unpaved)\n",
    "- Presence of street amenities (e.g., trash cans and benches)\n",
    "- Presence of utility pole\n",
    "- Presence of bike parking\n",
    "- Road width\n",
    "- Presence of sidewalk\n",
    "- Presense and quality of crosswalk (with/without traffic lights, traffic signs)\n",
    "- Presence of curb cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loaded-leone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Unnamed: 0.1  \\\n",
      "0              0             0   \n",
      "1              1             1   \n",
      "2              2             2   \n",
      "3              3             3   \n",
      "4              4             4   \n",
      "...          ...           ...   \n",
      "5828        5828          6115   \n",
      "5829        5829          6116   \n",
      "5830        5830          6117   \n",
      "5831        5831          6119   \n",
      "5832        5832          6120   \n",
      "\n",
      "                                                pano_id     panoLon   panoLat  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ  103.914512  1.400917   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA  103.965906  1.375535   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA  103.819038  1.293706   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA  103.703747  1.341976   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw  103.900929  1.311461   \n",
      "...                                                 ...         ...       ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...  103.835671  1.289808   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg  103.966108  1.373809   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ  103.701994  1.344878   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  103.632836  1.324383   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A  103.747407  1.428744   \n",
      "\n",
      "      distDiff      length_x  road_type_score  road_type  object--pothole  \\\n",
      "0     0.000084   3398.820525      2376.222393   0.699131              0.0   \n",
      "1     0.000015   1234.670526       396.184948   0.320883              0.0   \n",
      "2     0.000104   1266.703089      1013.362471   0.800000              0.0   \n",
      "3     0.000184   2650.131379      1376.420536   0.519378              0.0   \n",
      "4     0.000057   3137.033379      2383.607464   0.759829              0.0   \n",
      "...        ...           ...              ...        ...              ...   \n",
      "5828  0.000131   3117.833985      1694.088925   0.543354              0.0   \n",
      "5829  0.000204   1777.530090       576.130366   0.324118              0.0   \n",
      "5830  0.000181   2960.843490      1839.297329   0.621207              0.0   \n",
      "5831  0.000031  10916.937697      3001.333273   0.274924              0.0   \n",
      "5832  0.000018   2388.447673       964.126996   0.403663              0.0   \n",
      "\n",
      "      ...  road_width  construction--flat--sidewalk  side_walk  \\\n",
      "0     ...         0.0                      0.014104          1   \n",
      "1     ...         0.0                      0.002032          1   \n",
      "2     ...         0.0                      0.017012          1   \n",
      "3     ...         0.0                      0.006920          1   \n",
      "4     ...         0.0                      0.031495          1   \n",
      "...   ...         ...                           ...        ...   \n",
      "5828  ...         0.0                      0.000066          1   \n",
      "5829  ...         0.0                      0.008632          1   \n",
      "5830  ...         0.0                      0.011404          1   \n",
      "5831  ...         0.0                      0.005613          1   \n",
      "5832  ...         0.0                      0.023238          1   \n",
      "\n",
      "      construction--flat--crosswalk-plain  marking--crosswalk-zebra  \\\n",
      "0                                0.000000                  0.000000   \n",
      "1                                0.000000                  0.000000   \n",
      "2                                0.000000                  0.000000   \n",
      "3                                0.000000                  0.000000   \n",
      "4                                0.000000                  0.000000   \n",
      "...                                   ...                       ...   \n",
      "5828                             0.000000                  0.000000   \n",
      "5829                             0.000000                  0.000000   \n",
      "5830                             0.000000                  0.000000   \n",
      "5831                             0.000073                  0.000057   \n",
      "5832                             0.000000                  0.000000   \n",
      "\n",
      "      cross_walk  construction--flat--curb-cut  accessibility       city  \\\n",
      "0              0                      0.000000              0  Singapore   \n",
      "1              0                      0.000000              0  Singapore   \n",
      "2              0                      0.001079              1  Singapore   \n",
      "3              0                      0.000474              1  Singapore   \n",
      "4              0                      0.000152              1  Singapore   \n",
      "...          ...                           ...            ...        ...   \n",
      "5828           0                      0.000000              0  Singapore   \n",
      "5829           0                      0.000000              0  Singapore   \n",
      "5830           0                      0.000000              0  Singapore   \n",
      "5831           1                      0.000009              1  Singapore   \n",
      "5832           0                      0.000125              1  Singapore   \n",
      "\n",
      "      infrastructure  \n",
      "0          10.306356  \n",
      "1           8.185974  \n",
      "2          10.461538  \n",
      "3           8.747761  \n",
      "4          11.938198  \n",
      "...              ...  \n",
      "5828       10.027272  \n",
      "5829        6.652490  \n",
      "5830        8.648011  \n",
      "5831        9.653730  \n",
      "5832        9.851789  \n",
      "\n",
      "[5833 rows x 39 columns]\n",
      "      Unnamed: 0  Unnamed: 0.1                      pano_id     panoLon  \\\n",
      "0              0             0  pano=d1hU7t4QKuXqckKk29ENTQ  139.682778   \n",
      "1              1             1  pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401   \n",
      "2              2             2  pano=LySKMao7KBtKwGBAST6INA  139.866208   \n",
      "3              3             3  pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055   \n",
      "4              4             4  pano=pLawV8B_NY4JFfpRr8v-3A  139.622156   \n",
      "...          ...           ...                          ...         ...   \n",
      "6176        6176          6278  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896   \n",
      "6177        6177          6279  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517   \n",
      "6178        6178          6280  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967   \n",
      "6179        6179          6281  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638   \n",
      "6180        6180          6282  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100   \n",
      "\n",
      "        panoLat  distDiff     length_x  road_type_score  road_type  \\\n",
      "0     35.574288  0.000018  1389.245355        54.933858   0.039542   \n",
      "1     35.723570  0.000001  1209.563623       480.956193   0.397628   \n",
      "2     35.664492  0.000007  1653.129062       598.541645   0.362066   \n",
      "3     35.607340  0.000356  1248.126280       573.968477   0.459864   \n",
      "4     35.766678  0.000014   860.187652       528.073097   0.613905   \n",
      "...         ...       ...          ...              ...        ...   \n",
      "6176  35.694996  0.000006  2253.739947      1083.989037   0.480973   \n",
      "6177  35.704148  0.000004  1571.173749       756.790662   0.481672   \n",
      "6178  35.725836  0.000009  1586.877124      1199.104086   0.755638   \n",
      "6179  35.756726  0.000188  1527.305076       250.662923   0.164121   \n",
      "6180  35.751490  0.000019  1493.412598       554.749193   0.371464   \n",
      "\n",
      "      object--pothole  ...  road_width  construction--flat--sidewalk  \\\n",
      "0                 0.0  ...         0.0                      0.010486   \n",
      "1                 0.0  ...         0.0                      0.024543   \n",
      "2                 0.0  ...         0.0                      0.012275   \n",
      "3                 0.0  ...         0.0                      0.009009   \n",
      "4                 0.0  ...         0.0                      0.043199   \n",
      "...               ...  ...         ...                           ...   \n",
      "6176              0.0  ...         0.0                      0.082785   \n",
      "6177              0.0  ...         0.0                      0.015377   \n",
      "6178              0.0  ...         0.0                      0.006485   \n",
      "6179              0.0  ...         0.0                      0.032980   \n",
      "6180              0.0  ...         0.0                      0.016627   \n",
      "\n",
      "      side_walk  construction--flat--crosswalk-plain  \\\n",
      "0             1                                  0.0   \n",
      "1             1                                  0.0   \n",
      "2             1                                  0.0   \n",
      "3             1                                  0.0   \n",
      "4             1                                  0.0   \n",
      "...         ...                                  ...   \n",
      "6176          1                                  0.0   \n",
      "6177          1                                  0.0   \n",
      "6178          1                                  0.0   \n",
      "6179          1                                  0.0   \n",
      "6180          1                                  0.0   \n",
      "\n",
      "      marking--crosswalk-zebra  cross_walk  construction--flat--curb-cut  \\\n",
      "0                     0.000000           0                      0.000012   \n",
      "1                     0.000000           0                      0.000000   \n",
      "2                     0.000000           0                      0.000045   \n",
      "3                     0.000000           0                      0.000000   \n",
      "4                     0.000000           0                      0.001259   \n",
      "...                        ...         ...                           ...   \n",
      "6176                  0.000418           1                      0.000021   \n",
      "6177                  0.101890           1                      0.000000   \n",
      "6178                  0.008876           1                      0.000120   \n",
      "6179                  0.000000           0                      0.000000   \n",
      "6180                  0.000000           0                      0.000000   \n",
      "\n",
      "      accessibility   city  infrastructure  \n",
      "0                 1  Tokyo        8.778783  \n",
      "1                 0  Tokyo        4.457889  \n",
      "2                 1  Tokyo        5.941640  \n",
      "3                 0  Tokyo        7.886970  \n",
      "4                 1  Tokyo       10.175238  \n",
      "...             ...    ...             ...  \n",
      "6176              1  Tokyo        8.140843  \n",
      "6177              0  Tokyo       10.228214  \n",
      "6178              1  Tokyo        8.854827  \n",
      "6179              0  Tokyo        3.329417  \n",
      "6180              0  Tokyo        5.186868  \n",
      "\n",
      "[6181 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_list=['road_type',\n",
    "           'pavement',\n",
    "           'street_light',\n",
    "           'bike_lanes',\n",
    "           'transit',\n",
    "           'surface_type',\n",
    "           'street_amenity',\n",
    "           'utility_pole',\n",
    "           'bike_parking',\n",
    "           'road_width',\n",
    "           'side_walk',\n",
    "           'cross_walk',\n",
    "           'accessibility'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        data_df=data_df.rename(columns={data_df.columns[0]:'pano_id',data_df.columns[-1]:data})\n",
    "    \n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        ) \n",
    "        location_df.to_csv(os.path.join(root,'data/tabular_data/{}/infrastructure_before_scaling.csv'.format(city)))\n",
    "        \n",
    "# scaling\n",
    "# put both cities data together first\n",
    "df_temp=pd.DataFrame()\n",
    "for city in city_list:\n",
    "    infrastructure_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/infrastructure_before_scaling.csv'.format(city)))\n",
    "    infrastructure_df['city']=city\n",
    "    df_temp=df_temp.append(infrastructure_df)\n",
    "# scaling using Min-Max scale\n",
    "cols_transfomation_positive=['transit']\n",
    "# fit the scaler\n",
    "scaler = MinMaxScaler()\n",
    "df_temp[cols_transfomation_positive]=scaler.fit_transform(df_temp[cols_transfomation_positive])\n",
    "# 1 if >0\n",
    "cols_transfomation_above_0=['street_light','bike_lanes','street_amenity','bike_parking',\n",
    "                            'side_walk','cross_walk','accessibility']\n",
    "for col in cols_transfomation_above_0:\n",
    "    cond_list=[df_temp[col]>0,df_temp[col]==0]\n",
    "    choice_list=[1,0]\n",
    "    df_temp[col]=np.select(cond_list,choice_list)\n",
    "# 1 if ==0\n",
    "cols_transfomation_equal_to_0=['pavement','utility_pole']\n",
    "for col in cols_transfomation_equal_to_0:\n",
    "    cond_list=[df_temp[col]==0,df_temp[col]>0]\n",
    "    choice_list=[1,0]\n",
    "    df_temp[col]=np.select(cond_list,choice_list)\n",
    "\n",
    "# calculate the indicator\n",
    "df_temp['infrastructure']=df_temp[data_list].sum(axis=1)*(20/len(data_list))\n",
    "# save seperately by cities\n",
    "for city in city_list:\n",
    "    df_temp_city=df_temp.loc[df_temp['city']==city]\n",
    "    print(df_temp_city)\n",
    "    df_temp_city.to_csv(os.path.join(root,'data/tabular_data/{}/infrastructure.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-queue",
   "metadata": {},
   "source": [
    "## Vehicle_Cyclist_Interaction\n",
    "- No. of vehicles\n",
    "- Presence of off-street parking lot spaces\n",
    "- Number of speed bumps / choker / roundabout\n",
    "- traffic light / stop sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unable-oakland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0  Unnamed: 0.1      panoLon      panoLat      distDiff  \\\n",
      "count  5833.000000   5833.000000  5833.000000  5833.000000  5.833000e+03   \n",
      "mean   2916.000000   3061.086233   103.827390     1.347023  7.049910e-05   \n",
      "std    1683.986391   1764.040159     0.084191     0.044801  1.839192e-04   \n",
      "min       0.000000      0.000000   103.611153     1.244036  3.210881e-07   \n",
      "25%    1458.000000   1537.000000   103.764845     1.313970  2.542029e-05   \n",
      "50%    2916.000000   3060.000000   103.837668     1.339950  4.351301e-05   \n",
      "75%    4374.000000   4583.000000   103.889764     1.375763  7.797647e-05   \n",
      "max    5832.000000   6120.000000   104.025514     1.469106  8.622213e-03   \n",
      "\n",
      "       no_of_vehicles         length  street_parking_score  street_parking  \\\n",
      "count     5833.000000    5833.000000           5833.000000     5833.000000   \n",
      "mean         0.881842    5097.670675           5072.027550        0.994098   \n",
      "std          0.136240    4607.236936           4602.713191        0.021811   \n",
      "min          0.000000      89.742072             89.742072        0.678585   \n",
      "25%          0.833846    2837.367882           2812.354760        1.000000   \n",
      "50%          0.926154    3941.160217           3921.735588        1.000000   \n",
      "75%          0.978462    5568.314476           5539.596737        1.000000   \n",
      "max          1.000000  107415.968609         107415.968609        1.000000   \n",
      "\n",
      "       traffic_calming  traffic light    stop sign  traffic_light_stop_sign  \\\n",
      "count      5833.000000    5833.000000  5833.000000              5833.000000   \n",
      "mean          0.017144       0.639808     0.125664                 0.206069   \n",
      "std           0.129818       2.006515     0.581329                 0.404515   \n",
      "min           0.000000       0.000000     0.000000                 0.000000   \n",
      "25%           0.000000       0.000000     0.000000                 0.000000   \n",
      "50%           0.000000       0.000000     0.000000                 0.000000   \n",
      "75%           0.000000       0.000000     0.000000                 0.000000   \n",
      "max           1.000000      25.000000     8.000000                 1.000000   \n",
      "\n",
      "       vehicle_cyclist_interaction  \n",
      "count                  5833.000000  \n",
      "mean                     10.495766  \n",
      "std                       2.116221  \n",
      "min                       5.076923  \n",
      "25%                       9.362270  \n",
      "50%                       9.815385  \n",
      "75%                      10.000000  \n",
      "max                      20.000000  \n",
      "        Unnamed: 0  Unnamed: 0.1      panoLon      panoLat      distDiff  \\\n",
      "count  6181.000000   6181.000000  6181.000000  6181.000000  6.181000e+03   \n",
      "mean   3090.000000   3140.694871   139.729633    35.698696  4.895514e-05   \n",
      "std    1784.445339   1814.754371     0.082578     0.055719  1.382018e-04   \n",
      "min       0.000000      0.000000   139.564319    35.539842  1.000000e-07   \n",
      "25%    1545.000000   1568.000000   139.664294    35.664903  8.464632e-06   \n",
      "50%    3090.000000   3143.000000   139.725885    35.702985  2.039338e-05   \n",
      "75%    4635.000000   4711.000000   139.790308    35.741515  4.807149e-05   \n",
      "max    6180.000000   6282.000000   139.914750    35.816048  5.846748e-03   \n",
      "\n",
      "       no_of_vehicles        length  street_parking_score  street_parking  \\\n",
      "count     6181.000000   6181.000000           6181.000000     6181.000000   \n",
      "mean         0.961692   2505.478864           2505.093753        0.999880   \n",
      "std          0.055851   3042.046113           3041.878694        0.002474   \n",
      "min          0.330769    341.342518            341.342518        0.921248   \n",
      "25%          0.950769   1320.828993           1320.828993        1.000000   \n",
      "50%          0.981538   1697.444037           1697.444037        1.000000   \n",
      "75%          0.996923   2452.283044           2449.696715        1.000000   \n",
      "max          1.000000  45811.670434          45811.670434        1.000000   \n",
      "\n",
      "       traffic_calming  traffic light    stop sign  traffic_light_stop_sign  \\\n",
      "count      6181.000000    6181.000000  6181.000000              6181.000000   \n",
      "mean          0.002427       0.207086     0.047242                 0.120531   \n",
      "std           0.049207       0.890704     0.265066                 0.325607   \n",
      "min           0.000000       0.000000     0.000000                 0.000000   \n",
      "25%           0.000000       0.000000     0.000000                 0.000000   \n",
      "50%           0.000000       0.000000     0.000000                 0.000000   \n",
      "75%           0.000000       0.000000     0.000000                 0.000000   \n",
      "max           1.000000      16.000000     4.000000                 1.000000   \n",
      "\n",
      "       vehicle_cyclist_interaction  \n",
      "count                  6181.000000  \n",
      "mean                     10.422648  \n",
      "std                       1.607225  \n",
      "min                       6.676923  \n",
      "25%                       9.815385  \n",
      "50%                       9.930769  \n",
      "75%                      10.000000  \n",
      "max                      18.615385  \n"
     ]
    }
   ],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_list=['no_of_vehicles',\n",
    "           'street_parking',\n",
    "           'traffic_calming',\n",
    "           'traffic_light_stop_sign'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        data_df=data_df.rename(columns={data_df.columns[0]:'pano_id',data_df.columns[-1]:data})\n",
    "    \n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        ) \n",
    "        location_df.to_csv(os.path.join(root,'data/tabular_data/{}/vehicle_cyclist_interaction_before_scaling.csv'.format(city)))\n",
    "        \n",
    "# scaling\n",
    "# put both cities data together first\n",
    "df_temp=pd.DataFrame()\n",
    "for city in city_list:\n",
    "    vehicle_cyclist_interaction_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/vehicle_cyclist_interaction_before_scaling.csv'.format(city)))\n",
    "    vehicle_cyclist_interaction_df['city']=city\n",
    "    df_temp=df_temp.append(vehicle_cyclist_interaction_df)\n",
    "\n",
    "# scaling using Min-Max scale\n",
    "cols_transfomation_negative=['no_of_vehicles']\n",
    "df_temp[cols_transfomation_negative]=scaler.fit_transform(df_temp[cols_transfomation_negative])\n",
    "df_temp[cols_transfomation_negative]=1-df_temp[cols_transfomation_negative]\n",
    "\n",
    "# 1 if >0\n",
    "cols_transfomation_above_0=['traffic_calming']\n",
    "for col in cols_transfomation_above_0:\n",
    "    cond_list=[df_temp[col]>0]\n",
    "    choice_list=[1]\n",
    "    df_temp[col]=np.select(cond_list,choice_list)\n",
    "\n",
    "# calculate the indicator\n",
    "df_temp['vehicle_cyclist_interaction']=df_temp[data_list].sum(axis=1)*(20/len(data_list))\n",
    "# save seperately by cities\n",
    "for city in city_list:\n",
    "    df_temp_city=df_temp.loc[df_temp['city']==city]\n",
    "    print(df_temp_city.describe())\n",
    "    df_temp_city.to_csv(os.path.join(root,'data/tabular_data/{}/vehicle_cyclist_interaction.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-arlington",
   "metadata": {},
   "source": [
    "## Perception\n",
    "- beauty\n",
    "- building_attractiveness\n",
    "- cleanliness\n",
    "-  cycling_attractiveness\n",
    "- living_attractiveness\n",
    "- safety\n",
    "- spaciousness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "objective-laptop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                            pano_id  \\\n",
      "0              0                        pano=EIzSniBOXDgT4m2fGH2SNQ   \n",
      "1              1                        pano=tzvazBY0ag5RgAsvJgHvZA   \n",
      "2              2                        pano=FJjCz3jIAWYYnGq4PvR9HA   \n",
      "3              3                        pano=TfQN73pv-6mAZy2QnMIAPA   \n",
      "4              4                        pano=PATYQYa4O43_LjJ_JMbrFw   \n",
      "...          ...                                                ...   \n",
      "5828        6115  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...   \n",
      "5829        6116                        pano=4I7X92FmvGD7ALNXMZQuAg   \n",
      "5830        6117                        pano=EghtrEJcRO-l7XlA21zaNQ   \n",
      "5831        6119                        pano=O6UhfdrDWVrKDmJTaOLmFA   \n",
      "5832        6120                        pano=uvufId4xvxyRl-4TUehx_A   \n",
      "\n",
      "         panoLon   panoLat  distDiff  predicted_beauty  \\\n",
      "0     103.914512  1.400917  0.000084          0.677530   \n",
      "1     103.965906  1.375535  0.000015          0.632164   \n",
      "2     103.819038  1.293706  0.000104          0.704385   \n",
      "3     103.703747  1.341976  0.000184          0.635131   \n",
      "4     103.900929  1.311461  0.000057          0.698837   \n",
      "...          ...       ...       ...               ...   \n",
      "5828  103.835671  1.289808  0.000131          0.659081   \n",
      "5829  103.966108  1.373809  0.000204          0.680109   \n",
      "5830  103.701994  1.344878  0.000181          0.659934   \n",
      "5831  103.632836  1.324383  0.000031          0.593144   \n",
      "5832  103.747407  1.428744  0.000018          0.641129   \n",
      "\n",
      "      predicted_building_attractiveness  predicted_cleanliness  \\\n",
      "0                              0.649293               0.620930   \n",
      "1                              0.602554               0.731595   \n",
      "2                              0.630920               0.697841   \n",
      "3                              0.673030               0.675438   \n",
      "4                              0.666674               0.696352   \n",
      "...                                 ...                    ...   \n",
      "5828                           0.693948               0.702037   \n",
      "5829                           0.645774               0.684541   \n",
      "5830                           0.672811               0.672323   \n",
      "5831                           0.539087               0.650600   \n",
      "5832                           0.591219               0.636158   \n",
      "\n",
      "      predicted_cycling_attractiveness  predicted_living_attractiveness  \\\n",
      "0                             0.641197                         0.606929   \n",
      "1                             0.673347                         0.541503   \n",
      "2                             0.652612                         0.640286   \n",
      "3                             0.685385                         0.642010   \n",
      "4                             0.653492                         0.541376   \n",
      "...                                ...                              ...   \n",
      "5828                          0.645423                         0.667435   \n",
      "5829                          0.620375                         0.619863   \n",
      "5830                          0.632509                         0.655399   \n",
      "5831                          0.587558                         0.580468   \n",
      "5832                          0.660222                         0.582433   \n",
      "\n",
      "      predicted_safety  predicted_spaciousness  perception  \n",
      "0             0.649739                0.669198   12.899473  \n",
      "1             0.697143                0.727729   13.160099  \n",
      "2             0.647508                0.622054   13.130305  \n",
      "3             0.623593                0.618402   13.008538  \n",
      "4             0.674039                0.642970   13.067830  \n",
      "...                ...                     ...         ...  \n",
      "5828          0.645942                0.715142   13.511453  \n",
      "5829          0.692321                0.647682   13.116181  \n",
      "5830          0.658174                0.600460   13.004601  \n",
      "5831          0.564475                0.603476   11.768021  \n",
      "5832          0.584185                0.596253   12.261711  \n",
      "\n",
      "[5833 rows x 13 columns]\n",
      "      Unnamed: 0                      pano_id     panoLon    panoLat  \\\n",
      "0              0  pano=d1hU7t4QKuXqckKk29ENTQ  139.682778  35.574288   \n",
      "1              1  pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401  35.723570   \n",
      "2              2  pano=LySKMao7KBtKwGBAST6INA  139.866208  35.664492   \n",
      "3              3  pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055  35.607340   \n",
      "4              4  pano=pLawV8B_NY4JFfpRr8v-3A  139.622156  35.766678   \n",
      "...          ...                          ...         ...        ...   \n",
      "6176        6278  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896  35.694996   \n",
      "6177        6279  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517  35.704148   \n",
      "6178        6280  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967  35.725836   \n",
      "6179        6281  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638  35.756726   \n",
      "6180        6282  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100  35.751490   \n",
      "\n",
      "      distDiff  predicted_beauty  predicted_building_attractiveness  \\\n",
      "0     0.000018          0.638351                           0.577424   \n",
      "1     0.000001          0.632302                           0.606694   \n",
      "2     0.000007          0.658208                           0.580650   \n",
      "3     0.000356          0.706518                           0.649421   \n",
      "4     0.000014          0.647067                           0.649909   \n",
      "...        ...               ...                                ...   \n",
      "6176  0.000006          0.676970                           0.635674   \n",
      "6177  0.000004          0.674562                           0.619229   \n",
      "6178  0.000009          0.650342                           0.604554   \n",
      "6179  0.000188          0.571638                           0.650618   \n",
      "6180  0.000019          0.608041                           0.595031   \n",
      "\n",
      "      predicted_cleanliness  predicted_cycling_attractiveness  \\\n",
      "0                  0.635693                          0.607607   \n",
      "1                  0.652504                          0.572392   \n",
      "2                  0.599565                          0.593416   \n",
      "3                  0.675081                          0.623128   \n",
      "4                  0.704725                          0.637031   \n",
      "...                     ...                               ...   \n",
      "6176               0.694554                          0.662930   \n",
      "6177               0.608862                          0.651621   \n",
      "6178               0.638272                          0.608812   \n",
      "6179               0.642959                          0.521745   \n",
      "6180               0.549143                          0.513747   \n",
      "\n",
      "      predicted_living_attractiveness  predicted_safety  \\\n",
      "0                            0.598252          0.602905   \n",
      "1                            0.601970          0.573260   \n",
      "2                            0.634201          0.593787   \n",
      "3                            0.641235          0.633221   \n",
      "4                            0.595194          0.616714   \n",
      "...                               ...               ...   \n",
      "6176                         0.693628          0.647327   \n",
      "6177                         0.593059          0.616812   \n",
      "6178                         0.618361          0.559664   \n",
      "6179                         0.568837          0.624252   \n",
      "6180                         0.519538          0.577310   \n",
      "\n",
      "      predicted_spaciousness  perception  \n",
      "0                   0.649034   12.312189  \n",
      "1                   0.545333   11.955588  \n",
      "2                   0.578003   12.108087  \n",
      "3                   0.636739   13.043837  \n",
      "4                   0.624810   12.786997  \n",
      "...                      ...         ...  \n",
      "6176                0.661356   13.349823  \n",
      "6177                0.613055   12.506292  \n",
      "6178                0.546304   12.075167  \n",
      "6179                0.561992   11.834402  \n",
      "6180                0.560490   11.209428  \n",
      "\n",
      "[6181 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "\n",
    "data_list=['beauty',\n",
    "          'building_attractiveness','cleanliness','cycling_attractiveness',\n",
    "          'living_attractiveness','safety','spaciousness'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/predicted_{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        )\n",
    "    # scaling\n",
    "    location_df = location_df.apply(lambda x: x/10 if x.name.startswith('predicted_') else x)\n",
    "    location_df['perception']=location_df[location_df.columns[pd.Series(location_df.columns).str.startswith('predicted_')]].\\\n",
    "        sum(axis=1)*(20/len(data_list))\n",
    "    print(location_df)\n",
    "    location_df.to_csv(os.path.join(root,'data/tabular_data/{}/perception.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-sydney",
   "metadata": {},
   "source": [
    "## Bikeability\n",
    "- connectivity\n",
    "- environment\n",
    "- infrastructure\n",
    "- vehicle cyclist interaction\n",
    "- perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mexican-taylor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                pano_id     panoLon   panoLat  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ  103.914512  1.400917   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA  103.965906  1.375535   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA  103.819038  1.293706   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA  103.703747  1.341976   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw  103.900929  1.311461   \n",
      "...                                                 ...         ...       ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...  103.835671  1.289808   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg  103.966108  1.373809   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ  103.701994  1.344878   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  103.632836  1.324383   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A  103.747407  1.428744   \n",
      "\n",
      "      distDiff  connectivity  environment  infrastructure  \\\n",
      "0     0.000084     16.543840     6.028534       10.306356   \n",
      "1     0.000015     17.195767     6.197699        8.185974   \n",
      "2     0.000104     16.373772     5.177653       10.461538   \n",
      "3     0.000184     12.830688     7.477677        8.747761   \n",
      "4     0.000057     17.063492     6.286261       11.938198   \n",
      "...        ...           ...          ...             ...   \n",
      "5828  0.000131     17.012472     6.993153       10.027272   \n",
      "5829  0.000204     17.142857     6.203662        6.652490   \n",
      "5830  0.000181     13.930461     7.189436        8.648011   \n",
      "5831  0.000031     18.412698     4.689534        9.653730   \n",
      "5832  0.000018     19.484127     5.108357        9.851789   \n",
      "\n",
      "      vehicle_cyclist_interaction  perception  bikeability  \n",
      "0                        9.876923   12.899473    55.655126  \n",
      "1                        9.424588   13.160099    54.164127  \n",
      "2                        9.676923   13.130305    54.820192  \n",
      "3                       13.200000   13.008538    55.264664  \n",
      "4                        8.398851   13.067830    56.754631  \n",
      "...                           ...         ...          ...  \n",
      "5828                    10.000000   13.511453    57.544349  \n",
      "5829                     9.707246   13.116181    52.822436  \n",
      "5830                     9.746154   13.004601    52.518663  \n",
      "5831                    14.815385   11.768021    59.339368  \n",
      "5832                     9.930769   12.261711    56.636753  \n",
      "\n",
      "[5833 rows x 10 columns]\n",
      "                          pano_id     panoLon    panoLat  distDiff  \\\n",
      "0     pano=d1hU7t4QKuXqckKk29ENTQ  139.682778  35.574288  0.000018   \n",
      "1     pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401  35.723570  0.000001   \n",
      "2     pano=LySKMao7KBtKwGBAST6INA  139.866208  35.664492  0.000007   \n",
      "3     pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055  35.607340  0.000356   \n",
      "4     pano=pLawV8B_NY4JFfpRr8v-3A  139.622156  35.766678  0.000014   \n",
      "...                           ...         ...        ...       ...   \n",
      "6176  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896  35.694996  0.000006   \n",
      "6177  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517  35.704148  0.000004   \n",
      "6178  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967  35.725836  0.000009   \n",
      "6179  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638  35.756726  0.000188   \n",
      "6180  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100  35.751490  0.000019   \n",
      "\n",
      "      connectivity  environment  infrastructure  vehicle_cyclist_interaction  \\\n",
      "0        18.928571     7.666944        8.778783                     9.769231   \n",
      "1        17.063492     6.675860        4.457889                     9.907692   \n",
      "2        16.967120     7.997375        5.941640                     9.723077   \n",
      "3        20.000000     6.945001        7.886970                    10.000000   \n",
      "4        19.614512     5.696554       10.175238                     9.930769   \n",
      "...            ...          ...             ...                          ...   \n",
      "6176     17.103175     6.591233        8.140843                     9.792308   \n",
      "6177     19.047619     7.252214       10.228214                     9.846154   \n",
      "6178     20.000000     5.596910        8.854827                    10.000000   \n",
      "6179     19.642857     6.074682        3.329417                     9.961538   \n",
      "6180     18.928571     7.700299        5.186868                     9.769231   \n",
      "\n",
      "      perception  bikeability  \n",
      "0      12.312189    57.455717  \n",
      "1      11.955588    50.060521  \n",
      "2      12.108087    52.737299  \n",
      "3      13.043837    57.875808  \n",
      "4      12.786997    58.204071  \n",
      "...          ...          ...  \n",
      "6176   13.349823    54.977381  \n",
      "6177   12.506292    58.880492  \n",
      "6178   12.075167    56.526904  \n",
      "6179   11.834402    50.842896  \n",
      "6180   11.209428    52.794397  \n",
      "\n",
      "[6181 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "\n",
    "data_list=['connectivity',\n",
    "           'environment',\n",
    "           'infrastructure',\n",
    "           'vehicle_cyclist_interaction',\n",
    "           'perception'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df[['pano_id',data]],\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        )\n",
    "        location_df.drop(location_df.filter(regex='Unnamed').columns, axis=1, inplace=True)\n",
    "        \n",
    "    # scaling\n",
    "    location_df['bikeability']=location_df.iloc[:,-5:].sum(axis=1)\n",
    "    print(location_df)\n",
    "    location_df.to_csv(os.path.join(root,'data/tabular_data/{}/bikeability.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-destruction",
   "metadata": {},
   "source": [
    "## only Non-SVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-acoustic",
   "metadata": {},
   "source": [
    "### Connectivity\n",
    "- Intersection with traffic lights\n",
    "- Intersection without traffic lights\n",
    "- Cul de sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rapid-treat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Unnamed: 0.1  \\\n",
      "0              0             0   \n",
      "1              1             1   \n",
      "2              2             2   \n",
      "3              3             3   \n",
      "4              4             4   \n",
      "...          ...           ...   \n",
      "5828        5828          6115   \n",
      "5829        5829          6116   \n",
      "5830        5830          6117   \n",
      "5831        5831          6119   \n",
      "5832        5832          6120   \n",
      "\n",
      "                                                pano_id     panoLon   panoLat  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ  103.914512  1.400917   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA  103.965906  1.375535   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA  103.819038  1.293706   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA  103.703747  1.341976   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw  103.900929  1.311461   \n",
      "...                                                 ...         ...       ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...  103.835671  1.289808   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg  103.966108  1.373809   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ  103.701994  1.344878   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  103.632836  1.324383   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A  103.747407  1.428744   \n",
      "\n",
      "      distDiff  intersections_with_traffic_lights  \\\n",
      "0     0.000084                           0.979592   \n",
      "1     0.000015                           1.000000   \n",
      "2     0.000104                           0.846939   \n",
      "3     0.000184                           0.928571   \n",
      "4     0.000057                           1.000000   \n",
      "...        ...                                ...   \n",
      "5828  0.000131                           0.938776   \n",
      "5829  0.000204                           1.000000   \n",
      "5830  0.000181                           0.867347   \n",
      "5831  0.000031                           1.000000   \n",
      "5832  0.000018                           1.000000   \n",
      "\n",
      "      intersections_without_traffic_lights  cul_de_sac       city  \\\n",
      "0                                 0.613095    0.888889  Singapore   \n",
      "1                                 0.690476    0.888889  Singapore   \n",
      "2                                 0.720238    0.888889  Singapore   \n",
      "3                                 0.107143    0.888889  Singapore   \n",
      "4                                 0.559524    1.000000  Singapore   \n",
      "...                                    ...         ...        ...   \n",
      "5828                              0.613095    1.000000  Singapore   \n",
      "5829                              0.571429    1.000000  Singapore   \n",
      "5830                              0.333333    0.888889  Singapore   \n",
      "5831                              0.761905    1.000000  Singapore   \n",
      "5832                              0.922619    1.000000  Singapore   \n",
      "\n",
      "      connectivity  \n",
      "0        20.679800  \n",
      "1        21.494709  \n",
      "2        20.467215  \n",
      "3        16.038360  \n",
      "4        21.329365  \n",
      "...            ...  \n",
      "5828     21.265590  \n",
      "5829     21.428571  \n",
      "5830     17.413076  \n",
      "5831     23.015873  \n",
      "5832     24.355159  \n",
      "\n",
      "[5833 rows x 11 columns]\n",
      "      Unnamed: 0  Unnamed: 0.1                      pano_id     panoLon  \\\n",
      "0              0             0  pano=d1hU7t4QKuXqckKk29ENTQ  139.682778   \n",
      "1              1             1  pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401   \n",
      "2              2             2  pano=LySKMao7KBtKwGBAST6INA  139.866208   \n",
      "3              3             3  pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055   \n",
      "4              4             4  pano=pLawV8B_NY4JFfpRr8v-3A  139.622156   \n",
      "...          ...           ...                          ...         ...   \n",
      "6176        6176          6278  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896   \n",
      "6177        6177          6279  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517   \n",
      "6178        6178          6280  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967   \n",
      "6179        6179          6281  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638   \n",
      "6180        6180          6282  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100   \n",
      "\n",
      "        panoLat  distDiff  intersections_with_traffic_lights  \\\n",
      "0     35.574288  0.000018                           1.000000   \n",
      "1     35.723570  0.000001                           1.000000   \n",
      "2     35.664492  0.000007                           0.622449   \n",
      "3     35.607340  0.000356                           1.000000   \n",
      "4     35.766678  0.000014                           0.989796   \n",
      "...         ...       ...                                ...   \n",
      "6176  35.694996  0.000006                           0.928571   \n",
      "6177  35.704148  0.000004                           1.000000   \n",
      "6178  35.725836  0.000009                           1.000000   \n",
      "6179  35.756726  0.000188                           1.000000   \n",
      "6180  35.751490  0.000019                           1.000000   \n",
      "\n",
      "      intersections_without_traffic_lights  cul_de_sac   city  connectivity  \n",
      "0                                 0.839286         1.0  Tokyo     23.660714  \n",
      "1                                 0.559524         1.0  Tokyo     21.329365  \n",
      "2                                 0.922619         1.0  Tokyo     21.208900  \n",
      "3                                 1.000000         1.0  Tokyo     25.000000  \n",
      "4                                 0.952381         1.0  Tokyo     24.518141  \n",
      "...                                    ...         ...    ...           ...  \n",
      "6176                              0.636905         1.0  Tokyo     21.378968  \n",
      "6177                              0.857143         1.0  Tokyo     23.809524  \n",
      "6178                              1.000000         1.0  Tokyo     25.000000  \n",
      "6179                              0.946429         1.0  Tokyo     24.553571  \n",
      "6180                              0.839286         1.0  Tokyo     23.660714  \n",
      "\n",
      "[6181 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "num_category=4\n",
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_list=['intersections_with_traffic_lights',\n",
    "           'intersections_without_traffic_lights',\n",
    "           'cul_de_sac'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "   # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        data_df=data_df.rename(columns={data_df.columns[0]:'pano_id',data_df.columns[-1]:data})\n",
    "    \n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        ) \n",
    "        location_df.to_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_connectivity_before_scaling.csv'.format(city)))\n",
    "        \n",
    "# scaling\n",
    "# put both cities data together first\n",
    "df_temp=pd.DataFrame()\n",
    "for city in city_list:\n",
    "    connectivity_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_connectivity_before_scaling.csv'.format(city)))\n",
    "    connectivity_df['city']=city\n",
    "    df_temp=df_temp.append(connectivity_df)\n",
    "# scaling using Min-Max scale\n",
    "cols_transfomation_negative=data_list\n",
    "# fit the scaler\n",
    "scaler = MinMaxScaler()\n",
    "df_temp[cols_transfomation_negative]=scaler.fit_transform(df_temp[cols_transfomation_negative])\n",
    "df_temp[cols_transfomation_negative]=1-df_temp[cols_transfomation_negative]\n",
    "# calculate the indicator\n",
    "df_temp['connectivity']=df_temp[data_list].sum(axis=1)*((100/num_category)/len(data_list))\n",
    "\n",
    "# save seperately by cities\n",
    "for city in city_list:\n",
    "    df_temp_city=df_temp.loc[df_temp['city']==city]\n",
    "    print(df_temp_city)\n",
    "    df_temp_city.to_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_connectivity.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-shark",
   "metadata": {},
   "source": [
    "### Environment\n",
    "- Slope\n",
    "- Number of points of interest\n",
    "- Shannon land use mix index\n",
    "- Air quality index (AQI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "functional-internship",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Unnamed: 0.1  \\\n",
      "0              0             0   \n",
      "1              1             1   \n",
      "2              2             2   \n",
      "3              3             3   \n",
      "4              4             4   \n",
      "...          ...           ...   \n",
      "5828        5828          6115   \n",
      "5829        5829          6116   \n",
      "5830        5830          6117   \n",
      "5831        5831          6119   \n",
      "5832        5832          6120   \n",
      "\n",
      "                                                pano_id     panoLon   panoLat  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ  103.914512  1.400917   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA  103.965906  1.375535   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA  103.819038  1.293706   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA  103.703747  1.341976   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw  103.900929  1.311461   \n",
      "...                                                 ...         ...       ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...  103.835671  1.289808   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg  103.966108  1.373809   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ  103.701994  1.344878   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  103.632836  1.324383   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A  103.747407  1.428744   \n",
      "\n",
      "      distDiff     slope       poi  land_use       aqi       city  environment  \n",
      "0     0.000084  0.990570  0.005054  0.261089  0.528719  Singapore    11.158953  \n",
      "1     0.000015  0.941890  0.000000  0.361611  0.623963  Singapore    12.046651  \n",
      "2     0.000104  0.891331  0.008212  0.101656  0.475659  Singapore     9.230367  \n",
      "3     0.000184  0.813828  0.054959  0.665010  0.644805  Singapore    13.616261  \n",
      "4     0.000057  0.997301  0.048642  0.317303  0.512588  Singapore    11.723959  \n",
      "...        ...       ...       ...       ...       ...        ...          ...  \n",
      "5828  0.000131  0.686820  0.023373  0.634979  0.515324  Singapore    11.628100  \n",
      "5829  0.000204  0.990817  0.000000  0.230324  0.626108  Singapore    11.545308  \n",
      "5830  0.000181  0.870596  0.013266  0.357428  0.660571  Singapore    11.886631  \n",
      "5831  0.000031  0.948390  0.000000  0.000000  0.615572  Singapore     9.774763  \n",
      "5832  0.000018  0.934221  0.000000  0.000000  0.511336  Singapore     9.034730  \n",
      "\n",
      "[5833 rows x 12 columns]\n",
      "      Unnamed: 0  Unnamed: 0.1                      pano_id     panoLon  \\\n",
      "0              0             0  pano=d1hU7t4QKuXqckKk29ENTQ  139.682778   \n",
      "1              1             1  pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401   \n",
      "2              2             2  pano=LySKMao7KBtKwGBAST6INA  139.866208   \n",
      "3              3             3  pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055   \n",
      "4              4             4  pano=pLawV8B_NY4JFfpRr8v-3A  139.622156   \n",
      "...          ...           ...                          ...         ...   \n",
      "6176        6176          6278  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896   \n",
      "6177        6177          6279  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517   \n",
      "6178        6178          6280  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967   \n",
      "6179        6179          6281  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638   \n",
      "6180        6180          6282  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100   \n",
      "\n",
      "        panoLat  distDiff     slope       poi  land_use       aqi   city  \\\n",
      "0     35.574288  0.000018  0.919123  0.024005  0.842089  0.460704  Tokyo   \n",
      "1     35.723570  0.000001  0.979139  0.009476  0.533357  0.424238  Tokyo   \n",
      "2     35.664492  0.000007  0.954535  0.012634  0.914101  0.353818  Tokyo   \n",
      "3     35.607340  0.000356  0.944189  0.011371  0.524602  0.406564  Tokyo   \n",
      "4     35.766678  0.000014  0.986737  0.003790  0.342951  0.360041  Tokyo   \n",
      "...         ...       ...       ...       ...       ...       ...    ...   \n",
      "6176  35.694996  0.000006  0.943981  0.109918  0.547895  0.220556  Tokyo   \n",
      "6177  35.704148  0.000004  0.829054  0.197094  0.627163  0.438656  Tokyo   \n",
      "6178  35.725836  0.000009  0.980906  0.003790  0.410118  0.343141  Tokyo   \n",
      "6179  35.756726  0.000188  0.966229  0.013898  0.000000  0.418276  Tokyo   \n",
      "6180  35.751490  0.000019  0.990562  0.037271  0.840916  0.305290  Tokyo   \n",
      "\n",
      "      environment  \n",
      "0       14.037007  \n",
      "1       12.163811  \n",
      "2       13.969302  \n",
      "3       11.792035  \n",
      "4       10.584492  \n",
      "...           ...  \n",
      "6176    11.389684  \n",
      "6177    13.074792  \n",
      "6178    10.862224  \n",
      "6179     8.740015  \n",
      "6180    13.587740  \n",
      "\n",
      "[6181 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "num_category=4\n",
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_list=['slope',\n",
    "           'poi',\n",
    "           'land_use',\n",
    "           'aqi'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        data_df=data_df.rename(columns={data_df.columns[0]:'pano_id',data_df.columns[-1]:data})\n",
    "    \n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        ) \n",
    "        location_df.to_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_environment_before_scaling.csv'.format(city)))\n",
    "        \n",
    "# scaling\n",
    "# put both cities data together first\n",
    "df_temp=pd.DataFrame()\n",
    "for city in city_list:\n",
    "    environment_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_environment_before_scaling.csv'.format(city)))\n",
    "    environment_df['city']=city\n",
    "    df_temp=df_temp.append(environment_df)\n",
    "# scaling using Min-Max scale\n",
    "cols_transfomation_positive=['poi']\n",
    "cols_transfomation_negative=['slope','aqi']\n",
    "# fit the scaler\n",
    "scaler = MinMaxScaler()\n",
    "df_temp[cols_transfomation_positive]=scaler.fit_transform(df_temp[cols_transfomation_positive])\n",
    "df_temp[cols_transfomation_negative]=scaler.fit_transform(df_temp[cols_transfomation_negative])\n",
    "df_temp[cols_transfomation_negative]=1-df_temp[cols_transfomation_negative]\n",
    "\n",
    "# calculate the indicator\n",
    "df_temp['environment']=df_temp[data_list].sum(axis=1)*((100/num_category)/len(data_list))\n",
    "# save seperately by cities\n",
    "for city in city_list:\n",
    "    df_temp_city=df_temp.loc[df_temp['city']==city]\n",
    "    print(df_temp_city)\n",
    "    df_temp_city.to_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_environment.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-glasgow",
   "metadata": {},
   "source": [
    "### Infrastructure\n",
    "- Type of road\n",
    "- Number of transit facilities\n",
    "- Type of pavement (paved vs unpaved)\n",
    "- Road width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "published-south",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0  Unnamed: 0.1      panoLon      panoLat      distDiff  \\\n",
      "count  5833.000000   5833.000000  5833.000000  5833.000000  5.833000e+03   \n",
      "mean   2916.000000   3061.086233   103.827390     1.347023  7.049910e-05   \n",
      "std    1683.986391   1764.040159     0.084191     0.044801  1.839192e-04   \n",
      "min       0.000000      0.000000   103.611153     1.244036  3.210881e-07   \n",
      "25%    1458.000000   1537.000000   103.764845     1.313970  2.542029e-05   \n",
      "50%    2916.000000   3060.000000   103.837668     1.339950  4.351301e-05   \n",
      "75%    4374.000000   4583.000000   103.889764     1.375763  7.797647e-05   \n",
      "max    5832.000000   6120.000000   104.025514     1.469106  8.622213e-03   \n",
      "\n",
      "           length_x  road_type_score    road_type      transit      length_y  \\\n",
      "count   5833.000000      5833.000000  5833.000000  5833.000000   5833.000000   \n",
      "mean    2790.531982      1442.120853     0.526952     0.066832   1477.746906   \n",
      "std     1365.407053       751.281260     0.153542     0.105506   1985.845246   \n",
      "min       89.742072         0.000000     0.000000     0.000000      0.000000   \n",
      "25%     1969.058235       960.299272     0.438754     0.000000    593.294912   \n",
      "50%     2513.076775      1324.927649     0.533228     0.000000    957.647248   \n",
      "75%     3272.470113      1787.387306     0.629182     0.166667   1523.809342   \n",
      "max    23533.572883     18018.969311     0.971772     0.666667  18631.231586   \n",
      "\n",
      "       surface_type_score  surface_type        length  road_width_score  \\\n",
      "count         5833.000000   5833.000000   5833.000000       5833.000000   \n",
      "mean          1410.985684      0.933027     16.479948          2.961248   \n",
      "std           1934.577747      0.205115    184.212850         34.278291   \n",
      "min              0.000000      0.000000      0.000000          0.000000   \n",
      "25%            575.368026      1.000000      0.000000          0.000000   \n",
      "50%            929.610733      1.000000      0.000000          0.000000   \n",
      "75%           1424.683828      1.000000      0.000000          0.000000   \n",
      "max          18631.231586      1.000000  10307.987027       2061.597405   \n",
      "\n",
      "        road_width  infrastructure  \n",
      "count  5833.000000     5833.000000  \n",
      "mean      0.006883        9.585596  \n",
      "std       0.042219        1.785770  \n",
      "min       0.000000        0.210338  \n",
      "25%       0.000000        8.953934  \n",
      "50%       0.000000        9.827649  \n",
      "75%       0.000000       10.644009  \n",
      "max       1.000000       15.436962  \n",
      "        Unnamed: 0  Unnamed: 0.1      panoLon      panoLat      distDiff  \\\n",
      "count  6181.000000   6181.000000  6181.000000  6181.000000  6.181000e+03   \n",
      "mean   3090.000000   3140.694871   139.729633    35.698696  4.895514e-05   \n",
      "std    1784.445339   1814.754371     0.082578     0.055719  1.382018e-04   \n",
      "min       0.000000      0.000000   139.564319    35.539842  1.000000e-07   \n",
      "25%    1545.000000   1568.000000   139.664294    35.664903  8.464632e-06   \n",
      "50%    3090.000000   3143.000000   139.725885    35.702985  2.039338e-05   \n",
      "75%    4635.000000   4711.000000   139.790308    35.741515  4.807149e-05   \n",
      "max    6180.000000   6282.000000   139.914750    35.816048  5.846748e-03   \n",
      "\n",
      "           length_x  road_type_score    road_type      transit      length_y  \\\n",
      "count   6181.000000      6181.000000  6181.000000  6181.000000   6181.000000   \n",
      "mean    1960.808951       876.011167     0.461696     0.085234   1804.605208   \n",
      "std     1312.193574       560.641114     0.198642     0.120909   2981.202257   \n",
      "min      278.766421         0.000000     0.000000     0.000000      0.000000   \n",
      "25%     1273.522953       494.010431     0.314982     0.000000      0.000000   \n",
      "50%     1600.586124       804.382184     0.482997     0.000000    851.853472   \n",
      "75%     2197.580730      1152.841569     0.618778     0.166667   2383.330885   \n",
      "max    15767.212392      7052.766355     0.921668     1.000000  30935.504684   \n",
      "\n",
      "       surface_type_score  surface_type       length  road_width_score  \\\n",
      "count         6181.000000   6181.000000  6181.000000       6181.000000   \n",
      "mean          1061.052579      0.415263     6.584494          2.854017   \n",
      "std           1729.686506      0.319046    57.071757         26.011553   \n",
      "min              0.000000      0.000000     0.000000          0.000000   \n",
      "25%              0.000000      0.000000     0.000000          0.000000   \n",
      "50%            487.100872      0.500000     0.000000          0.000000   \n",
      "75%           1377.105358      0.519847     0.000000          0.000000   \n",
      "max          17405.455726      1.000000  1526.681563        586.545271   \n",
      "\n",
      "        road_width  infrastructure  \n",
      "count  6181.000000     6181.000000  \n",
      "mean      0.010075        6.076679  \n",
      "std       0.073842        2.706364  \n",
      "min       0.000000        0.000000  \n",
      "25%       0.000000        4.177414  \n",
      "50%       0.000000        6.106627  \n",
      "75%       0.000000        7.840393  \n",
      "max       1.000000       17.948596  \n"
     ]
    }
   ],
   "source": [
    "num_category=4\n",
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_list=['road_type',\n",
    "           'transit',\n",
    "           'surface_type',\n",
    "           'road_width'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        data_df=data_df.rename(columns={data_df.columns[0]:'pano_id',data_df.columns[-1]:data})\n",
    "    \n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        ) \n",
    "        location_df.to_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_infrastructure_before_scaling.csv'.format(city)))\n",
    "        \n",
    "# scaling\n",
    "# put both cities data together first\n",
    "df_temp=pd.DataFrame()\n",
    "for city in city_list:\n",
    "    infrastructure_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_infrastructure_before_scaling.csv'.format(city)))\n",
    "    infrastructure_df['city']=city\n",
    "    df_temp=df_temp.append(infrastructure_df)\n",
    "# scaling using Min-Max scale\n",
    "cols_transfomation_positive=['transit']\n",
    "# fit the scaler\n",
    "scaler = MinMaxScaler()\n",
    "df_temp[cols_transfomation_positive]=scaler.fit_transform(df_temp[cols_transfomation_positive])\n",
    "\n",
    "# calculate the indicator\n",
    "df_temp['infrastructure']=df_temp[data_list].sum(axis=1)*((100/num_category)/len(data_list))\n",
    "# save seperately by cities\n",
    "for city in city_list:\n",
    "    df_temp_city=df_temp.loc[df_temp['city']==city]\n",
    "    print(df_temp_city.describe())\n",
    "    df_temp_city.to_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_infrastructure.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-detection",
   "metadata": {},
   "source": [
    "### Vehicle_Cyclist_Interaction\n",
    "- Presence of off-street parking lot spaces\n",
    "- Number of speed bumps / choker / roundabout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "arctic-delivery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Unnamed: 0.1  \\\n",
      "0              0             0   \n",
      "1              1             1   \n",
      "2              2             2   \n",
      "3              3             3   \n",
      "4              4             4   \n",
      "...          ...           ...   \n",
      "5828        5828          6115   \n",
      "5829        5829          6116   \n",
      "5830        5830          6117   \n",
      "5831        5831          6119   \n",
      "5832        5832          6120   \n",
      "\n",
      "                                                pano_id     panoLon   panoLat  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ  103.914512  1.400917   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA  103.965906  1.375535   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA  103.819038  1.293706   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA  103.703747  1.341976   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw  103.900929  1.311461   \n",
      "...                                                 ...         ...       ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...  103.835671  1.289808   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg  103.966108  1.373809   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ  103.701994  1.344878   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  103.632836  1.324383   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A  103.747407  1.428744   \n",
      "\n",
      "      distDiff        length  street_parking_score  street_parking  \\\n",
      "0     0.000084   6348.347232           6348.347232        1.000000   \n",
      "1     0.000015   1450.064562           1376.884265        0.949533   \n",
      "2     0.000104   1673.813864           1673.813864        1.000000   \n",
      "3     0.000184   4238.337148           4238.337148        1.000000   \n",
      "4     0.000057   6105.134562           5840.740719        0.956693   \n",
      "...        ...           ...                   ...             ...   \n",
      "5828  0.000131   4919.412261           4919.412261        1.000000   \n",
      "5829  0.000204   2213.970191           2118.401573        0.956834   \n",
      "5830  0.000181   4977.809348           4977.809348        1.000000   \n",
      "5831  0.000031  30680.605921          30680.605921        1.000000   \n",
      "5832  0.000018   4214.781146           4214.781146        1.000000   \n",
      "\n",
      "      traffic_calming       city  vehicle_cyclist_interaction  \n",
      "0                   0  Singapore                    12.500000  \n",
      "1                   0  Singapore                    11.869163  \n",
      "2                   0  Singapore                    12.500000  \n",
      "3                   0  Singapore                    12.500000  \n",
      "4                   0  Singapore                    11.958665  \n",
      "...               ...        ...                          ...  \n",
      "5828                0  Singapore                    12.500000  \n",
      "5829                0  Singapore                    11.960423  \n",
      "5830                0  Singapore                    12.500000  \n",
      "5831                0  Singapore                    12.500000  \n",
      "5832                0  Singapore                    12.500000  \n",
      "\n",
      "[5833 rows x 12 columns]\n",
      "      Unnamed: 0  Unnamed: 0.1                      pano_id     panoLon  \\\n",
      "0              0             0  pano=d1hU7t4QKuXqckKk29ENTQ  139.682778   \n",
      "1              1             1  pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401   \n",
      "2              2             2  pano=LySKMao7KBtKwGBAST6INA  139.866208   \n",
      "3              3             3  pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055   \n",
      "4              4             4  pano=pLawV8B_NY4JFfpRr8v-3A  139.622156   \n",
      "...          ...           ...                          ...         ...   \n",
      "6176        6176          6278  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896   \n",
      "6177        6177          6279  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517   \n",
      "6178        6178          6280  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967   \n",
      "6179        6179          6281  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638   \n",
      "6180        6180          6282  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100   \n",
      "\n",
      "        panoLat  distDiff       length  street_parking_score  street_parking  \\\n",
      "0     35.574288  0.000018  1629.511615           1629.511615             1.0   \n",
      "1     35.723570  0.000001  1293.201743           1293.201743             1.0   \n",
      "2     35.664492  0.000007  1653.129062           1653.129062             1.0   \n",
      "3     35.607340  0.000356  1335.158553           1335.158553             1.0   \n",
      "4     35.766678  0.000014   860.187652            860.187652             1.0   \n",
      "...         ...       ...          ...                   ...             ...   \n",
      "6176  35.694996  0.000006  2390.733194           2390.733194             1.0   \n",
      "6177  35.704148  0.000004  1571.173749           1571.173749             1.0   \n",
      "6178  35.725836  0.000009  1586.877124           1586.877124             1.0   \n",
      "6179  35.756726  0.000188  1527.305076           1527.305076             1.0   \n",
      "6180  35.751490  0.000019  1544.234442           1544.234442             1.0   \n",
      "\n",
      "      traffic_calming   city  vehicle_cyclist_interaction  \n",
      "0                   0  Tokyo                         12.5  \n",
      "1                   0  Tokyo                         12.5  \n",
      "2                   0  Tokyo                         12.5  \n",
      "3                   0  Tokyo                         12.5  \n",
      "4                   0  Tokyo                         12.5  \n",
      "...               ...    ...                          ...  \n",
      "6176                0  Tokyo                         12.5  \n",
      "6177                0  Tokyo                         12.5  \n",
      "6178                0  Tokyo                         12.5  \n",
      "6179                0  Tokyo                         12.5  \n",
      "6180                0  Tokyo                         12.5  \n",
      "\n",
      "[6181 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "num_category=4\n",
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_list=[\n",
    "           'street_parking',\n",
    "           'traffic_calming'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        data_df=data_df.rename(columns={data_df.columns[0]:'pano_id',data_df.columns[-1]:data})\n",
    "    \n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        ) \n",
    "        location_df.to_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_vehicle_cyclist_interaction_before_scaling.csv'.format(city)))\n",
    "        \n",
    "# scaling\n",
    "# put both cities data together first\n",
    "df_temp=pd.DataFrame()\n",
    "for city in city_list:\n",
    "    vehicle_cyclist_interaction_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_vehicle_cyclist_interaction_before_scaling.csv'.format(city)))\n",
    "    vehicle_cyclist_interaction_df['city']=city\n",
    "    df_temp=df_temp.append(vehicle_cyclist_interaction_df)\n",
    "\n",
    "# 1 if >0\n",
    "cols_transfomation_above_0=['traffic_calming']\n",
    "for col in cols_transfomation_above_0:\n",
    "    cond_list=[df_temp[col]>0,df_temp[col]==0]\n",
    "    choice_list=[1,0]\n",
    "    df_temp[col]=np.select(cond_list,choice_list)\n",
    "\n",
    "# calculate the indicator\n",
    "df_temp['vehicle_cyclist_interaction']=df_temp[data_list].sum(axis=1)*((100/num_category)/len(data_list))\n",
    "# save seperately by cities\n",
    "for city in city_list:\n",
    "    df_temp_city=df_temp.loc[df_temp['city']==city]\n",
    "    print(df_temp_city)\n",
    "    df_temp_city.to_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_vehicle_cyclist_interaction.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-burns",
   "metadata": {},
   "source": [
    "### Bikeability\n",
    "- connectivity\n",
    "- environment\n",
    "- infrastructure\n",
    "- vehicle cyclist interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "recreational-extent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                pano_id     panoLon   panoLat  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ  103.914512  1.400917   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA  103.965906  1.375535   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA  103.819038  1.293706   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA  103.703747  1.341976   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw  103.900929  1.311461   \n",
      "...                                                 ...         ...       ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...  103.835671  1.289808   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg  103.966108  1.373809   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ  103.701994  1.344878   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  103.632836  1.324383   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A  103.747407  1.428744   \n",
      "\n",
      "      distDiff  connectivity  environment  infrastructure  \\\n",
      "0     0.000084     20.679800    11.158953       10.619572   \n",
      "1     0.000015     21.494709    12.046651        8.255520   \n",
      "2     0.000104     20.467215     9.230367       11.250000   \n",
      "3     0.000184     16.038360    13.616261       10.537781   \n",
      "4     0.000057     21.329365    11.723959       10.998928   \n",
      "...        ...           ...          ...             ...   \n",
      "5828  0.000131     21.265590    11.628100        9.485790   \n",
      "5829  0.000204     21.428571    11.545308        8.275741   \n",
      "5830  0.000181     17.413076    11.886631       10.132545   \n",
      "5831  0.000031     23.015873     9.774763        7.968278   \n",
      "5832  0.000018     24.355159     9.034730        8.772891   \n",
      "\n",
      "      vehicle_cyclist_interaction  bikeability  \n",
      "0                       12.500000    54.958325  \n",
      "1                       11.869163    53.666043  \n",
      "2                       12.500000    53.447582  \n",
      "3                       12.500000    52.692401  \n",
      "4                       11.958665    56.010918  \n",
      "...                           ...          ...  \n",
      "5828                    12.500000    54.879481  \n",
      "5829                    11.960423    53.210043  \n",
      "5830                    12.500000    51.932252  \n",
      "5831                    12.500000    53.258913  \n",
      "5832                    12.500000    54.662780  \n",
      "\n",
      "[5833 rows x 9 columns]\n",
      "                          pano_id     panoLon    panoLat  distDiff  \\\n",
      "0     pano=d1hU7t4QKuXqckKk29ENTQ  139.682778  35.574288  0.000018   \n",
      "1     pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401  35.723570  0.000001   \n",
      "2     pano=LySKMao7KBtKwGBAST6INA  139.866208  35.664492  0.000007   \n",
      "3     pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055  35.607340  0.000356   \n",
      "4     pano=pLawV8B_NY4JFfpRr8v-3A  139.622156  35.766678  0.000014   \n",
      "...                           ...         ...        ...       ...   \n",
      "6176  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896  35.694996  0.000006   \n",
      "6177  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517  35.704148  0.000004   \n",
      "6178  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967  35.725836  0.000009   \n",
      "6179  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638  35.756726  0.000188   \n",
      "6180  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100  35.751490  0.000019   \n",
      "\n",
      "      connectivity  environment  infrastructure  vehicle_cyclist_interaction  \\\n",
      "0        23.660714    14.037007        4.413806                         12.5   \n",
      "1        21.329365    12.163811        5.610174                         12.5   \n",
      "2        21.208900    13.969302        5.387912                         12.5   \n",
      "3        25.000000    11.792035        7.040817                         12.5   \n",
      "4        24.518141    10.584492        3.836903                         12.5   \n",
      "...            ...          ...             ...                          ...   \n",
      "6176     21.378968    11.389684        8.072176                         12.5   \n",
      "6177     23.809524    13.074792        4.052118                         12.5   \n",
      "6178     25.000000    10.862224        4.722735                         12.5   \n",
      "6179     24.553571     8.740015        1.025757                         12.5   \n",
      "6180     23.660714    13.587740        2.321651                         12.5   \n",
      "\n",
      "      bikeability  \n",
      "0       54.611527  \n",
      "1       51.603350  \n",
      "2       53.066114  \n",
      "3       56.332852  \n",
      "4       51.439536  \n",
      "...           ...  \n",
      "6176    53.340829  \n",
      "6177    53.436434  \n",
      "6178    53.084960  \n",
      "6179    46.819343  \n",
      "6180    52.070105  \n",
      "\n",
      "[6181 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "num_category=4\n",
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "\n",
    "data_list=['connectivity',\n",
    "           'environment',\n",
    "           'infrastructure',\n",
    "           'vehicle_cyclist_interaction'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df[['pano_id',data]],\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        )\n",
    "        location_df.drop(location_df.filter(regex='Unnamed').columns, axis=1, inplace=True)\n",
    "        \n",
    "    # scaling\n",
    "    location_df['bikeability']=location_df.iloc[:,-(num_category):].sum(axis=1)\n",
    "    print(location_df)\n",
    "    location_df.to_csv(os.path.join(root,'data/tabular_data/{}/only_non_svi_bikeability.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-differential",
   "metadata": {},
   "source": [
    "## only SVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-karaoke",
   "metadata": {},
   "source": [
    "### Environment\n",
    "- Scenery: buildings\n",
    "- Scenery: greenery\n",
    "- Scenery: water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "north-starter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                            pano_id  \\\n",
      "0              0                        pano=EIzSniBOXDgT4m2fGH2SNQ   \n",
      "1              1                        pano=tzvazBY0ag5RgAsvJgHvZA   \n",
      "2              2                        pano=FJjCz3jIAWYYnGq4PvR9HA   \n",
      "3              3                        pano=TfQN73pv-6mAZy2QnMIAPA   \n",
      "4              4                        pano=PATYQYa4O43_LjJ_JMbrFw   \n",
      "...          ...                                                ...   \n",
      "5828        6115  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...   \n",
      "5829        6116                        pano=4I7X92FmvGD7ALNXMZQuAg   \n",
      "5830        6117                        pano=EghtrEJcRO-l7XlA21zaNQ   \n",
      "5831        6119                        pano=O6UhfdrDWVrKDmJTaOLmFA   \n",
      "5832        6120                        pano=uvufId4xvxyRl-4TUehx_A   \n",
      "\n",
      "         panoLon   panoLat  distDiff  construction--structure--building  \\\n",
      "0     103.914512  1.400917  0.000084                           0.084285   \n",
      "1     103.965906  1.375535  0.000015                           0.098788   \n",
      "2     103.819038  1.293706  0.000104                           0.056864   \n",
      "3     103.703747  1.341976  0.000184                           0.065220   \n",
      "4     103.900929  1.311461  0.000057                           0.219165   \n",
      "...          ...       ...       ...                                ...   \n",
      "5828  103.835671  1.289808  0.000131                           0.195024   \n",
      "5829  103.966108  1.373809  0.000204                           0.059570   \n",
      "5830  103.701994  1.344878  0.000181                           0.545886   \n",
      "5831  103.632836  1.324383  0.000031                           0.024952   \n",
      "5832  103.747407  1.428744  0.000018                           0.026038   \n",
      "\n",
      "      scenery_building  nature--vegetation  scenery_greenery  nature--water  \\\n",
      "0             0.084285            0.240269          0.240269       0.000000   \n",
      "1             0.098788            0.142943          0.142943       0.000000   \n",
      "2             0.056864            0.278456          0.278456       0.000000   \n",
      "3             0.065220            0.373365          0.373365       0.000000   \n",
      "4             0.219165            0.105193          0.105193       0.000000   \n",
      "...                ...                 ...               ...            ...   \n",
      "5828          0.195024            0.207520          0.207520       0.184564   \n",
      "5829          0.059570            0.264463          0.264463       0.000000   \n",
      "5830          0.545886            0.068556          0.068556       0.000000   \n",
      "5831          0.024952            0.052423          0.052423       0.000000   \n",
      "5832          0.026038            0.316331          0.316331       0.000000   \n",
      "\n",
      "      scenery_water  environment  \n",
      "0          0.000000     2.704620  \n",
      "1          0.000000     2.014420  \n",
      "2          0.000000     2.794332  \n",
      "3          0.000000     3.654877  \n",
      "4          0.000000     2.702983  \n",
      "...             ...          ...  \n",
      "5828       0.184564     4.892563  \n",
      "5829       0.000000     2.700272  \n",
      "5830       0.000000     5.120346  \n",
      "5831       0.000000     0.644791  \n",
      "5832       0.000000     2.853068  \n",
      "\n",
      "[5833 rows x 12 columns]\n",
      "      Unnamed: 0                      pano_id     panoLon    panoLat  \\\n",
      "0              0  pano=d1hU7t4QKuXqckKk29ENTQ  139.682778  35.574288   \n",
      "1              1  pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401  35.723570   \n",
      "2              2  pano=LySKMao7KBtKwGBAST6INA  139.866208  35.664492   \n",
      "3              3  pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055  35.607340   \n",
      "4              4  pano=pLawV8B_NY4JFfpRr8v-3A  139.622156  35.766678   \n",
      "...          ...                          ...         ...        ...   \n",
      "6176        6278  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896  35.694996   \n",
      "6177        6279  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517  35.704148   \n",
      "6178        6280  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967  35.725836   \n",
      "6179        6281  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638  35.756726   \n",
      "6180        6282  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100  35.751490   \n",
      "\n",
      "      distDiff  construction--structure--building  scenery_building  \\\n",
      "0     0.000018                           0.413802          0.413802   \n",
      "1     0.000001                           0.327120          0.327120   \n",
      "2     0.000007                           0.543613          0.543613   \n",
      "3     0.000356                           0.470069          0.470069   \n",
      "4     0.000014                           0.206431          0.206431   \n",
      "...        ...                                ...               ...   \n",
      "6176  0.000006                           0.464005          0.464005   \n",
      "6177  0.000004                           0.330244          0.330244   \n",
      "6178  0.000009                           0.164340          0.164340   \n",
      "6179  0.000188                           0.618107          0.618107   \n",
      "6180  0.000019                           0.492637          0.492637   \n",
      "\n",
      "      nature--vegetation  scenery_greenery  nature--water  scenery_water  \\\n",
      "0               0.023707          0.023707            0.0            0.0   \n",
      "1               0.063221          0.063221            0.0            0.0   \n",
      "2               0.020380          0.020380            0.0            0.0   \n",
      "3               0.073956          0.073956            0.0            0.0   \n",
      "4               0.093844          0.093844            0.0            0.0   \n",
      "...                  ...               ...            ...            ...   \n",
      "6176            0.020577          0.020577            0.0            0.0   \n",
      "6177            0.116064          0.116064            0.0            0.0   \n",
      "6178            0.056622          0.056622            0.0            0.0   \n",
      "6179            0.109629          0.109629            0.0            0.0   \n",
      "6180            0.028429          0.028429            0.0            0.0   \n",
      "\n",
      "      environment  \n",
      "0        3.645910  \n",
      "1        3.252843  \n",
      "2        4.699941  \n",
      "3        4.533539  \n",
      "4        2.502294  \n",
      "...           ...  \n",
      "6176     4.038183  \n",
      "6177     3.719233  \n",
      "6178     1.841354  \n",
      "6179     6.064468  \n",
      "6180     4.342219  \n",
      "\n",
      "[6181 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "num_category=4\n",
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_list=['scenery_building',\n",
    "           'scenery_greenery',\n",
    "           'scenery_water'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        data_df=data_df.rename(columns={data_df.columns[0]:'pano_id',data_df.columns[-1]:data})\n",
    "    \n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        ) \n",
    "        location_df.to_csv(os.path.join(root,'data/tabular_data/{}/only_svi_environment_before_scaling.csv'.format(city)))\n",
    "        \n",
    "\n",
    "    # calculate the indicator\n",
    "    location_df['environment']=location_df[data_list].sum(axis=1)*((100/num_category)/len(data_list))\n",
    "    print(location_df)\n",
    "    location_df.to_csv(os.path.join(root,'data/tabular_data/{}/only_svi_environment.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-resident",
   "metadata": {},
   "source": [
    "## Infrastructure\n",
    "- Presence of potholes\n",
    "- Presence of street light\n",
    "- Presence of bike lanes\n",
    "- Presence of street amenities (e.g., trash cans and benches)\n",
    "- Presence of utility pole\n",
    "- Presence of bike parking\n",
    "- Presence of sidewalk\n",
    "- Presense and quality of crosswalk (with/without traffic lights, traffic signs)\n",
    "- Presence of curb cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "designed-humor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Unnamed: 0.1  \\\n",
      "0              0             0   \n",
      "1              1             1   \n",
      "2              2             2   \n",
      "3              3             3   \n",
      "4              4             4   \n",
      "...          ...           ...   \n",
      "5828        5828          6115   \n",
      "5829        5829          6116   \n",
      "5830        5830          6117   \n",
      "5831        5831          6119   \n",
      "5832        5832          6120   \n",
      "\n",
      "                                                pano_id     panoLon   panoLat  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ  103.914512  1.400917   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA  103.965906  1.375535   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA  103.819038  1.293706   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA  103.703747  1.341976   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw  103.900929  1.311461   \n",
      "...                                                 ...         ...       ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...  103.835671  1.289808   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg  103.966108  1.373809   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ  103.701994  1.344878   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  103.632836  1.324383   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A  103.747407  1.428744   \n",
      "\n",
      "      distDiff  object--pothole  pavement  object--street-light  street_light  \\\n",
      "0     0.000084              0.0         1              0.000365             1   \n",
      "1     0.000015              0.0         1              0.000167             1   \n",
      "2     0.000104              0.0         1              0.000190             1   \n",
      "3     0.000184              0.0         1              0.000000             0   \n",
      "4     0.000057              0.0         1              0.000099             1   \n",
      "...        ...              ...       ...                   ...           ...   \n",
      "5828  0.000131              0.0         1              0.001256             1   \n",
      "5829  0.000204              0.0         1              0.000000             0   \n",
      "5830  0.000181              0.0         1              0.000000             0   \n",
      "5831  0.000031              0.0         1              0.000000             0   \n",
      "5832  0.000018              0.0         1              0.000188             1   \n",
      "\n",
      "      ...  bike_parking  construction--flat--sidewalk  side_walk  \\\n",
      "0     ...             0                      0.014104          1   \n",
      "1     ...             0                      0.002032          1   \n",
      "2     ...             0                      0.017012          1   \n",
      "3     ...             0                      0.006920          1   \n",
      "4     ...             0                      0.031495          1   \n",
      "...   ...           ...                           ...        ...   \n",
      "5828  ...             0                      0.000066          1   \n",
      "5829  ...             0                      0.008632          1   \n",
      "5830  ...             0                      0.011404          1   \n",
      "5831  ...             0                      0.005613          1   \n",
      "5832  ...             0                      0.023238          1   \n",
      "\n",
      "      construction--flat--crosswalk-plain  marking--crosswalk-zebra  \\\n",
      "0                                0.000000                  0.000000   \n",
      "1                                0.000000                  0.000000   \n",
      "2                                0.000000                  0.000000   \n",
      "3                                0.000000                  0.000000   \n",
      "4                                0.000000                  0.000000   \n",
      "...                                   ...                       ...   \n",
      "5828                             0.000000                  0.000000   \n",
      "5829                             0.000000                  0.000000   \n",
      "5830                             0.000000                  0.000000   \n",
      "5831                             0.000073                  0.000057   \n",
      "5832                             0.000000                  0.000000   \n",
      "\n",
      "      cross_walk  construction--flat--curb-cut  accessibility       city  \\\n",
      "0              0                      0.000000              0  Singapore   \n",
      "1              0                      0.000000              0  Singapore   \n",
      "2              0                      0.001079              1  Singapore   \n",
      "3              0                      0.000474              1  Singapore   \n",
      "4              0                      0.000152              1  Singapore   \n",
      "...          ...                           ...            ...        ...   \n",
      "5828           0                      0.000000              0  Singapore   \n",
      "5829           0                      0.000000              0  Singapore   \n",
      "5830           0                      0.000000              0  Singapore   \n",
      "5831           1                      0.000009              1  Singapore   \n",
      "5832           0                      0.000125              1  Singapore   \n",
      "\n",
      "      infrastructure  \n",
      "0          13.888889  \n",
      "1          11.111111  \n",
      "2          13.888889  \n",
      "3          11.111111  \n",
      "4          16.666667  \n",
      "...              ...  \n",
      "5828       13.888889  \n",
      "5829        8.333333  \n",
      "5830       11.111111  \n",
      "5831       13.888889  \n",
      "5832       13.888889  \n",
      "\n",
      "[5833 rows x 29 columns]\n",
      "      Unnamed: 0  Unnamed: 0.1                      pano_id     panoLon  \\\n",
      "0              0             0  pano=d1hU7t4QKuXqckKk29ENTQ  139.682778   \n",
      "1              1             1  pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401   \n",
      "2              2             2  pano=LySKMao7KBtKwGBAST6INA  139.866208   \n",
      "3              3             3  pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055   \n",
      "4              4             4  pano=pLawV8B_NY4JFfpRr8v-3A  139.622156   \n",
      "...          ...           ...                          ...         ...   \n",
      "6176        6176          6278  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896   \n",
      "6177        6177          6279  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517   \n",
      "6178        6178          6280  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967   \n",
      "6179        6179          6281  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638   \n",
      "6180        6180          6282  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100   \n",
      "\n",
      "        panoLat  distDiff  object--pothole  pavement  object--street-light  \\\n",
      "0     35.574288  0.000018              0.0         1              0.000057   \n",
      "1     35.723570  0.000001              0.0         1              0.000000   \n",
      "2     35.664492  0.000007              0.0         1              0.000000   \n",
      "3     35.607340  0.000356              0.0         1              0.000000   \n",
      "4     35.766678  0.000014              0.0         1              0.000078   \n",
      "...         ...       ...              ...       ...                   ...   \n",
      "6176  35.694996  0.000006              0.0         1              0.000000   \n",
      "6177  35.704148  0.000004              0.0         1              0.000259   \n",
      "6178  35.725836  0.000009              0.0         1              0.000060   \n",
      "6179  35.756726  0.000188              0.0         1              0.000000   \n",
      "6180  35.751490  0.000019              0.0         1              0.000667   \n",
      "\n",
      "      street_light  ...  bike_parking  construction--flat--sidewalk  \\\n",
      "0                1  ...             0                      0.010486   \n",
      "1                0  ...             0                      0.024543   \n",
      "2                0  ...             0                      0.012275   \n",
      "3                0  ...             0                      0.009009   \n",
      "4                1  ...             1                      0.043199   \n",
      "...            ...  ...           ...                           ...   \n",
      "6176             0  ...             0                      0.082785   \n",
      "6177             1  ...             1                      0.015377   \n",
      "6178             1  ...             0                      0.006485   \n",
      "6179             0  ...             0                      0.032980   \n",
      "6180             1  ...             0                      0.016627   \n",
      "\n",
      "      side_walk  construction--flat--crosswalk-plain  \\\n",
      "0             1                                  0.0   \n",
      "1             1                                  0.0   \n",
      "2             1                                  0.0   \n",
      "3             1                                  0.0   \n",
      "4             1                                  0.0   \n",
      "...         ...                                  ...   \n",
      "6176          1                                  0.0   \n",
      "6177          1                                  0.0   \n",
      "6178          1                                  0.0   \n",
      "6179          1                                  0.0   \n",
      "6180          1                                  0.0   \n",
      "\n",
      "      marking--crosswalk-zebra  cross_walk  construction--flat--curb-cut  \\\n",
      "0                     0.000000           0                      0.000012   \n",
      "1                     0.000000           0                      0.000000   \n",
      "2                     0.000000           0                      0.000045   \n",
      "3                     0.000000           0                      0.000000   \n",
      "4                     0.000000           0                      0.001259   \n",
      "...                        ...         ...                           ...   \n",
      "6176                  0.000418           1                      0.000021   \n",
      "6177                  0.101890           1                      0.000000   \n",
      "6178                  0.008876           1                      0.000120   \n",
      "6179                  0.000000           0                      0.000000   \n",
      "6180                  0.000000           0                      0.000000   \n",
      "\n",
      "      accessibility   city  infrastructure  \n",
      "0                 1  Tokyo       13.888889  \n",
      "1                 0  Tokyo        5.555556  \n",
      "2                 1  Tokyo        8.333333  \n",
      "3                 0  Tokyo       11.111111  \n",
      "4                 1  Tokyo       16.666667  \n",
      "...             ...    ...             ...  \n",
      "6176              1  Tokyo       11.111111  \n",
      "6177              0  Tokyo       16.666667  \n",
      "6178              1  Tokyo       13.888889  \n",
      "6179              0  Tokyo        5.555556  \n",
      "6180              0  Tokyo        8.333333  \n",
      "\n",
      "[6181 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "num_category=4\n",
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_list=[\n",
    "           'pavement',\n",
    "           'street_light',\n",
    "           'bike_lanes',\n",
    "           'street_amenity',\n",
    "           'utility_pole',\n",
    "           'bike_parking',\n",
    "           'side_walk',\n",
    "           'cross_walk',\n",
    "           'accessibility'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        data_df=data_df.rename(columns={data_df.columns[0]:'pano_id',data_df.columns[-1]:data})\n",
    "    \n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        ) \n",
    "        location_df.to_csv(os.path.join(root,'data/tabular_data/{}/only_svi_infrastructure_before_scaling.csv'.format(city)))\n",
    "        \n",
    "# scaling\n",
    "# put both cities data together first\n",
    "df_temp=pd.DataFrame()\n",
    "for city in city_list:\n",
    "    infrastructure_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/only_svi_infrastructure_before_scaling.csv'.format(city)))\n",
    "    infrastructure_df['city']=city\n",
    "    df_temp=df_temp.append(infrastructure_df)\n",
    "# 1 if >0\n",
    "cols_transfomation_above_0=['street_light','bike_lanes','street_amenity','bike_parking',\n",
    "                            'side_walk','cross_walk','accessibility']\n",
    "for col in cols_transfomation_above_0:\n",
    "    cond_list=[df_temp[col]>0,df_temp[col]==0]\n",
    "    choice_list=[1,0]\n",
    "    df_temp[col]=np.select(cond_list,choice_list)\n",
    "# 1 if ==0\n",
    "cols_transfomation_equal_to_0=['pavement','utility_pole']\n",
    "for col in cols_transfomation_equal_to_0:\n",
    "    cond_list=[df_temp[col]==0,df_temp[col]>0]\n",
    "    choice_list=[1,0]\n",
    "    df_temp[col]=np.select(cond_list,choice_list)\n",
    "\n",
    "# calculate the indicator\n",
    "df_temp['infrastructure']=df_temp[data_list].sum(axis=1)*((100/num_category)/len(data_list))\n",
    "# save seperately by cities\n",
    "for city in city_list:\n",
    "    df_temp_city=df_temp.loc[df_temp['city']==city]\n",
    "    print(df_temp_city)\n",
    "    df_temp_city.to_csv(os.path.join(root,'data/tabular_data/{}/only_svi_infrastructure.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-driver",
   "metadata": {},
   "source": [
    "### Vehicle_Cyclist_Interaction\n",
    "- No. of vehicles\n",
    "- traffic light / stop sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "careful-keeping",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Unnamed: 0.1  \\\n",
      "0              0             0   \n",
      "1              1             1   \n",
      "2              2             2   \n",
      "3              3             3   \n",
      "4              4             4   \n",
      "...          ...           ...   \n",
      "5828        5828          6115   \n",
      "5829        5829          6116   \n",
      "5830        5830          6117   \n",
      "5831        5831          6119   \n",
      "5832        5832          6120   \n",
      "\n",
      "                                                pano_id     panoLon   panoLat  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ  103.914512  1.400917   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA  103.965906  1.375535   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA  103.819038  1.293706   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA  103.703747  1.341976   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw  103.900929  1.311461   \n",
      "...                                                 ...         ...       ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...  103.835671  1.289808   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg  103.966108  1.373809   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ  103.701994  1.344878   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  103.632836  1.324383   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A  103.747407  1.428744   \n",
      "\n",
      "      distDiff  no_of_vehicles  traffic light  stop sign  \\\n",
      "0     0.000084        0.975385              0          0   \n",
      "1     0.000015        0.935385              0          0   \n",
      "2     0.000104        0.935385              0          0   \n",
      "3     0.000184        0.640000              0          5   \n",
      "4     0.000057        0.723077              0          0   \n",
      "...        ...             ...            ...        ...   \n",
      "5828  0.000131        1.000000              0          0   \n",
      "5829  0.000204        0.984615              0          0   \n",
      "5830  0.000181        0.949231              0          0   \n",
      "5831  0.000031        0.963077              2          0   \n",
      "5832  0.000018        0.986154              0          0   \n",
      "\n",
      "      traffic_light_stop_sign       city  vehicle_cyclist_interaction  \n",
      "0                           0  Singapore                    12.192308  \n",
      "1                           0  Singapore                    11.692308  \n",
      "2                           0  Singapore                    11.692308  \n",
      "3                           1  Singapore                    20.500000  \n",
      "4                           0  Singapore                     9.038462  \n",
      "...                       ...        ...                          ...  \n",
      "5828                        0  Singapore                    12.500000  \n",
      "5829                        0  Singapore                    12.307692  \n",
      "5830                        0  Singapore                    11.865385  \n",
      "5831                        1  Singapore                    24.538462  \n",
      "5832                        0  Singapore                    12.326923  \n",
      "\n",
      "[5833 rows x 12 columns]\n",
      "      Unnamed: 0  Unnamed: 0.1                      pano_id     panoLon  \\\n",
      "0              0             0  pano=d1hU7t4QKuXqckKk29ENTQ  139.682778   \n",
      "1              1             1  pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401   \n",
      "2              2             2  pano=LySKMao7KBtKwGBAST6INA  139.866208   \n",
      "3              3             3  pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055   \n",
      "4              4             4  pano=pLawV8B_NY4JFfpRr8v-3A  139.622156   \n",
      "...          ...           ...                          ...         ...   \n",
      "6176        6176          6278  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896   \n",
      "6177        6177          6279  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517   \n",
      "6178        6178          6280  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967   \n",
      "6179        6179          6281  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638   \n",
      "6180        6180          6282  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100   \n",
      "\n",
      "        panoLat  distDiff  no_of_vehicles  traffic light  stop sign  \\\n",
      "0     35.574288  0.000018        0.953846              0          0   \n",
      "1     35.723570  0.000001        0.981538              0          0   \n",
      "2     35.664492  0.000007        0.944615              0          0   \n",
      "3     35.607340  0.000356        1.000000              0          0   \n",
      "4     35.766678  0.000014        0.986154              0          0   \n",
      "...         ...       ...             ...            ...        ...   \n",
      "6176  35.694996  0.000006        0.958462              0          0   \n",
      "6177  35.704148  0.000004        0.969231              0          0   \n",
      "6178  35.725836  0.000009        1.000000              0          0   \n",
      "6179  35.756726  0.000188        0.992308              0          0   \n",
      "6180  35.751490  0.000019        0.953846              0          0   \n",
      "\n",
      "      traffic_light_stop_sign   city  vehicle_cyclist_interaction  \n",
      "0                           0  Tokyo                    11.923077  \n",
      "1                           0  Tokyo                    12.269231  \n",
      "2                           0  Tokyo                    11.807692  \n",
      "3                           0  Tokyo                    12.500000  \n",
      "4                           0  Tokyo                    12.326923  \n",
      "...                       ...    ...                          ...  \n",
      "6176                        0  Tokyo                    11.980769  \n",
      "6177                        0  Tokyo                    12.115385  \n",
      "6178                        0  Tokyo                    12.500000  \n",
      "6179                        0  Tokyo                    12.403846  \n",
      "6180                        0  Tokyo                    11.923077  \n",
      "\n",
      "[6181 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "num_category=4\n",
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_list=['no_of_vehicles',\n",
    "           'traffic_light_stop_sign'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        data_df=data_df.rename(columns={data_df.columns[0]:'pano_id',data_df.columns[-1]:data})\n",
    "    \n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        ) \n",
    "        location_df.to_csv(os.path.join(root,'data/tabular_data/{}/only_svi_vehicle_cyclist_interaction_before_scaling.csv'.format(city)))\n",
    "        \n",
    "# scaling\n",
    "# put both cities data together first\n",
    "df_temp=pd.DataFrame()\n",
    "for city in city_list:\n",
    "    vehicle_cyclist_interaction_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/only_svi_vehicle_cyclist_interaction_before_scaling.csv'.format(city)))\n",
    "    vehicle_cyclist_interaction_df['city']=city\n",
    "    df_temp=df_temp.append(vehicle_cyclist_interaction_df)\n",
    "\n",
    "# scaling using Min-Max scale\n",
    "cols_transfomation_negative=['no_of_vehicles']\n",
    "df_temp[cols_transfomation_negative]=scaler.fit_transform(df_temp[cols_transfomation_negative])\n",
    "df_temp[cols_transfomation_negative]=1-df_temp[cols_transfomation_negative]\n",
    "\n",
    "\n",
    "# calculate the indicator\n",
    "df_temp['vehicle_cyclist_interaction']=df_temp[data_list].sum(axis=1)*((100/num_category)/len(data_list))\n",
    "# save seperately by cities\n",
    "for city in city_list:\n",
    "    df_temp_city=df_temp.loc[df_temp['city']==city]\n",
    "    print(df_temp_city)\n",
    "    df_temp_city.to_csv(os.path.join(root,'data/tabular_data/{}/only_svi_vehicle_cyclist_interaction.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-context",
   "metadata": {},
   "source": [
    "## Perception\n",
    "- beauty\n",
    "- building_attractiveness\n",
    "- cleanliness\n",
    "- cycling_attractiveness\n",
    "- living_attractiveness\n",
    "- safety\n",
    "- spaciousness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "maritime-panama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                            pano_id  \\\n",
      "0              0                        pano=EIzSniBOXDgT4m2fGH2SNQ   \n",
      "1              1                        pano=tzvazBY0ag5RgAsvJgHvZA   \n",
      "2              2                        pano=FJjCz3jIAWYYnGq4PvR9HA   \n",
      "3              3                        pano=TfQN73pv-6mAZy2QnMIAPA   \n",
      "4              4                        pano=PATYQYa4O43_LjJ_JMbrFw   \n",
      "...          ...                                                ...   \n",
      "5828        6115  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...   \n",
      "5829        6116                        pano=4I7X92FmvGD7ALNXMZQuAg   \n",
      "5830        6117                        pano=EghtrEJcRO-l7XlA21zaNQ   \n",
      "5831        6119                        pano=O6UhfdrDWVrKDmJTaOLmFA   \n",
      "5832        6120                        pano=uvufId4xvxyRl-4TUehx_A   \n",
      "\n",
      "         panoLon   panoLat  distDiff  predicted_beauty  \\\n",
      "0     103.914512  1.400917  0.000084          0.677530   \n",
      "1     103.965906  1.375535  0.000015          0.632164   \n",
      "2     103.819038  1.293706  0.000104          0.704385   \n",
      "3     103.703747  1.341976  0.000184          0.635131   \n",
      "4     103.900929  1.311461  0.000057          0.698837   \n",
      "...          ...       ...       ...               ...   \n",
      "5828  103.835671  1.289808  0.000131          0.659081   \n",
      "5829  103.966108  1.373809  0.000204          0.680109   \n",
      "5830  103.701994  1.344878  0.000181          0.659934   \n",
      "5831  103.632836  1.324383  0.000031          0.593144   \n",
      "5832  103.747407  1.428744  0.000018          0.641129   \n",
      "\n",
      "      predicted_building_attractiveness  predicted_cleanliness  \\\n",
      "0                              0.649293               0.620930   \n",
      "1                              0.602554               0.731595   \n",
      "2                              0.630920               0.697841   \n",
      "3                              0.673030               0.675438   \n",
      "4                              0.666674               0.696352   \n",
      "...                                 ...                    ...   \n",
      "5828                           0.693948               0.702037   \n",
      "5829                           0.645774               0.684541   \n",
      "5830                           0.672811               0.672323   \n",
      "5831                           0.539087               0.650600   \n",
      "5832                           0.591219               0.636158   \n",
      "\n",
      "      predicted_cycling_attractiveness  predicted_living_attractiveness  \\\n",
      "0                             0.641197                         0.606929   \n",
      "1                             0.673347                         0.541503   \n",
      "2                             0.652612                         0.640286   \n",
      "3                             0.685385                         0.642010   \n",
      "4                             0.653492                         0.541376   \n",
      "...                                ...                              ...   \n",
      "5828                          0.645423                         0.667435   \n",
      "5829                          0.620375                         0.619863   \n",
      "5830                          0.632509                         0.655399   \n",
      "5831                          0.587558                         0.580468   \n",
      "5832                          0.660222                         0.582433   \n",
      "\n",
      "      predicted_safety  predicted_spaciousness  perception  \n",
      "0             0.649739                0.669198   16.124342  \n",
      "1             0.697143                0.727729   16.450124  \n",
      "2             0.647508                0.622054   16.412882  \n",
      "3             0.623593                0.618402   16.260672  \n",
      "4             0.674039                0.642970   16.334787  \n",
      "...                ...                     ...         ...  \n",
      "5828          0.645942                0.715142   16.889316  \n",
      "5829          0.692321                0.647682   16.395226  \n",
      "5830          0.658174                0.600460   16.255751  \n",
      "5831          0.564475                0.603476   14.710027  \n",
      "5832          0.584185                0.596253   15.327139  \n",
      "\n",
      "[5833 rows x 13 columns]\n",
      "      Unnamed: 0                      pano_id     panoLon    panoLat  \\\n",
      "0              0  pano=d1hU7t4QKuXqckKk29ENTQ  139.682778  35.574288   \n",
      "1              1  pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401  35.723570   \n",
      "2              2  pano=LySKMao7KBtKwGBAST6INA  139.866208  35.664492   \n",
      "3              3  pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055  35.607340   \n",
      "4              4  pano=pLawV8B_NY4JFfpRr8v-3A  139.622156  35.766678   \n",
      "...          ...                          ...         ...        ...   \n",
      "6176        6278  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896  35.694996   \n",
      "6177        6279  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517  35.704148   \n",
      "6178        6280  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967  35.725836   \n",
      "6179        6281  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638  35.756726   \n",
      "6180        6282  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100  35.751490   \n",
      "\n",
      "      distDiff  predicted_beauty  predicted_building_attractiveness  \\\n",
      "0     0.000018          0.638351                           0.577424   \n",
      "1     0.000001          0.632302                           0.606694   \n",
      "2     0.000007          0.658208                           0.580650   \n",
      "3     0.000356          0.706518                           0.649421   \n",
      "4     0.000014          0.647067                           0.649909   \n",
      "...        ...               ...                                ...   \n",
      "6176  0.000006          0.676970                           0.635674   \n",
      "6177  0.000004          0.674562                           0.619229   \n",
      "6178  0.000009          0.650342                           0.604554   \n",
      "6179  0.000188          0.571638                           0.650618   \n",
      "6180  0.000019          0.608041                           0.595031   \n",
      "\n",
      "      predicted_cleanliness  predicted_cycling_attractiveness  \\\n",
      "0                  0.635693                          0.607607   \n",
      "1                  0.652504                          0.572392   \n",
      "2                  0.599565                          0.593416   \n",
      "3                  0.675081                          0.623128   \n",
      "4                  0.704725                          0.637031   \n",
      "...                     ...                               ...   \n",
      "6176               0.694554                          0.662930   \n",
      "6177               0.608862                          0.651621   \n",
      "6178               0.638272                          0.608812   \n",
      "6179               0.642959                          0.521745   \n",
      "6180               0.549143                          0.513747   \n",
      "\n",
      "      predicted_living_attractiveness  predicted_safety  \\\n",
      "0                            0.598252          0.602905   \n",
      "1                            0.601970          0.573260   \n",
      "2                            0.634201          0.593787   \n",
      "3                            0.641235          0.633221   \n",
      "4                            0.595194          0.616714   \n",
      "...                               ...               ...   \n",
      "6176                         0.693628          0.647327   \n",
      "6177                         0.593059          0.616812   \n",
      "6178                         0.618361          0.559664   \n",
      "6179                         0.568837          0.624252   \n",
      "6180                         0.519538          0.577310   \n",
      "\n",
      "      predicted_spaciousness  perception  \n",
      "0                   0.649034   15.390236  \n",
      "1                   0.545333   14.944485  \n",
      "2                   0.578003   15.135109  \n",
      "3                   0.636739   16.304796  \n",
      "4                   0.624810   15.983747  \n",
      "...                      ...         ...  \n",
      "6176                0.661356   16.687279  \n",
      "6177                0.613055   15.632865  \n",
      "6178                0.546304   15.093959  \n",
      "6179                0.561992   14.793002  \n",
      "6180                0.560490   14.011785  \n",
      "\n",
      "[6181 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "num_category=4\n",
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "\n",
    "data_list=['beauty',\n",
    "          'building_attractiveness','cleanliness','cycling_attractiveness',\n",
    "          'living_attractiveness','safety','spaciousness'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/predicted_{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df,\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        )\n",
    "    # scaling\n",
    "    location_df = location_df.apply(lambda x: x/10 if x.name.startswith('predicted_') else x)\n",
    "    location_df['perception']=location_df[location_df.columns[pd.Series(location_df.columns).str.startswith('predicted_')]].\\\n",
    "        sum(axis=1)*((100/num_category)/len(data_list))\n",
    "    print(location_df)\n",
    "    location_df.to_csv(os.path.join(root,'data/tabular_data/{}/only_svi_perception.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-mills",
   "metadata": {},
   "source": [
    "### Bikeability\n",
    "- environment\n",
    "- infrastructure\n",
    "- vehicle cyclist interaction\n",
    "- perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "functional-smart",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                pano_id     panoLon   panoLat  \\\n",
      "0                           pano=EIzSniBOXDgT4m2fGH2SNQ  103.914512  1.400917   \n",
      "1                           pano=tzvazBY0ag5RgAsvJgHvZA  103.965906  1.375535   \n",
      "2                           pano=FJjCz3jIAWYYnGq4PvR9HA  103.819038  1.293706   \n",
      "3                           pano=TfQN73pv-6mAZy2QnMIAPA  103.703747  1.341976   \n",
      "4                           pano=PATYQYa4O43_LjJ_JMbrFw  103.900929  1.311461   \n",
      "...                                                 ...         ...       ...   \n",
      "5828  pano=CAoSLEFGMVFpcE9YcGFGMVpBOURjN21NS0NQS1h3L...  103.835671  1.289808   \n",
      "5829                        pano=4I7X92FmvGD7ALNXMZQuAg  103.966108  1.373809   \n",
      "5830                        pano=EghtrEJcRO-l7XlA21zaNQ  103.701994  1.344878   \n",
      "5831                        pano=O6UhfdrDWVrKDmJTaOLmFA  103.632836  1.324383   \n",
      "5832                        pano=uvufId4xvxyRl-4TUehx_A  103.747407  1.428744   \n",
      "\n",
      "      distDiff  environment  infrastructure  vehicle_cyclist_interaction  \\\n",
      "0     0.000084     2.704620       13.888889                    12.192308   \n",
      "1     0.000015     2.014420       11.111111                    11.692308   \n",
      "2     0.000104     2.794332       13.888889                    11.692308   \n",
      "3     0.000184     3.654877       11.111111                    20.500000   \n",
      "4     0.000057     2.702983       16.666667                     9.038462   \n",
      "...        ...          ...             ...                          ...   \n",
      "5828  0.000131     4.892563       13.888889                    12.500000   \n",
      "5829  0.000204     2.700272        8.333333                    12.307692   \n",
      "5830  0.000181     5.120346       11.111111                    11.865385   \n",
      "5831  0.000031     0.644791       13.888889                    24.538462   \n",
      "5832  0.000018     2.853068       13.888889                    12.326923   \n",
      "\n",
      "      perception  bikeability  \n",
      "0      16.124342    44.910159  \n",
      "1      16.450124    41.267962  \n",
      "2      16.412882    44.788410  \n",
      "3      16.260672    51.526660  \n",
      "4      16.334787    44.742898  \n",
      "...          ...          ...  \n",
      "5828   16.889316    48.170768  \n",
      "5829   16.395226    39.736523  \n",
      "5830   16.255751    44.352593  \n",
      "5831   14.710027    53.782168  \n",
      "5832   15.327139    44.396019  \n",
      "\n",
      "[5833 rows x 9 columns]\n",
      "                          pano_id     panoLon    panoLat  distDiff  \\\n",
      "0     pano=d1hU7t4QKuXqckKk29ENTQ  139.682778  35.574288  0.000018   \n",
      "1     pano=HRySgi0gFKuRD9QrQ3BdFw  139.620401  35.723570  0.000001   \n",
      "2     pano=LySKMao7KBtKwGBAST6INA  139.866208  35.664492  0.000007   \n",
      "3     pano=EjuTxRVkM6ywSKSNtLzFrg  139.663055  35.607340  0.000356   \n",
      "4     pano=pLawV8B_NY4JFfpRr8v-3A  139.622156  35.766678  0.000014   \n",
      "...                           ...         ...        ...       ...   \n",
      "6176  pano=cvvj9FbzQNnYAXXZ32FL-A  139.794896  35.694996  0.000006   \n",
      "6177  pano=NoizYPZvzL1Q1YU8Y-HIfQ  139.651517  35.704148  0.000004   \n",
      "6178  pano=6F5q3CCcFuPTD8w1hL0VwA  139.877967  35.725836  0.000009   \n",
      "6179  pano=Xo1hWeHXGvfx6eTGzDas7Q  139.604638  35.756726  0.000188   \n",
      "6180  pano=Iq25sqICPU6LMag1xv4U-Q  139.759100  35.751490  0.000019   \n",
      "\n",
      "      environment  infrastructure  vehicle_cyclist_interaction  perception  \\\n",
      "0        3.645910       13.888889                    11.923077   15.390236   \n",
      "1        3.252843        5.555556                    12.269231   14.944485   \n",
      "2        4.699941        8.333333                    11.807692   15.135109   \n",
      "3        4.533539       11.111111                    12.500000   16.304796   \n",
      "4        2.502294       16.666667                    12.326923   15.983747   \n",
      "...           ...             ...                          ...         ...   \n",
      "6176     4.038183       11.111111                    11.980769   16.687279   \n",
      "6177     3.719233       16.666667                    12.115385   15.632865   \n",
      "6178     1.841354       13.888889                    12.500000   15.093959   \n",
      "6179     6.064468        5.555556                    12.403846   14.793002   \n",
      "6180     4.342219        8.333333                    11.923077   14.011785   \n",
      "\n",
      "      bikeability  \n",
      "0       44.848111  \n",
      "1       36.022114  \n",
      "2       39.976076  \n",
      "3       44.449446  \n",
      "4       47.479630  \n",
      "...           ...  \n",
      "6176    43.817341  \n",
      "6177    48.134149  \n",
      "6178    43.324202  \n",
      "6179    38.816872  \n",
      "6180    38.610415  \n",
      "\n",
      "[6181 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "num_category=4\n",
    "city_list=['Singapore',\n",
    "           'Tokyo'\n",
    "          ]\n",
    "root = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "\n",
    "data_list=[\n",
    "           'environment',\n",
    "           'infrastructure',\n",
    "           'vehicle_cyclist_interaction',\n",
    "           'perception'\n",
    "          ]\n",
    "\n",
    "for city in city_list:\n",
    "    # load location\n",
    "    location=os.path.join(root,'data/meta_data_{}/{}_remaining_points.csv'.format(city,city))\n",
    "    location_df=pd.read_csv(location)\n",
    "    location_df['panoId']='pano='+location_df['panoId']\n",
    "    location_df=location_df.rename(columns={'panoId':'pano_id'})\n",
    "    \n",
    "    # go through data_list and left join it to location_df\n",
    "    for data in data_list:\n",
    "        # import as df\n",
    "        data_df=pd.read_csv(os.path.join(root,'data/tabular_data/{}/only_svi_{}.csv'.format(city,data)))\n",
    "        data_df=data_df.iloc[:,1:]\n",
    "        # merge and scale them\n",
    "        location_df=pd.merge(location_df,\n",
    "                         data_df[['pano_id',data]],\n",
    "                         on='pano_id',\n",
    "                         how='left'\n",
    "                        )\n",
    "        location_df.drop(location_df.filter(regex='Unnamed').columns, axis=1, inplace=True)\n",
    "        \n",
    "    # scaling\n",
    "    location_df['bikeability']=location_df.iloc[:,-(num_category):].sum(axis=1)\n",
    "    print(location_df)\n",
    "    location_df.to_csv(os.path.join(root,'data/tabular_data/{}/only_svi_bikeability.csv'.format(city)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-bulletin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
